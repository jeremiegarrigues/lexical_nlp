{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmtbvkjFFu9I"
   },
   "source": [
    "#**PHASE D'AMORCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLABAWSWT_kg"
   },
   "source": [
    "## **INITIALISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # <--- Importer le module sys\n",
    "new_limit = 10 * 1024 * 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite de taille de champ CSV augmentée à 10485760 bytes\n",
      "------------------------------\n",
      "Traitement terminé.\n",
      "Fichier d'entrée : '/app/DATA/hard_right_wing_all_transcripts.csv'\n",
      "Fichier de sortie : '/app/DATA/filtered_transcripts.csv'\n",
      "Lignes lues au total : 35730\n",
      "Lignes contenant 'Jordan B Peterson' supprimées : 1204\n",
      "Lignes écrites dans le nouveau fichier : 34526\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys # <--- Importer le module sys\n",
    "\n",
    "# --- Configuration ---\n",
    "input_filename = '/app/DATA/hard_right_wing_all_transcripts.csv'\n",
    "output_filename = '/app/DATA/filtered_transcripts.csv'\n",
    "column_to_check = 'channel_title'\n",
    "value_to_remove = 'Jordan B Peterson'\n",
    "delimiter = ';'\n",
    "\n",
    "# --- Augmenter la limite de taille de champ CSV ---\n",
    "# Essayez d'augmenter la limite. Commencez par une valeur plus grande,\n",
    "# par exemple 1 Mo (1024*1024 bytes) ou 10 Mo (10*1024*1024).\n",
    "# Si l'erreur persiste, augmentez davantage cette valeur.\n",
    "# Attention : une valeur trop grande peut consommer beaucoup de mémoire.\n",
    "# Vous pouvez aussi essayer la limite maximale théorique : sys.maxsize\n",
    "# mais commencez par une valeur définie plus raisonnable.\n",
    "\n",
    "# Tentative avec 10 Mo (ajustez si nécessaire) :\n",
    "new_limit = 10 * 1024 * 1024\n",
    "try:\n",
    "    # Boucle pour gérer les cas où même sys.maxsize n'est pas suffisant (très rare)\n",
    "    # ou pour trouver une limite fonctionnelle de manière itérative si on ne met pas sys.maxsize directement\n",
    "    current_limit = csv.field_size_limit()\n",
    "    while new_limit > current_limit:\n",
    "        try:\n",
    "            csv.field_size_limit(new_limit)\n",
    "            print(f\"Limite de taille de champ CSV augmentée à {new_limit} bytes\")\n",
    "            break # Sortir si le réglage a fonctionné\n",
    "        except OverflowError:\n",
    "            # Si new_limit est trop grand pour le système (ex: sys.maxsize sur certains systèmes 32 bits)\n",
    "            # Réduire la limite par étapes peut être une stratégie, mais souvent on met sys.maxsize\n",
    "            print(f\"Impossible de définir la limite à {new_limit}, tentative avec une valeur légèrement inférieure ou sys.maxsize.\")\n",
    "            # Pour simplifier ici, on peut directement essayer sys.maxsize\n",
    "            # ou simplement réduire la valeur manuellement si on sait qu'elle est trop grande.\n",
    "            # Mettons sys.maxsize comme alternative courante:\n",
    "            try:\n",
    "                csv.field_size_limit(sys.maxsize)\n",
    "                print(f\"Limite de taille de champ CSV augmentée à sys.maxsize ({sys.maxsize} bytes)\")\n",
    "                new_limit = sys.maxsize # Mettre à jour new_limit pour sortir de la boucle\n",
    "            except OverflowError:\n",
    "                 print(\"ERREUR CRITIQUE : Impossible d'augmenter suffisamment la limite de taille de champ CSV.\")\n",
    "                 raise # Relancer l'erreur si même sys.maxsize échoue\n",
    "            break # Sortir après avoir tenté sys.maxsize\n",
    "\n",
    "# Gestion plus simple (souvent suffisante):\n",
    "# try:\n",
    "#    csv.field_size_limit(new_limit) # Essayez avec 10MB\n",
    "# except OverflowError:\n",
    "#    csv.field_size_limit(sys.maxsize) # Si 10MB trop grand (peu probable), essayez max\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la modification de la limite de taille de champ : {e}\")\n",
    "    # Optionnel: quitter si on ne peut pas changer la limite\n",
    "    # raise SystemExit()\n",
    "\n",
    "\n",
    "# --- Vérification de l'existence du fichier d'entrée ---\n",
    "if not os.path.exists(input_filename):\n",
    "    print(f\"Erreur : Le fichier d'entrée '{input_filename}' n'a pas été trouvé.\")\n",
    "else:\n",
    "    try:\n",
    "        # --- Traitement ---\n",
    "        rows_written = 0\n",
    "        rows_read = 0\n",
    "        rows_removed = 0\n",
    "\n",
    "        # Ouvre le fichier d'entrée en lecture et le fichier de sortie en écriture\n",
    "        with open(input_filename, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "             open(output_filename, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "            # Crée un lecteur CSV pour lire le fichier d'entrée\n",
    "            # NOTE: Le lecteur utilisera la nouvelle limite de champ définie globalement\n",
    "            reader = csv.reader(infile, delimiter=delimiter)\n",
    "\n",
    "            # Crée un écrivain CSV pour écrire dans le fichier de sortie\n",
    "            writer = csv.writer(outfile, delimiter=delimiter)\n",
    "\n",
    "            # Lit la ligne d'en-tête\n",
    "            header = next(reader)\n",
    "            rows_read += 1\n",
    "\n",
    "            # Trouve l'index de la colonne 'channel_title'\n",
    "            try:\n",
    "                channel_title_index = header.index(column_to_check)\n",
    "            except ValueError:\n",
    "                print(f\"Erreur : La colonne '{column_to_check}' n'existe pas dans l'en-tête du fichier.\")\n",
    "                exit()\n",
    "\n",
    "            # Écrit l'en-tête dans le fichier de sortie\n",
    "            writer.writerow(header)\n",
    "            rows_written += 1\n",
    "\n",
    "            # Parcourt chaque ligne restante dans le fichier d'entrée\n",
    "            for row in reader:\n",
    "                rows_read += 1\n",
    "                if len(row) > channel_title_index:\n",
    "                    if row[channel_title_index] != value_to_remove:\n",
    "                        writer.writerow(row)\n",
    "                        rows_written += 1\n",
    "                    else:\n",
    "                        rows_removed += 1\n",
    "                else:\n",
    "                    print(f\"Avertissement : Ligne {rows_read} ignorée car elle a moins de colonnes que prévu.\")\n",
    "\n",
    "        # --- Fin du traitement ---\n",
    "        print(\"-\" * 30)\n",
    "        print(\"Traitement terminé.\")\n",
    "        print(f\"Fichier d'entrée : '{input_filename}'\")\n",
    "        print(f\"Fichier de sortie : '{output_filename}'\")\n",
    "        print(f\"Lignes lues au total : {rows_read}\")\n",
    "        print(f\"Lignes contenant '{value_to_remove}' supprimées : {rows_removed}\")\n",
    "        print(f\"Lignes écrites dans le nouveau fichier : {rows_written}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier d'entrée '{input_filename}' n'a pas été trouvé lors de l'ouverture.\")\n",
    "    except csv.Error as e: # Capture spécifique de l'erreur CSV\n",
    "        print(f\"Erreur CSV lors de la lecture de la ligne {rows_read+1}: {e}\")\n",
    "        print(\"Cela peut se produire si la limite de champ est encore trop petite ou si le fichier CSV est mal formé.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur inattendue est survenue (ligne approx {rows_read+1}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/vscode/.local/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/vscode/.local/lib/python3.12/site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/vscode/.local/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/vscode/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/vscode/.local/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/vscode/.local/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/vscode/.local/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/vscode/.local/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/vscode/.local/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bibliothèque standard ---\n",
    "from collections import Counter, defaultdict\n",
    "import contextlib\n",
    "import copy\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "import gc\n",
    "import html\n",
    "import importlib\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import locale\n",
    "import logging\n",
    "import math\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import subprocess\n",
    "import time\n",
    "from urllib import request # UTILE ?\n",
    "import spacy\n",
    "from smart_open import open\n",
    "import openai\n",
    "from openai import OpenAIError\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import Counter\n",
    "import contextlib\n",
    "\n",
    "# --- Bibliothèques externes ---\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import from_path\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from dateutil import parser\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "from ortools.linear_solver import pywraplp\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from pyate import cvalues\n",
    "from pyate.term_extraction_pipeline import TermExtractionPipeline\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import cosine, pdist\n",
    "import seaborn as sns\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import (\n",
    "    cosine_distances,\n",
    "    cosine_similarity,\n",
    "    manhattan_distances\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import torch\n",
    "#import torchaudio\n",
    "#import torchvision\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "import unidecode\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/app/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kz3aCJkxkSdX"
   },
   "outputs": [],
   "source": [
    "PX_PER_TOPIC = 60      # Hauteur en pixels par topic\n",
    "DPI = 72*4              # Résolution : 300 dpi\n",
    "FIGURE_WIDTH_INCH = 6.30 # A4 AVEC 2.5CM DE MARGES #4.02 UN GALLIMARD AVEC 2.4CM DE MARGES # Largeur fixe (en pouces) que vous souhaitez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actuellement, seuls les langues suivantes sont prises en charge :\n",
    "# français, anglais, espagnol, allemand, catalan, chinois, danois, japonais, slovaque, et ukrainien.\n",
    "# Codes linguistiques correspondants : fr, en, es, de, ca, zh, da, ja, sl, uk.\n",
    "language = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3hO-36U0IBcg"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import font_manager, rcParams\n",
    "\n",
    "matplotlib.rcParams['font.size'] = 10.5\n",
    "\n",
    "# Spécifiez le chemin vers le fichier de police\n",
    "font_path = folder_path + \"Roboto/XCharter-Roman.otf\"\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "xcharter_font = font_manager.FontProperties(fname=font_path)\n",
    "matplotlib.rcParams['font.family'] = xcharter_font.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "nlp_pipeline = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21253,
     "status": "ok",
     "timestamp": 1741769248254,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "EtUDkn0mNFAF",
    "outputId": "ba0b0025-ed33-4468-fdba-58d7a7bfe8fc"
   },
   "outputs": [],
   "source": [
    "#nlp_pipeline = None\n",
    "file_to_run = f\"{folder_path}lexical_topic_modeling_backend.ipynb\"\n",
    "%run \"$file_to_run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPy-S4HFDH1P"
   },
   "source": [
    "## **DÉFINITION DES PARAMÈTRES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "q-C1iPquO3Th"
   },
   "outputs": [],
   "source": [
    "# **grammatical_classes** : list[str], optionnel\n",
    "#     Une liste de classes grammaticales à conserver lors de la construction des vecteurs TF-IDF.\n",
    "#     Les classes doivent correspondre aux catégories grammaticales reconnues par spaCy.\n",
    "#     Par défaut, les classes `['NOUN', 'PROPN', 'VERB']` sont utilisées, ciblant les noms communs,\n",
    "#     noms propres et verbes.\n",
    "#\n",
    "# ### Classes grammaticales reconnues par spaCy :\n",
    "# ------------------------------------------\n",
    "# - 'NOUN' : Noms communs\n",
    "# - 'PROPN' : Noms propres\n",
    "# - 'VERB' : Verbes\n",
    "# - 'ADJ' : Adjectifs\n",
    "# - 'ADV' : Adverbes\n",
    "# - 'PRON' : Pronoms\n",
    "# - 'DET' : Déterminants (articles)\n",
    "# - 'ADP' : Prépositions ou postpositions\n",
    "# - 'CONJ' : Conjonctions de coordination\n",
    "# - 'CCONJ' : Conjonctions de coordination (UD)\n",
    "# - 'SCONJ' : Conjonctions de subordination (UD)\n",
    "# - 'AUX' : Verbes auxiliaires (ex. \"être\", \"avoir\")\n",
    "# - 'PART' : Particules\n",
    "# - 'INTJ' : Interjections\n",
    "# - 'NUM' : Nombres\n",
    "# - 'SYM' : Symboles\n",
    "# - 'X' : Autres (inclassables)\n",
    "#\n",
    "# ### Retourne :\n",
    "# ---------\n",
    "# **tuple** :\n",
    "#     Un tuple contenant les résultats de la vectorisation TF-IDF :\n",
    "#     - **tfidf_vectorizer** : L'objet `TfidfVectorizer` utilisé pour la vectorisation.\n",
    "#     - **tfidf** : Une matrice sparse TF-IDF des documents vectorisés.\n",
    "#     - **tfidf_feature_names** : La liste des unigrams sélectionnés comme caractéristiques.\n",
    "#     - **tokenized_documents** : Une version tokenisée et filtrée des documents d'entrée, limitée\n",
    "#       aux classes grammaticales spécifiées.\n",
    "#\n",
    "# ### Exemple :\n",
    "# ---------\n",
    "# Pour effectuer une vectorisation TF-IDF sur les catégories grammaticales 'NOUN' et 'PROPN' :\n",
    "# grammatical_classes = ['NOUN', 'PROPN']\n",
    "grammatical_classes = ['NOUN', 'PROPN', 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CZJeY61n_92x"
   },
   "outputs": [],
   "source": [
    "# La variable `threshold` fixe le seuil minimal d'articles requis pour qu'un journal\n",
    "# soit inclus dans certaines analyses spécifiques :\n",
    "# - Les journaux ayant un nombre d'articles inférieur à ce seuil ne seront pas pris\n",
    "#   en compte dans les visualisations portant sur les journaux (exemple : graphiques\n",
    "#   comparant des volumes de publications ou des analyses statistiques spécifiques).\n",
    "# - Cela n'exclut pas ces journaux du corpus pour d'autres types d'analyse, comme le topic modeling,\n",
    "#   où tous les articles restent intégrés indépendamment de leur origine.\n",
    "#\n",
    "# Ce seuil permet de concentrer les analyses et tests statistiques sur des journaux\n",
    "# suffisamment représentatifs, en évitant que des titres marginalement présents\n",
    "# introduisent du bruit dans les résultats.\n",
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OqQlVOQP-0D4"
   },
   "outputs": [],
   "source": [
    "# Ces paramètres s'appliquent principalement aux corpus issus d'Europresse.\n",
    "# Par exemple, les articles contenant moins de 500 caractères ou plus de 100 000 caractères\n",
    "# sont considérés comme suspects et sont donc supprimés du corpus.\n",
    "#\n",
    "# Ces valeurs doivent être ajustées en fonction du type de corpus analysé,\n",
    "# car elles peuvent varier significativement selon les sources et les objectifs de l'étude.\n",
    "#\n",
    "# Pour les corpus provenant d'Europresse, l'algorithme élimine systématiquement les articles\n",
    "# dont le nombre de caractères est inférieur à `minimum_caracters_nb_by_document` ou supérieur à\n",
    "# `maximum_caracters_nb_by_document`. Cette approche repose sur trois considérations :\n",
    "#\n",
    "# 1. **Suspicion de contenu non standard** : Un article comportant moins de 500 caractères ou\n",
    "#    plus de 100 000 caractères s'écarte des normes usuelles. Ces cas extrêmes suggèrent des anomalies\n",
    "#    (ex. : méta-données mal extraites, textes hors contexte ou dégradés).\n",
    "#\n",
    "# 2. **Analyse thématique limitée** : Un article très court (moins de 100 ou 200 caractères,\n",
    "#    soit environ vingt mots) ne fournit pas suffisamment de matière pour une analyse lexicale\n",
    "#    ou thématique pertinente. Cela revient à une situation comparable au rejet d'une analyse\n",
    "#    du chi² pour des croisements avec des effectifs insuffisants.\n",
    "#\n",
    "# 3. **Limitation technique de l’algorithme de NER (Reconnaissance d’Entités Nommées)** :\n",
    "#    Les outils de NER utilisés (par exemple, spaCy, Stanford NER, etc.) peuvent présenter\n",
    "#    des contraintes de longueur. Au-delà de 100 000 caractères, l’algorithme risque de ne plus\n",
    "#    pouvoir traiter correctement le document, entraînant un blocage technique. Le seuil\n",
    "#    maximum de 100 000 caractères vise donc également à préserver la faisabilité et la\n",
    "#    qualité de l’analyse NER.\n",
    "minimum_caracters_nb_by_document = 2000\n",
    "maximum_caracters_nb_by_document = 100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wYtY3pyB-_rT"
   },
   "outputs": [],
   "source": [
    "# Ce paramètre détermine si les doublons doivent être conservés ou non.\n",
    "\n",
    "\n",
    "# - À False : les doublons sont conservés.\n",
    "# - À True : les doublons sont supprimés.\n",
    "#\n",
    "# La valeur True est particulièrement pertinente pour les corpus issus d'Europresse.\n",
    "# Elle permet de gérer plusieurs problèmes liés à la collecte des données :\n",
    "# - Élimination des doublons potentiels causés par le fonctionnement spécifique d'Europresse.\n",
    "# - Suppression des recouvrements partiels entre différents fichiers d'articles,\n",
    "#   évitant ainsi les répétitions accidentelles.\n",
    "# - Identification et retrait des articles produits par simple copié/collé,\n",
    "#   grâce à un algorithme qui résiste aux variations mineures. Ainsi, les articles\n",
    "#   n'apportant aucune information substantiellement nouvelle sont exclus du corpus.\n",
    "#\n",
    "# Toutefois, conserver les doublons (valeur False) peut également être justifié,\n",
    "# car ces répétitions reflètent une certaine dynamique médiatique. Dans ce cas,\n",
    "# le paramètre devra rester à False pour intégrer ces éléments au corpus.\n",
    "go_remove_duplicates = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1-KjNjyJ_P2D"
   },
   "outputs": [],
   "source": [
    "# La variable `web_paper_differentiation` détermine si les versions papier et web\n",
    "\n",
    "# d'une même revue doivent être fusionnées ou conservées distinctes :\n",
    "# - `False` : les versions papier et web sont fusionnées en un seul ensemble.\n",
    "# - `True` : les versions papier et web sont traitées comme des entités distinctes.\n",
    "#\n",
    "# Ce choix dépend de la finalité de l'analyse :\n",
    "# - Si l'objectif est de considérer un média dans son ensemble, sans distinction\n",
    "#   entre ses formats, laissez cette variable à `False`.\n",
    "# - Si l'on souhaite analyser les spécificités des contenus selon leur support\n",
    "#   (papier ou web), définissez cette variable à `True`.\n",
    "web_paper_differentiation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hXT9yj4Z_e1R"
   },
   "outputs": [],
   "source": [
    "# La variable `source_type` indique l'origine du corpus et ajuste le traitement des données en conséquence.\n",
    "#\n",
    "# Valeurs possibles pour `source_type` :\n",
    "# - \"europresse\" : le corpus provient de la plateforme Europresse.\n",
    "#   - Des prétraitements spécifiques sont appliqués pour gérer les particularités de cette source,\n",
    "#     telles que le format des fichiers ou les métadonnées propres à Europresse.\n",
    "# - \"istex\" : le corpus provient de la plateforme ISTEX.\n",
    "#   - Des étapes adaptées à la structure des métadonnées et des documents fournis par ISTEX sont appliquées.\n",
    "# - \"csv\" : le corpus provient d'un fichier CSV ou d'autres formats standards.\n",
    "#   - Le séparateur du fichier doit être une virgule ou un point-virgule. Le fichier doit également\n",
    "#     contenir a minima deux colonnes : une colonne \"text\" et une colonne \"date\".\n",
    "#\n",
    "# Le choix de `source_type` influence directement les étapes de prétraitement et d'analyse des données.\n",
    "source_type = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gWkHUM-O-ywf"
   },
   "outputs": [],
   "source": [
    "# La variable `base_name` correspond au nom du corpus. Elle doit être modifiée de manière à\n",
    "\n",
    "# décrire précisément le corpus analysé, en suivant des bonnes pratiques de nommage.\n",
    "# Exemple : \"ukraine_russie__presse_francilienne__fev2022_feb2023\".\n",
    "#\n",
    "# Le contenu de `base_name` doit être présent dans les noms des fichiers situés dans le dossier \"DATA\"\n",
    "# et utilisés pour l'analyse. L'algorithme est conçu pour gérer des corpus répartis sur plusieurs fichiers :\n",
    "# il suffit que tous les fichiers concernés contiennent le texte exact de `base_name` dans leur nom.\n",
    "#\n",
    "# Exemple de fichiers conformes :\n",
    "# - \"ukraine_russie__presse_francilienne__fev2022_mars2024__feb2022_sep2022.HTML\"\n",
    "# - \"ukraine_russie__presse_francilienne__fev2022_feb2023__oct2022_feb2023.HTML\"\n",
    "#\n",
    "# Ces fichiers seront inclus dans l'analyse dès lors qu'ils partagent le même `base_name`.\n",
    "#\n",
    "# De plus, les résultats de l'analyse seront stockés dans un dossier spécifique dont le chemin\n",
    "# est défini par la variable `RESULTS_PATH`. Ce chemin inclut le `base_name` pour assurer une\n",
    "# organisation cohérente des résultats.\n",
    "base_name = 'hard_right_wing_all'\n",
    "results_path = folder_path + \"RESULTS_\" + base_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bgooqvcWUFJj"
   },
   "outputs": [],
   "source": [
    "# Cette fonction crée un dossier de résultats basé sur le nom spécifié dans `base_name`.\n",
    "\n",
    "# Elle vérifie si le dossier existe déjà, et si ce n'est pas le cas, elle le crée.\n",
    "# Le nom du dossier sera préfixé par \"RESULTS_\" et inclura le contenu de `base_name`.\n",
    "# Elle définit également le nom du fichier CSV où les résultats seront sauvegardés.\n",
    "create_results_folder(base_name)# CA PEUT PASSER DANS LA LECTURE DES DOCUMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llQhJTcDDNdk"
   },
   "source": [
    "## **PRÉPARATION DES DOCUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "6d411e848df049acba679d8f6ac021f4",
      "b951c23a57b24298bbfc84a946219bd5",
      "ff08f96f6c68498699fa5aa86e5c3f50",
      "6c798065d3d34f33a5cb4ef3a86352a3",
      "813a0096c0a14544bf8587197047d519",
      "a24d702ff6494dac8b6ed48a464c7e06",
      "40bbe11df03b4da3b011249554f5a4af",
      "96e1596583854dd28f6f45d8018127b0",
      "8e0f122d9b38442586646c7d54f2b905",
      "c4f4c50162da4955884c79c8bf78db39",
      "5945ec1d12e84726bb59acd843647bba"
     ]
    },
    "executionInfo": {
     "elapsed": 30914,
     "status": "ok",
     "timestamp": 1741769666853,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "1ABNfjfms9Pk",
    "outputId": "f339d61d-d32e-40f5-80bf-a24f394b14a4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5885bd5aaf34173b0e431445ee51d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DOCUMENTS PROCESSÉS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Taille du fichier CSV = 693.73 Mo\n",
      "[*] Le fichier est > 200 Mo : lecture directe en UTF-8 (séparateur ';')\n",
      "\n",
      "\n",
      "27009 documents\n"
     ]
    }
   ],
   "source": [
    "# Initialisation des variables pour stocker les documents, les objets BeautifulSoup associés,\n",
    "# et les métadonnées extraites des différentes sources de données (europresse, CSV, ISTEX).\n",
    "#\n",
    "# - 'documents' : liste qui contiendra les textes extraits des documents.\n",
    "#    - Pour 'europresse', ce seront les textes nettoyés extraits des articles HTML.\n",
    "#    - Pour 'csv', ce seront les contenus des colonnes 'text' ou 'description' des fichiers CSV filtrés par le nombre de caractères.\n",
    "#    - Pour 'istex', ce seront les contenus texte extraits des fichiers `.txt` associés aux fichiers `.json` correspondants.\n",
    "documents = []\n",
    "\n",
    "# - 'all_soups' : liste qui contiendra les objets BeautifulSoup pour chaque document.\n",
    "#    Cela est utilisé pour manipuler et analyser la structure HTML des documents extraits, en particulier pour 'europresse',\n",
    "#    où chaque document est transformé en un objet BeautifulSoup afin de procéder à des nettoyages supplémentaires (par exemple, suppression de certains éléments HTML).\n",
    "all_soups = []\n",
    "\n",
    "# - 'columns_dict' : dictionnaire qui stocke les métadonnées liées à chaque document.\n",
    "#    - Pour 'csv', cela contiendra les autres colonnes du fichier (autres que 'text' et 'description') que l'on souhaite conserver comme métadonnées.\n",
    "#    - Pour 'istex', ce dictionnaire contiendra des champs supplémentaires extraits des fichiers JSON associés à chaque document, tels que 'doi', 'journal', 'date', etc.\n",
    "columns_dict = {}\n",
    "\n",
    "# La fonction 'meta_load_documents()' est appelée pour charger et traiter les données des documents à partir de la source spécifiée\n",
    "# (définie par 'source_type'). En fonction de cette source, les données sont extraites et stockées dans les structures ci-dessus pour un traitement ultérieur.\n",
    "meta_load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYISqZLGF6Wn"
   },
   "source": [
    "#**PHASE DE PRODUCTION DES MATRICES H ET W**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y__k8itnLupq"
   },
   "source": [
    "##**LEMMATISATION ET VECTORISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Méthode de démarrage multiprocessing définie sur 'forkserver'\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if sys.platform.startswith('linux') or sys.platform.startswith('darwin'): # Fonctionne sous Linux/macOS\n",
    "         try:\n",
    "             multiprocessing.set_start_method('forkserver', force=True)\n",
    "             print(\"Méthode de démarrage multiprocessing définie sur 'forkserver'\")\n",
    "         except RuntimeError:\n",
    "             print(\"Impossible de redéfinir la méthode de démarrage multiprocessing (déjà démarrée?).\")\n",
    "    # ... reste du code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "d29f1fc02d964c8e8600308bf6d09dec",
      "0dc48acef57f428f972e75ebfa50338e",
      "6a52320dd96d4e0c89f9b16d75772ff0",
      "e10268dc4c3e4c4e978c057af32147c2",
      "5245b6f1cf074219aba13aadb968246f",
      "bbdfb3a58d8f4388bfd1b771bab3bb9b",
      "257f2a9bb9a7453dbb59546de453ad76",
      "a5ac99fc9e7747808608e466b3fb6172",
      "0c4c95e25a114e10beeab3b484b61cd3",
      "c998fc39c42743e38f86e2b5ebae80d9",
      "ff9c6da282f442359847c81b69cb8243"
     ]
    },
    "executionInfo": {
     "elapsed": 1198143,
     "status": "ok",
     "timestamp": 1741770903565,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "YZPAL-JPMyec",
    "outputId": "f8129932-ab39-4c6a-ab92-968a9cccc697"
   },
   "outputs": [],
   "source": [
    "# La fonction process_documents traite une liste de documents en effectuant les opérations suivantes :\n",
    "\n",
    "\n",
    "# 1. Chargement d'un modèle linguistique spaCy en fonction de la langue spécifiée.\n",
    "# 2. Pour chaque document, elle effectue :\n",
    "#    - La lemmatisation des tokens (avec normalisation des noms propres via unidecode).\n",
    "#    - L'extraction des étiquettes de parties du discours (POS).\n",
    "#    - La création de listes de mots lemmatisés et normalisés pour chaque phrase.\n",
    "# 3. Les résultats sont stockés dans quatre listes :\n",
    "#    - 'documents_lemmatized' (pour les n-grams),\n",
    "#    - 'all_tab_pos' (mots et leurs POS par document),\n",
    "#    - 'sentences_norms' (phrases normalisées),\n",
    "#    - 'all_sentence_pos' (mots et leurs POS par phrase).\n",
    "# 4. Un suivi de la progression est effectué via tqdm, et des erreurs sont loggées sans interrompre l'exécution.\n",
    "#documents_lemmatized = []\n",
    "all_tab_pos = []\n",
    "sentences_norms = []\n",
    "all_sentence_pos = []\n",
    "#process_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xQNj6K3ZYL2"
   },
   "source": [
    "##**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Main Workflow (Single SpaCy Pass) ---\n",
      "Attempting to set spaCy GPU preference...\n",
      "-> Could not set/check spaCy GPU preference: Cannot use GPU, CuPy is not installed. Relying on pipeline's loaded device.\n",
      "Using provided spaCy pipeline potentially configured for device: CPU (default)\n",
      "Temporary sentence file created at: /tmp/sentences_vhu659ht.txt\n",
      "\n",
      "--- Running Single SpaCy Processing and Data Collection ---\n",
      "SpaCy pipeline operating on CPU. Using 8 processes for nlp.pipe.\n",
      "Starting single spaCy processing pass...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4933c6c976740fca87a9a7c5dfc878d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Docs (spaCy):   0%|          | 0/27009 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy processing complete. Collected data for 3155973 sentences.\n",
      "Successfully processed 3155973 sentences.\n",
      "Found 27009 documents in the processed data.\n",
      "\n",
      "--- Creating Gensim Dictionary and Tokenized Corpus (Document Level) ---\n",
      "Tokenized corpus created with 27009 documents.\n",
      "Total tokens in tokenized corpus: 35614172\n",
      "Gensim Dictionary created with 144341 unique tokens.\n",
      "\n",
      "--- Vectorizing Documents (TF-IDF) ---\n",
      "Starting TF-IDF vectorization (level: document)...\n",
      "Applying CountVectorizer (fit_transform) for level 'document'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3f42b927b24830bf987a53674f2281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing documents:   0%|          | 0/27009 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts matrix shape for level 'document': (27009, 83367)\n",
      "Applying TfidfTransformer (fit_transform) for level 'document'...\n",
      "TF-IDF matrix shape for level 'document': (27009, 83367)\n",
      "Vocabulary size for level 'document': 83367\n",
      "\n",
      "--- Vectorizing Sentences (TF-IDF) ---\n",
      "Starting TF-IDF vectorization (level: sentence)...\n",
      "Applying CountVectorizer (fit_transform) for level 'sentence'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac21447eb4b40ffaebaa6a921aeb9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing sentences:   0%|          | 0/3155973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts matrix shape for level 'sentence': (3155973, 86000)\n",
      "Applying TfidfTransformer (fit_transform) for level 'sentence'...\n",
      "TF-IDF matrix shape for level 'sentence': (3155973, 86000)\n",
      "Vocabulary size for level 'sentence': 86000\n",
      "Warning: Vocabularies differ between document (83367) and sentence (86000) vectorization (expected if min_df > 1 in vectorizer).\n",
      "Info: Scikit-learn sentence vocabulary (86000 terms) and Gensim dictionary (144341 tokens) may differ due to vectorizer settings (e.g., min_df) or Gensim filtering (if applied).\n",
      "\n",
      "--- Main Workflow Completed Successfully ---\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_feature_names, X_sentences, gensim_dictionary, tokenized_corpus, sentence_file_path, sentence_offsets = main_workflow_single_pass(documents, grammatical_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up temporary sentence file: /tmp/sentences_188n59ce.txt\n",
      "Temporary file removed.\n"
     ]
    }
   ],
   "source": [
    "# --- Cleanup ---\n",
    "print(f\"\\nCleaning up temporary sentence file: {sentence_file_path}\")\n",
    "try:\n",
    "    os.remove(sentence_file_path)\n",
    "    print(\"Temporary file removed.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error removing temporary file {sentence_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "109minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_relevant_sentences_and_titles(\n",
    "    nmf_models,              # Dictionnaire des modèles NMF entraînés {num_topic: model}\n",
    "    X_sentences,             # La matrice TF-IDF des phrases (calculée avant)\n",
    "    sentence_file_path,      # Chemin vers le fichier temporaire des phrases\n",
    "    sentence_offsets,        # Liste des offsets des phrases dans le fichier\n",
    "    language,                # Info langue ('fr', 'en', etc.)\n",
    "    preprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "6ed24d4ab2b34844ac740f610899f716",
      "bdd47d4986834f368168d34776965aa5",
      "2e43333161254325a19e8285edbb93ce",
      "8247fe49192a47c88da052388b309c9a",
      "993e44343c394e46996719dc596d9468",
      "77968bf9e2d245959a8baaa74ac969ce",
      "c0501456c0034c30984d2f2b2e6b0195",
      "284ec8e15a2a4c839f436d8d20c8268c",
      "6f477fbb55f140e9856f82f4d38ed312",
      "634033e7fbe345c58d7dfd276a2f2789",
      "2b5fb742bc2f42fcbdaa176eb3194b62",
      "d95819ac1f024d1f825f373852b4f848",
      "f2cb881e86a84b9d97fdd96ebd8509c8",
      "72bf281e72104988b8af544dcef9cbe5",
      "592368db4fcb41849664dc13fe64be77",
      "556e919d627c4e13a3e4192a85cdf6e4",
      "d10a1681a9e44f499be3a68f9e638f94",
      "9a91c1a4c25049049eca8b933b191941",
      "199d5c7aad2b4945b43d4459d594a2c7",
      "660a0dda97ca416e8b97ce48adf066ed",
      "0b05464c072c435fa7dc4d80172a633e",
      "b6bf255c06a9413d8060331dddf1ccf5"
     ]
    },
    "executionInfo": {
     "elapsed": 61988,
     "status": "ok",
     "timestamp": 1741770965558,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "QZFa9S_AOMZl",
    "outputId": "068e9204-fc00-457b-9e22-03e1fd3f819a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf7318f061b4ecabf7387c17fb3edb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mise à jour des unigrams:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758417c4833b4828a140e208d052dc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtrage des stopwords:   0%|          | 0/86338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Effectue une vectorisation TF-IDF des documents, en se concentrant sur des unigrams correspondant\n",
    "# aux classes grammaticales spécifiées.\n",
    "#\n",
    "# Cette fonction applique une sélection basée sur les classes grammaticales définies par `grammatical_classes`\n",
    "# afin de limiter la construction des vecteurs TF-IDF aux unigrams pertinents. Cela permet de réduire\n",
    "# le bruit en excluant les mots non pertinents (par exemple, les déterminants ou les particules)\n",
    "# et d'améliorer la qualité des résultats analytiques.\n",
    "unigrams = {}\n",
    "tfidf, tfidf_feature_names = go_tfidf_vectorization(grammatical_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add this import at the top of your script ---\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# --- Fonction d'Initialisation H (Décroissance Items + Bruit) - CORRECTED ---\n",
    "def create_decreasing_H(n_components, n_features, n_items_decay, decay_power, noise_level, random_state, epsilon, initial_peak):\n",
    "    \"\"\"Crée H où chaque ligne suit une tendance décroissante sur les items.\"\"\"\n",
    "    rng = check_random_state(random_state)\n",
    "    base_decay_pattern = np.full(n_features, epsilon)\n",
    "\n",
    "    # Calculate potential float value\n",
    "    _actual_decay_items_float = min(n_items_decay, n_features)\n",
    "\n",
    "    # --- FIX: Cast to integer before use ---\n",
    "    actual_decay_items = int(_actual_decay_items_float)\n",
    "    # Note: This truncates. If you prefer rounding, use int(round(_actual_decay_items_float))\n",
    "    # but simple truncation is usually fine here.\n",
    "\n",
    "    if actual_decay_items > 1:\n",
    "        # Now uses the integer version\n",
    "        item_indices = np.arange(actual_decay_items)\n",
    "        # Use the float version for potentially more precise calculation if needed,\n",
    "        # but ensure denominator isn't zero if actual_decay_items_float is exactly 1.0\n",
    "        denominator = max(_actual_decay_items_float - 1, epsilon) # Avoid division by zero\n",
    "        decay_values = (1 - item_indices / denominator)**decay_power\n",
    "        decay_values = (decay_values * (1 - epsilon)) + epsilon\n",
    "        # Now uses the integer version for slicing\n",
    "        base_decay_pattern[:actual_decay_items] = decay_values\n",
    "\n",
    "    # Handle case where int(float_value < 1) becomes 0\n",
    "    elif actual_decay_items == 1 and n_features > 0:\n",
    "        base_decay_pattern[0] = initial_peak # Assign high value to the first feature if decay length >= 1\n",
    "    # If actual_decay_items is 0 (e.g., if n_items_decay was < 1), the pattern remains epsilon\n",
    "\n",
    "    H = np.zeros((n_components, n_features))\n",
    "    for k in range(n_components):\n",
    "        noise = rng.rand(n_features) * noise_level\n",
    "        H[k, :] = base_decay_pattern + noise\n",
    "    H = np.maximum(H, epsilon) # Ensure non-negativity\n",
    "    return H\n",
    "\n",
    "# --- Fonction d'Initialisation W (Décroissance Topics + Bruit) - Unchanged ---\n",
    "# (Keep the create_decreasing_W function as you provided it)\n",
    "def create_decreasing_W(n_samples, n_components, decay_power, noise_level, random_state, epsilon, initial_peak):\n",
    "    \"\"\"Crée W où chaque ligne suit une tendance décroissante sur les topics.\"\"\"\n",
    "    rng = check_random_state(random_state)\n",
    "    base_decay_pattern = np.full(n_components, epsilon)\n",
    "    if n_components > 1:\n",
    "        topic_indices = np.arange(n_components)\n",
    "        decay_values = (1 - topic_indices / (n_components - 1))**decay_power\n",
    "        decay_values = (decay_values * (1 - epsilon)) + epsilon\n",
    "        base_decay_pattern = decay_values\n",
    "    elif n_components == 1:\n",
    "        base_decay_pattern[0] = initial_peak\n",
    "    W = np.zeros((n_samples, n_components))\n",
    "    for i in range(n_samples):\n",
    "        noise = rng.rand(n_components) * noise_level\n",
    "        W[i, :] = base_decay_pattern + noise\n",
    "    W = np.maximum(W, epsilon)\n",
    "    return W\n",
    "\n",
    "# --- Fonction pour sauvegarder les topics formatés ---\n",
    "# (Doit être définie avant d'être appelée)\n",
    "def save_topics_to_file(H_matrix, feature_names, filepath, n_top_words=20):\n",
    "    \"\"\"\n",
    "    Formate et sauvegarde les topics (mots et scores triés) dans un fichier CSV.\n",
    "    \"\"\"\n",
    "    all_topics_dfs = []\n",
    "    num_topics = H_matrix.shape[0]\n",
    "\n",
    "    # Assurer que feature_names est une liste ou un array indexable\n",
    "    if isinstance(feature_names, pd.Series):\n",
    "        feature_names = feature_names.tolist()\n",
    "    elif isinstance(feature_names, np.ndarray):\n",
    "         # Pas besoin de conversion si c'est déjà un ndarray\n",
    "         pass\n",
    "    elif not isinstance(feature_names, list):\n",
    "        print(f\"    ATTENTION: feature_names est de type {type(feature_names)}, tentative de conversion en liste.\")\n",
    "        try:\n",
    "            feature_names = list(feature_names)\n",
    "        except TypeError:\n",
    "            print(f\"    ERREUR: Impossible de convertir feature_names en liste. Sauvegarde des topics annulée pour {filepath}\")\n",
    "            return # Arrêter la fonction si les noms ne sont pas accessibles\n",
    "\n",
    "    num_features = len(feature_names)\n",
    "    if H_matrix.shape[1] != num_features:\n",
    "         print(f\"    ERREUR: Incohérence de dimensions! H_matrix a {H_matrix.shape[1]} features, mais feature_names en a {num_features}. Sauvegarde annulée pour {filepath}\")\n",
    "         return\n",
    "\n",
    "    n_top_words = min(n_top_words, num_features)\n",
    "\n",
    "    for topic_idx in range(num_topics):\n",
    "        topic_vector = H_matrix[topic_idx, :]\n",
    "        top_indices = topic_vector.argsort()[::-1][:n_top_words]\n",
    "        try:\n",
    "            # Tenter de récupérer les mots - peut échouer si les indices sont hors limites\n",
    "            # (ne devrait pas arriver si les dimensions sont vérifiées, mais sécurité)\n",
    "            words = [feature_names[i] for i in top_indices]\n",
    "        except IndexError:\n",
    "             print(f\"    ERREUR: Problème d'index lors de la récupération des mots pour Topic {topic_idx+1}. Vérifiez feature_names.\")\n",
    "             # Mettre des placeholders ou arrêter ? Mettons des placeholders.\n",
    "             words = [f\"ERREUR_INDEX_{i}\" for i in top_indices]\n",
    "        except Exception as e:\n",
    "             print(f\"    ERREUR inattendue lors de la récupération des mots pour Topic {topic_idx+1}: {e}\")\n",
    "             words = [f\"ERREUR_MOT_{i}\" for i in top_indices]\n",
    "\n",
    "        scores = topic_vector[top_indices]\n",
    "\n",
    "        topic_df = pd.DataFrame({\n",
    "            f'Topic {topic_idx+1} Word': words,\n",
    "            f'Topic {topic_idx+1} Score': scores\n",
    "        })\n",
    "        all_topics_dfs.append(topic_df)\n",
    "\n",
    "    final_df = pd.concat(all_topics_dfs, axis=1)\n",
    "    try:\n",
    "        final_df.to_csv(filepath, index=False, sep=',', float_format='%.5f', encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"\\n    ATTENTION : Échec sauvegarde CSV des topics ({filepath}): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'exploration NMF avec scénarios H & W...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDémarrage de l\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexploration NMF avec scénarios H & W...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     os.makedirs(\u001b[43moutput_dir\u001b[49m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     os.makedirs(topic_output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m     os.makedirs(init_csv_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Exécution Principale ---\n",
    "print(\"Démarrage de l'exploration NMF avec scénarios H & W...\")\n",
    "try:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(topic_output_dir, exist_ok=True)\n",
    "    os.makedirs(init_csv_dir, exist_ok=True)\n",
    "    print(f\"Vérification: Utilisation du dossier de sortie général: '{output_dir}'\")\n",
    "    print(f\"              Dossier pour initialisations CSV: '{init_csv_dir}'\")\n",
    "    print(f\"              Dossier pour détails des topics: '{topic_output_dir}'\")\n",
    "except OSError as e:\n",
    "    print(f\"\\nERREUR CRITIQUE: Impossible de créer les dossiers de sortie dans '{output_dir}'. Vérifiez les permissions.\")\n",
    "    print(f\"Erreur système: {e}\")\n",
    "    # Optionnel: arrêter le script si les dossiers sont indispensables\n",
    "    # import sys\n",
    "    # sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliothèque GENSIM (CoherenceModel, Dictionary) trouvée.\n",
      "\n",
      "Création du dictionnaire Gensim à partir de tokenized_documents...\n",
      "Dictionnaire Gensim créé avec 61062 tokens uniques.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59dd4371f814d89983fd8d1e0ac193e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSUS GLOBAL (Nb Topics):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Test pour num_topic = 7 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460535a67df74840ab0ff2f3227b91d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Scénarios Custom (K=7):   0%|          | 0/9 [00:00<?, ?scenario/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Meilleur scénario custom pour K=7 (par erreur): 'H_p=1.00_W_p=1.00' (Erreur: 8675.5236, Cohérence(Gensim): 0.1040)\n",
      "\n",
      "===== Test pour num_topic = 12 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecbf5eda93148668184a81d977dd95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Scénarios Custom (K=12):   0%|          | 0/9 [00:00<?, ?scenario/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Meilleur scénario custom pour K=12 (par erreur): 'H_p=1.00_W_p=1.50' (Erreur: 8627.0289, Cohérence(Gensim): 0.0914)\n",
      "\n",
      "===== Test pour num_topic = 15 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b04a06e77f74f1581c38e155da8c9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Scénarios Custom (K=15):   0%|          | 0/9 [00:00<?, ?scenario/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Meilleur scénario custom pour K=15 (par erreur): 'H_p=1.00_W_p=0.70' (Erreur: 8602.2419, Cohérence(Gensim): 0.0851)\n",
      "\n",
      "===== Comparaison avec l'initialisation NNDSVD (Cohérence via Gensim) =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796b899c382f47cdbdb68224ab0a8f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  NMF NNDSVD (par K):   0%|          | 0/3 [00:00<?, ?K/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparaison K=7:\n",
      "    NNDSVD (rs=1): Err=8675.5232, Coh(Gensim)=0.1040 (Dur: 2.52s)\n",
      "    Best Custom(err): Err=8675.5236, Coh(Gensim)=0.1040\n",
      "    -> NNDSVD supérieur ou égal (par erreur).\n",
      "  Comparaison K=12:\n",
      "    NNDSVD (rs=1): Err=8627.0285, Coh(Gensim)=0.0914 (Dur: 5.46s)\n",
      "    Best Custom(err): Err=8627.0289, Coh(Gensim)=0.0914\n",
      "    -> NNDSVD supérieur ou égal (par erreur).\n",
      "  Comparaison K=15:\n",
      "    NNDSVD (rs=1): Err=8602.0467, Coh(Gensim)=0.0917 (Dur: 7.53s)\n",
      "    Best Custom(err): Err=8602.2419, Coh(Gensim)=0.0851\n",
      "    -> NNDSVD supérieur ou égal (par erreur).\n",
      "\n",
      "===== Synthèse Globale des Initialisations Custom (Cohérence via Gensim) =====\n",
      "\n",
      "--- Tableau Récapitulatif des Résultats Custom ---\n",
      " num_topic  h_power  w_power          scenario     error  coherence_npmi_gensim  duration  success\n",
      "         7   1.0000   1.0000 H_p=1.00_W_p=1.00 8675.5236                 0.1040    2.6801     True\n",
      "         7   1.5000   1.0000 H_p=1.50_W_p=1.00 8675.5236                 0.1040    4.3266     True\n",
      "         7   0.7000   1.5000 H_p=0.70_W_p=1.50 8678.1484                 0.0931    2.7583     True\n",
      "         7   1.0000   1.5000 H_p=1.00_W_p=1.50 8678.1485                 0.0931    2.8739     True\n",
      "         7   1.5000   0.7000 H_p=1.50_W_p=0.70 8678.1485                 0.0939    2.7256     True\n",
      "         7   1.5000   1.5000 H_p=1.50_W_p=1.50 8678.1485                 0.0931    2.8587     True\n",
      "         7   0.7000   0.7000 H_p=0.70_W_p=0.70 8678.1485                 0.0939    2.6328     True\n",
      "         7   1.0000   0.7000 H_p=1.00_W_p=0.70 8678.1485                 0.0939    2.6300     True\n",
      "         7   0.7000   1.0000 H_p=0.70_W_p=1.00 8678.1485                 0.0939    3.2321     True\n",
      "        12   1.0000   1.5000 H_p=1.00_W_p=1.50 8627.0289                 0.0914    4.6992     True\n",
      "        12   1.5000   1.5000 H_p=1.50_W_p=1.50 8627.0289                 0.0914    4.7873     True\n",
      "        12   0.7000   1.5000 H_p=0.70_W_p=1.50 8627.0289                 0.0914    4.5449     True\n",
      "        12   1.5000   1.0000 H_p=1.50_W_p=1.00 8627.2743                 0.0962    7.8815     True\n",
      "        12   0.7000   1.0000 H_p=0.70_W_p=1.00 8627.2743                 0.0962    8.0845     True\n",
      "        12   1.0000   1.0000 H_p=1.00_W_p=1.00 8627.2743                 0.0951    7.8841     True\n",
      "        12   0.7000   0.7000 H_p=0.70_W_p=0.70 8627.2745                 0.0951    7.6591     True\n",
      "        12   1.0000   0.7000 H_p=1.00_W_p=0.70 8627.2745                 0.0951    7.6668     True\n",
      "        12   1.5000   0.7000 H_p=1.50_W_p=0.70 8627.2745                 0.0951    7.6391     True\n",
      "        15   1.0000   0.7000 H_p=1.00_W_p=0.70 8602.2419                 0.0851    4.0311     True\n",
      "        15   1.5000   0.7000 H_p=1.50_W_p=0.70 8602.2419                 0.0851    4.0157     True\n",
      "        15   0.7000   0.7000 H_p=0.70_W_p=0.70 8602.2420                 0.0851    4.2287     True\n",
      "        15   0.7000   1.0000 H_p=0.70_W_p=1.00 8602.8743                 0.0913    4.3940     True\n",
      "        15   1.0000   1.0000 H_p=1.00_W_p=1.00 8602.8743                 0.0913    4.3439     True\n",
      "        15   0.7000   1.5000 H_p=0.70_W_p=1.50 8602.8743                 0.0913    4.4590     True\n",
      "        15   1.5000   1.0000 H_p=1.50_W_p=1.00 8602.8743                 0.0913    4.0949     True\n",
      "        15   1.0000   1.5000 H_p=1.00_W_p=1.50 8602.8743                 0.0913    4.3164     True\n",
      "        15   1.5000   1.5000 H_p=1.50_W_p=1.50 8602.8743                 0.0913    4.3228     True\n",
      "\n",
      "Tableau des résultats custom sauvegardé dans : '/app/nmf_results/nmf_all_scenario_results_gensim_coherence.csv'\n",
      "\n",
      "--- Meilleur Résultat Global Custom Trouvé (basé sur l'ERREUR de reconstruction) ---\n",
      "Paramètres correspondants :\n",
      "  num_topic: 15\n",
      "  h_power: 1.0000\n",
      "  w_power: 0.7000\n",
      "  scenario: H_p=1.00_W_p=0.70\n",
      "  error: 8602.2419\n",
      "  coherence_npmi_gensim: 0.0851\n",
      "  duration: 4.0311\n",
      "  h_num_items_decay: 50\n",
      "  h_noise_level: 0.1500\n",
      "  w_noise_level: 0.1500\n",
      "  random_state_init: 42\n",
      "  random_state_solver: 1\n",
      "  alpha_W: 0.0000\n",
      "  alpha_H: 0.0000\n",
      "  l1_ratio: 0.0000\n",
      "  max_iter: 10000\n",
      "\n",
      "--- Rappel Comparaison Finale NNDSVD vs Meilleur Custom (par K, Cohérence Gensim w=30) ---\n",
      "(NNDSVD: rs=1. Best Custom: basé sur min ERREUR pour ce K)\n",
      "K    | NNDSVD Err   | NNDSVD Coh(G)  | Best Cust Err   | Best Cust Coh(G) \n",
      "------------------------------------------------------------------------\n",
      "7    | 8675.5232    | 0.1040         | 8675.5236       | 0.1040           \n",
      "12   | 8627.0285    | 0.0914         | 8627.0289       | 0.0914           \n",
      "15   | 8602.0467    | 0.0917         | 8602.2419       | 0.0851           \n",
      "\n",
      "===== Sauvegarde des Détails des Topics (Top 20 Mots) =====\n",
      "Les fichiers seront sauvegardés dans : '/app/nmf_results/topic_details'\n",
      "\n",
      "--- Sauvegarde Topics pour K=7 ---\n",
      "  -> Fichier Best Custom (par erreur) : 'topics_K7_best_custom_by_error.csv'\n",
      "  -> Fichier NNDSVD                 : 'topics_K7_nndsvd_rs1.csv'\n",
      "\n",
      "--- Sauvegarde Topics pour K=12 ---\n",
      "  -> Fichier Best Custom (par erreur) : 'topics_K12_best_custom_by_error.csv'\n",
      "  -> Fichier NNDSVD                 : 'topics_K12_nndsvd_rs1.csv'\n",
      "\n",
      "--- Sauvegarde Topics pour K=15 ---\n",
      "  -> Fichier Best Custom (par erreur) : 'topics_K15_best_custom_by_error.csv'\n",
      "  -> Fichier NNDSVD                 : 'topics_K15_nndsvd_rs1.csv'\n",
      "\n",
      "===== Poids Totaux des Topics et Coefficient de Variation (CV) =====\n",
      "\n",
      "--- Statistiques Poids pour K=7 ---\n",
      "  Statistiques Best Custom (par erreur) :\n",
      "    Poids Totaux par Topic      : [659.146, 212.652, 109.711, 136.044, 5.936, 438.852, 626.874]\n",
      "    Coefficient de Variation    : 0.775\n",
      "  Statistiques NNDSVD                 :\n",
      "    Poids Totaux par Topic      : [3756.295, 150.041, 2204.011, 1425.045, 3935.262, 2186.905, 3926.442]\n",
      "    Coefficient de Variation    : 0.533\n",
      "\n",
      "--- Statistiques Poids pour K=12 ---\n",
      "  Statistiques Best Custom (par erreur) :\n",
      "    Poids Totaux par Topic      : [530.841, 265.720, 177.750, 133.142, 104.887, 109.623, 116.492, 1.531, 47.090, 150.294, 355.768, 285.670]\n",
      "    Coefficient de Variation    : 0.742\n",
      "  Statistiques NNDSVD                 :\n",
      "    Poids Totaux par Topic      : [1834.612, 144.270, 1949.003, 1143.369, 2452.253, 2027.244, 3002.090, 3180.132, 3214.444, 1692.696, 2364.252, 2281.938]\n",
      "    Coefficient de Variation    : 0.398\n",
      "\n",
      "--- Statistiques Poids pour K=15 ---\n",
      "  Statistiques Best Custom (par erreur) :\n",
      "    Poids Totaux par Topic      : [215.655, 225.185, 167.526, 125.141, 101.416, 107.709, 159.124, 114.416, 60.051, 39.603, 61.327, 0.826, 99.545, 281.010, 446.447]\n",
      "    Coefficient de Variation    : 0.733\n",
      "  Statistiques NNDSVD                 :\n",
      "    Poids Totaux par Topic      : [1360.609, 144.195, 1902.144, 1095.966, 2280.468, 1967.088, 2759.755, 3361.760, 1701.091, 1482.969, 2315.475, 1382.620, 2322.241, 3159.850, 1206.815]\n",
      "    Coefficient de Variation    : 0.429\n",
      "\n",
      "Calcul des statistiques des topics terminé.\n",
      "\n",
      "Exploration NMF complète terminée (Cohérence via Gensim).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from tqdm.auto import tqdm # Use auto for better notebook/console detection\n",
    "\n",
    "# --- NOUVEAUX IMPORTS (GENSIM) ---\n",
    "# Assurez-vous que Gensim est installé: pip install gensim\n",
    "try:\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "    from gensim.corpora import Dictionary\n",
    "    print(\"Bibliothèque GENSIM (CoherenceModel, Dictionary) trouvée.\")\n",
    "    GENSIM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"\\nATTENTION : La bibliothèque GENSIM n'a pas pu être importée.\")\n",
    "    print(\"Veuillez l'installer via 'pip install gensim'.\")\n",
    "    print(\"Le calcul de la cohérence c_npmi sera désactivé.\")\n",
    "    CoherenceModel = None\n",
    "    Dictionary = None\n",
    "    GENSIM_AVAILABLE = False\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.utils import check_random_state\n",
    "# from sklearn.utils.extmath import safe_sparse_dot, squared_norm # Pas nécessaire ici\n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd # Ajouté pour une gestion plus facile des résultats finaux\n",
    "# Tqdm import was missing in the provided snippet, assuming it's needed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Supposons que ces variables sont déjà définies ---\n",
    "# ATTENTION: Les variables suivantes DOIVENT être définies dans une cellule précédente\n",
    "# ou au début de ce script pour qu'il fonctionne :\n",
    "# - tfidf: La matrice de données (np.ndarray ou sparse matrix, shape: n_samples, n_features)\n",
    "# - tfidf_feature_names: La liste ou array des noms de features/mots (longueur: n_features)\n",
    "#\n",
    "# Exemple de déclaration (si elles n'existent pas vraiment avant):\n",
    "# n_samples_in_tfidf = 100\n",
    "# n_features_in_tfidf = 500\n",
    "# np.random.seed(42)\n",
    "# tfidf = np.random.rand(n_samples_in_tfidf, n_features_in_tfidf) * 10\n",
    "# tfidf[tfidf < 0] = 0\n",
    "# tfidf_feature_names = np.array([f\"mot_{i}\" for i in range(n_features_in_tfidf)], dtype=object)\n",
    "# print(f\"Utilisation de données simulées: tfidf shape={tfidf.shape}, feature_names len={len(tfidf_feature_names)}\")\n",
    "\n",
    "\n",
    "alpha_W = 0.0 # Fourni\n",
    "alpha_H = 0.0 # Fourni\n",
    "l1_ratio = 0.5 # Ratio L1/L2 pour la régularisation (peut être fourni aussi)\n",
    "# --- Fin des variables supposées définies ---\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "topic_list = [7, 12, 15] # EXEMPLE: Tester pour 7, 12 et 15 topics\n",
    "\n",
    "# Scénarios de décroissance pour H (sur les items/mots)\n",
    "h_decay_powers = [0.7, 1.0, 1.5]\n",
    "h_num_items_decay = 50          # Nb items pour la décroissance dans H\n",
    "h_noise_level = 0.15            # Bruit/variation pour H\n",
    "\n",
    "# Scénarios de décroissance pour W (sur les topics/composantes)\n",
    "w_decay_powers = [0.7, 1.0, 1.5]\n",
    "w_noise_level = 0.15            # Bruit/variation pour W\n",
    "\n",
    "# Paramètres NMF et autres\n",
    "max_iter = 10000                 # Max iterations pour solveur NMF\n",
    "random_state_solver = 1         # Graine pour le solveur NMF (si applicable)\n",
    "random_state_init = 42          # Graine pour la génération des initialisations custom H et W\n",
    "random_state_nndsvd = 1         # Graine spécifique pour la comparaison NNDSVD\n",
    "epsilon = 1e-6                  # Petite valeur pour éviter les zéros\n",
    "output_dir = \"/app/nmf_results\" # Dossier général pour tous les résultats\n",
    "topic_output_dir = os.path.join(output_dir, \"topic_details\") # Sous-dossier pour les CSV de topics\n",
    "init_csv_dir = os.path.join(output_dir, \"initializations_csv\") # Sous-dossier pour les CSV d'initialisation\n",
    "\n",
    "\n",
    "# --- Assurez-vous que ces variables sont définies AVANT ---\n",
    "# Exemple de valeurs par défaut, REMPLACEZ par vos vraies valeurs\n",
    "# topic_list = [5, 10, 15]\n",
    "# h_decay_powers = [0.5, 1.0]\n",
    "# w_decay_powers = [0.0, 0.5]\n",
    "# tfidf = ... # Votre matrice TF-IDF (sparse ou dense)\n",
    "# tfidf_feature_names = ... # Liste/Array des noms de features (mots)\n",
    "# tokenized_documents = ... # LISTE DE LISTES DE TOKENS (ex: [['mot1', 'mot2'], ['mot3', 'mot1']])\n",
    "# create_decreasing_H = ... # Votre fonction\n",
    "# create_decreasing_W = ... # Votre fonction\n",
    "# save_topics_to_file = ... # Votre fonction\n",
    "# h_num_items_decay = 10\n",
    "# h_noise_level = 0.01\n",
    "# w_noise_level = 0.01\n",
    "\n",
    "alpha_W = 0.0 # Ou votre valeur de régularisation\n",
    "alpha_H = 0.0 # Ou votre valeur de régularisation\n",
    "l1_ratio = 0.0 # Ou votre valeur\n",
    "\n",
    "n_top_words = 10 # Nombre de mots clés à extraire/sauvegarder ET utiliser pour définir les topics pour Gensim\n",
    "COHERENCE_WINDOW_SIZE = 30 # Fenêtre pour le calcul de cohérence NPMI avec Gensim\n",
    "\n",
    "# --- Vérifications initiales des prérequis ---\n",
    "required_vars = ['topic_list', 'h_decay_powers', 'w_decay_powers', 'tfidf',\n",
    "                 'tfidf_feature_names', 'tokenized_documents', #'init_csv_dir', # Optionnel\n",
    "                 'output_dir', 'topic_output_dir', 'create_decreasing_H',\n",
    "                 'create_decreasing_W', 'save_topics_to_file', 'n_top_words']\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        raise NameError(f\"Variable essentielle '{var}' n'est pas définie.\")\n",
    "\n",
    "if not GENSIM_AVAILABLE:\n",
    "     print(\"\\nATTENTION: Gensim n'est pas chargé. Le calcul de la cohérence sera désactivé.\")\n",
    "else:\n",
    "    # Vérification spécifique pour Gensim: tokenized_documents doit être list[list[str]]\n",
    "    if 'tokenized_documents' not in globals():\n",
    "         raise NameError(\"Variable essentielle 'tokenized_documents' non définie (requise pour Gensim).\")\n",
    "    if not isinstance(tokenized_documents, list) or not all(isinstance(doc, list) for doc in tokenized_documents):\n",
    "        raise TypeError(\"`tokenized_documents` doit être une liste de listes de strings pour GENSIM.\")\n",
    "    # Vérification cohérence tokenized_documents et tfidf (nombre de documents)\n",
    "    if 'tfidf' in globals():\n",
    "        if len(tokenized_documents) != tfidf.shape[0]:\n",
    "             print(f\"ATTENTION: Le nombre de documents dans 'tokenized_documents' ({len(tokenized_documents)}) \"\n",
    "                   f\"ne correspond pas au nombre de lignes dans 'tfidf' ({tfidf.shape[0]}). \"\n",
    "                   f\"La cohérence Gensim pourrait être basée sur un nombre différent de textes.\")\n",
    "\n",
    "# --- Création du Dictionnaire Gensim (une seule fois) ---\n",
    "gensim_dictionary = None\n",
    "if GENSIM_AVAILABLE:\n",
    "    print(\"\\nCréation du dictionnaire Gensim à partir de tokenized_documents...\")\n",
    "    try:\n",
    "        gensim_dictionary = Dictionary(tokenized_documents)\n",
    "        # Optionnel: Filtrer\n",
    "        # gensim_dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "        print(f\"Dictionnaire Gensim créé avec {len(gensim_dictionary)} tokens uniques.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERREUR lors de la création du dictionnaire Gensim : {e}\")\n",
    "        print(\"Le calcul de la cohérence c_npmi via Gensim sera désactivé.\")\n",
    "        GENSIM_AVAILABLE = False # Désactiver si le dictionnaire échoue\n",
    "        CoherenceModel = None\n",
    "        Dictionary = None\n",
    "\n",
    "\n",
    "# --- Fonctions d'aide (Extraction + Cohérence Gensim) ---\n",
    "\n",
    "# --- Initialisation des variables de suivi ---\n",
    "overall_best_error = float('inf')\n",
    "overall_best_params = {}\n",
    "all_results_list = []\n",
    "\n",
    "# Dictionnaires pour stocker les résultats finaux par K\n",
    "best_custom_H_matrices = {}\n",
    "nndsvd_H_matrices = {}\n",
    "best_custom_models = {} # Stocke les objets model custom (celui avec la meilleure erreur)\n",
    "nndsvd_models = {}      # Stocke les objets model NNDSVD\n",
    "best_custom_coherence = {} # Stocker la cohérence du meilleur modèle custom (par erreur) par K\n",
    "nndsvd_coherence = {} # Stocker la cohérence NNDSVD par K\n",
    "\n",
    "# --- Boucle sur le nombre de topics ---\n",
    "for num_topic in tqdm(topic_list, desc=\"PROCESSUS GLOBAL (Nb Topics)\"):\n",
    "\n",
    "    print(f\"\\n===== Test pour num_topic = {num_topic} =====\")\n",
    "    n_samples, n_features = tfidf.shape # Récupérer dimensions\n",
    "\n",
    "    # Vérifier tfidf_feature_names (cohérence) - déjà fait mais bon à garder ici aussi\n",
    "    if len(tfidf_feature_names) != n_features:\n",
    "         print(f\"ATTENTION K={num_topic}: Longueur 'tfidf_feature_names' ({len(tfidf_feature_names)}) \"\n",
    "               f\"!= nb features tfidf ({n_features}). Extraction de mots échouera.\")\n",
    "         continue # Sauter ce K si les features ne correspondent pas\n",
    "\n",
    "    scenario_combinations = list(itertools.product(h_decay_powers, w_decay_powers))\n",
    "\n",
    "    # Variables pour suivre le meilleur run custom pour CE num_topic\n",
    "    current_best_model_for_topic = None\n",
    "    current_best_error_for_topic = float('inf')\n",
    "    current_best_coherence_for_topic = -float('inf') # Init à -inf car on maximise cohérence (mais on stocke celle du meilleur par erreur)\n",
    "    current_best_scenario_name_for_topic = None\n",
    "\n",
    "    pbar_scenarios = tqdm(scenario_combinations, desc=f\"  Scénarios Custom (K={num_topic})\", leave=False, unit=\"scenario\")\n",
    "    for h_power, w_power in pbar_scenarios:\n",
    "\n",
    "        scenario_name = f\"H_p={h_power:.2f}_W_p={w_power:.2f}\"\n",
    "        pbar_scenarios.set_postfix_str(scenario_name, refresh=True)\n",
    "\n",
    "        # 1. Créer les matrices d'initialisation\n",
    "        current_H_init, current_W_init = None, None\n",
    "        init_creation_success = False\n",
    "        try:\n",
    "            current_H_init = create_decreasing_H(\n",
    "                num_topic, n_features, h_num_items_decay, h_power, h_noise_level, random_state_init, epsilon\n",
    "            )\n",
    "            current_W_init = create_decreasing_W(\n",
    "                n_samples, num_topic, w_power, w_noise_level, random_state_init, epsilon\n",
    "            )\n",
    "            # Vérifier les dimensions AVANT de lancer NMF\n",
    "            if current_W_init.shape != (n_samples, num_topic) or current_H_init.shape != (num_topic, n_features):\n",
    "                 raise ValueError(f\"Dims init custom INCOHÉRENTES: W={current_W_init.shape} (attendu {(n_samples, num_topic)}), H={current_H_init.shape} (attendu {(num_topic, n_features)})\")\n",
    "            init_creation_success = True\n",
    "        except Exception as e:\n",
    "            print(f\"\\n    ERREUR création init pour {scenario_name} (K={num_topic}): {e}\")\n",
    "\n",
    "        # 2. Sauvegarder les matrices d'initialisation (Optionnel - désactivé ici)\n",
    "        # if init_creation_success and init_csv_dir:\n",
    "        #     # ... votre code de sauvegarde CSV ...\n",
    "        #     pass\n",
    "\n",
    "        # 3. Exécuter NMF\n",
    "        model = None\n",
    "        error = float('inf')\n",
    "        coherence_score = np.nan # Initialiser score cohérence\n",
    "        duration = 0\n",
    "        success = False\n",
    "        topic_words_custom = [] # Initialiser liste de mots\n",
    "\n",
    "        if init_creation_success:\n",
    "            model = NMF(\n",
    "                n_components=num_topic, init='custom', solver='cd',\n",
    "                random_state=random_state_solver, max_iter=max_iter,\n",
    "                alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio,\n",
    "                # tol=1e-4 # Peut aider parfois\n",
    "            )\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # NMF fit avec les matrices fournies\n",
    "                model.fit(tfidf, W=current_W_init.copy(), H=current_H_init.copy())\n",
    "                error = model.reconstruction_err_\n",
    "                duration = time.time() - start_time\n",
    "                success = True\n",
    "\n",
    "                # --- NOUVEAU: Calcul de la cohérence GENSIM APRÈS fit réussi ---\n",
    "                if success:\n",
    "                    #print(f\"    NMF Custom {scenario_name} K={num_topic} réussi. Calcul cohérence (Gensim)...\")\n",
    "                    # Extraire les mots clés\n",
    "                    topic_words_custom = extract_top_words(model.components_, tfidf_feature_names, n_top_words)\n",
    "                    if topic_words_custom and GENSIM_AVAILABLE: # Si extraction ok ET Gensim dispo\n",
    "                        # Calculer la cohérence avec Gensim\n",
    "                         coherence_score = calculate_coherence_gensim( # Utiliser la nouvelle fonction\n",
    "                             topic_words_custom,\n",
    "                             tokenized_documents,\n",
    "                             gensim_dictionary,         # Passer le dictionnaire\n",
    "                             COHERENCE_WINDOW_SIZE      # Passer la window size\n",
    "                             # measure='c_npmi' # est la valeur par défaut dans la fonction\n",
    "                         )\n",
    "                         #print(f\"    -> Cohérence C_NPMI (Gensim): {coherence_score:.4f}\") # Log optionnel ici, fait dans comparaison\n",
    "                    elif not topic_words_custom:\n",
    "                        print(f\"    WARN: Échec extraction mots pour cohérence (custom {scenario_name} K={num_topic}).\")\n",
    "                    # Si Gensim non dispo, coherence_score reste NaN (initialisé)\n",
    "\n",
    "            except ValueError as ve:\n",
    "                 # Gère spécifiquement les erreurs de dimensions ou autres ValueError de NMF/fit\n",
    "                 print(f\"\\n    ERREUR (ValueError) NMF {scenario_name} (K={num_topic}): {ve}\")\n",
    "                 duration = time.time() - start_time\n",
    "                 success = False\n",
    "            except Exception as e:\n",
    "                print(f\"\\n    ERREUR (Autre) NMF {scenario_name} (K={num_topic}): {e}\")\n",
    "                duration = time.time() - start_time\n",
    "                success = False\n",
    "\n",
    "        # 4. Stocker les résultats du run (erreur ET cohérence Gensim)\n",
    "        result_data = {\n",
    "            'num_topic': num_topic, 'h_power': h_power, 'w_power': w_power,\n",
    "            'scenario': scenario_name,\n",
    "            'error': error if success else np.nan,\n",
    "            'coherence_npmi_gensim': coherence_score, # Nom de colonne mis à jour\n",
    "            'duration': duration, 'success': success\n",
    "        }\n",
    "        all_results_list.append(result_data)\n",
    "\n",
    "        # 5. Mettre à jour le meilleur modèle pour ce `num_topic` (basé sur l'ERREUR)\n",
    "        if success and error < current_best_error_for_topic:\n",
    "            # print(f\"    -> Nouveau meilleur custom pour K={num_topic} (par erreur): {scenario_name} (Err: {error:.4f}, Coh(Gensim): {coherence_score:.4f})\") # Log optionnel\n",
    "            current_best_error_for_topic = error\n",
    "            current_best_coherence_for_topic = coherence_score # Stocker aussi sa cohérence\n",
    "            current_best_model_for_topic = model # Stocker l'objet modèle\n",
    "            current_best_scenario_name_for_topic = scenario_name\n",
    "\n",
    "        # 6. Mettre à jour le meilleur modèle global (basé sur l'ERREUR)\n",
    "        if success and error < overall_best_error:\n",
    "            overall_best_error = error\n",
    "            overall_best_params = {\n",
    "                'num_topic': num_topic, 'h_power': h_power, 'w_power': w_power,\n",
    "                'scenario': scenario_name, 'error': error,\n",
    "                'coherence_npmi_gensim': coherence_score, # Nom de colonne mis à jour\n",
    "                'duration': duration,\n",
    "                'h_num_items_decay': h_num_items_decay, 'h_noise_level': h_noise_level,\n",
    "                'w_noise_level': w_noise_level,\n",
    "                'random_state_init': random_state_init, 'random_state_solver': random_state_solver,\n",
    "                'alpha_W': alpha_W, 'alpha_H': alpha_H, 'l1_ratio': l1_ratio, 'max_iter': max_iter\n",
    "            }\n",
    "\n",
    "    # --- Fin de la boucle sur les combinaisons H/W pour un K donné ---\n",
    "    if current_best_scenario_name_for_topic:\n",
    "        # Récupérer le meilleur modèle stocké et sa cohérence\n",
    "        best_model = current_best_model_for_topic\n",
    "        best_coherence = current_best_coherence_for_topic\n",
    "        print(f\"  Meilleur scénario custom pour K={num_topic} (par erreur): '{current_best_scenario_name_for_topic}' \"\n",
    "              f\"(Erreur: {current_best_error_for_topic:.4f}, Cohérence(Gensim): {best_coherence:.4f})\")\n",
    "        # Stocker le meilleur modèle, sa matrice H, et sa cohérence DÉFINITIVEMENT pour ce K\n",
    "        if best_model is not None:\n",
    "            best_custom_models[num_topic] = best_model\n",
    "            best_custom_H_matrices[num_topic] = best_model.components_.copy()\n",
    "            best_custom_coherence[num_topic] = best_coherence # Stocker la cohérence associée\n",
    "    else:\n",
    "        print(f\"  Aucun run NMF custom réussi pour K={num_topic} avec les initialisations testées.\")\n",
    "        best_custom_coherence[num_topic] = np.nan # Marquer comme non calculé/échoué\n",
    "\n",
    "# --- Fin de la boucle sur num_topic (Initialisations custom) ---\n",
    "\n",
    "\n",
    "# --- Convertir la liste des résultats en DataFrame pandas ---\n",
    "results_df = pd.DataFrame(all_results_list)\n",
    "\n",
    "# Récupérer les meilleures erreurs custom pour la comparaison directe\n",
    "if not results_df.empty and 'error' in results_df.columns:\n",
    "    best_custom_errors = results_df[results_df['success']].groupby('num_topic')['error'].min().to_dict()\n",
    "else:\n",
    "    best_custom_errors = {}\n",
    "# Note: best_custom_coherence[k] contient déjà la cohérence Gensim du modèle ayant la meilleure ERREUR pour K=k.\n",
    "\n",
    "\n",
    "# --- NOUVELLE ÉTAPE : Comparaison avec NNDSVD (avec calcul cohérence Gensim) ---\n",
    "print(\"\\n===== Comparaison avec l'initialisation NNDSVD (Cohérence via Gensim) =====\")\n",
    "nndsvd_results = {} # Stocke les infos complètes par K pour NNDSVD\n",
    "\n",
    "pbar_nndsvd = tqdm(topic_list, desc=\"  NMF NNDSVD (par K)\", leave=False, unit=\"K\")\n",
    "for num_topic in pbar_nndsvd:\n",
    "    pbar_nndsvd.set_postfix_str(f\"K={num_topic}\", refresh=True)\n",
    "\n",
    "    n_samples, n_features = tfidf.shape # Répété pour clarté\n",
    "\n",
    "    model_nndsvd = NMF(\n",
    "        n_components=num_topic, init='nndsvd', random_state=random_state_nndsvd,\n",
    "        solver='cd', max_iter=max_iter, alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio\n",
    "        # tol=1e-4\n",
    "    )\n",
    "\n",
    "    start_time_nndsvd = time.time()\n",
    "    success_nndsvd = False\n",
    "    error_nndsvd = float('inf')\n",
    "    coherence_nndsvd = np.nan # Initialiser cohérence NNDSVD\n",
    "    duration_nndsvd = 0\n",
    "    topic_words_nndsvd = []\n",
    "\n",
    "    try:\n",
    "        # print(f\"  Running NNDSVD for K={num_topic}...\") # Moins verbeux\n",
    "        model_nndsvd.fit(tfidf)\n",
    "        error_nndsvd = model_nndsvd.reconstruction_err_\n",
    "        duration_nndsvd = time.time() - start_time_nndsvd\n",
    "        success_nndsvd = True\n",
    "        # print(f\"    NNDSVD K={num_topic} réussi. Calcul cohérence (Gensim)...\") # Moins verbeux\n",
    "\n",
    "        # Stocker le modèle et H si succès\n",
    "        nndsvd_models[num_topic] = model_nndsvd\n",
    "        nndsvd_H_matrices[num_topic] = model_nndsvd.components_.copy()\n",
    "\n",
    "        # --- NOUVEAU: Calcul cohérence GENSIM pour NNDSVD ---\n",
    "        if success_nndsvd:\n",
    "             # Extraire les mots clés\n",
    "             topic_words_nndsvd = extract_top_words(model_nndsvd.components_, tfidf_feature_names, n_top_words)\n",
    "             if topic_words_nndsvd and GENSIM_AVAILABLE: # Si extraction ok ET Gensim dispo\n",
    "                 # Calculer la cohérence Gensim\n",
    "                 coherence_nndsvd = calculate_coherence_gensim( # Utiliser la nouvelle fonction\n",
    "                     topic_words_nndsvd,\n",
    "                     tokenized_documents,\n",
    "                     gensim_dictionary,         # Passer le dictionnaire\n",
    "                     COHERENCE_WINDOW_SIZE      # Passer la window size\n",
    "                 )\n",
    "                 # print(f\"    -> Cohérence C_NPMI (Gensim): {coherence_nndsvd:.4f}\") # Log optionnel\n",
    "             elif not topic_words_nndsvd:\n",
    "                 print(f\"    WARN: Échec extraction mots pour cohérence (NNDSVD K={num_topic}).\")\n",
    "             # Si Gensim non dispo, coherence_nndsvd reste NaN\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n    ERREUR NMF NNDSVD pour K={num_topic}: {e}\")\n",
    "        duration_nndsvd = time.time() - start_time_nndsvd # Durée même si erreur\n",
    "        success_nndsvd = False\n",
    "\n",
    "    # Stocker les résultats NNDSVD (incluant cohérence Gensim)\n",
    "    nndsvd_results[num_topic] = {\n",
    "        'error': error_nndsvd if success_nndsvd else np.nan,\n",
    "        'coherence_npmi_gensim': coherence_nndsvd, # Nom de colonne mis à jour\n",
    "        'duration': duration_nndsvd,\n",
    "        'success': success_nndsvd\n",
    "    }\n",
    "    # Aussi stocker séparément pour accès facile dans la synthèse\n",
    "    nndsvd_coherence[num_topic] = coherence_nndsvd\n",
    "\n",
    "\n",
    "    # Afficher la comparaison immédiate (erreur et cohérence Gensim)\n",
    "    best_custom_err_k = best_custom_errors.get(num_topic, float('inf'))\n",
    "    best_custom_coh_k = best_custom_coherence.get(num_topic, np.nan) # Récupérer cohérence du meilleur custom (par erreur)\n",
    "\n",
    "    print(f\"  Comparaison K={num_topic}:\")\n",
    "    err_nndsvd_str = f\"{error_nndsvd:.4f}\" if success_nndsvd else \"ÉCHEC\"\n",
    "    coh_nndsvd_str = f\"{coherence_nndsvd:.4f}\" if not np.isnan(coherence_nndsvd) else \"N/A\"\n",
    "    print(f\"    NNDSVD (rs={random_state_nndsvd}): Err={err_nndsvd_str}, Coh(Gensim)={coh_nndsvd_str} (Dur: {duration_nndsvd:.2f}s)\")\n",
    "\n",
    "    if not np.isnan(best_custom_err_k) and best_custom_err_k != float('inf'):\n",
    "        best_custom_err_str = f\"{best_custom_err_k:.4f}\"\n",
    "        best_custom_coh_str = f\"{best_custom_coh_k:.4f}\" if not np.isnan(best_custom_coh_k) else \"N/A\"\n",
    "        print(f\"    Best Custom(err): Err={best_custom_err_str}, Coh(Gensim)={best_custom_coh_str}\")\n",
    "        # Comparaison basée sur l'erreur\n",
    "        if success_nndsvd and best_custom_err_k < error_nndsvd:\n",
    "             print(f\"    -> Meilleur Custom supérieur (par erreur).\")\n",
    "        elif success_nndsvd:\n",
    "             print(f\"    -> NNDSVD supérieur ou égal (par erreur).\")\n",
    "        elif not success_nndsvd: # NNDSVD échoue, Custom réussit\n",
    "             print(f\"    -> Meilleur Custom a réussi là où NNDSVD a échoué.\")\n",
    "    else: # Aucun run custom réussi pour ce K\n",
    "        if success_nndsvd:\n",
    "            print(f\"    (NNDSVD a réussi, aucun run custom réussi pour comparer)\")\n",
    "        else: # Aucun des deux n'a réussi\n",
    "            print(f\"    (Ni Custom ni NNDSVD n'ont réussi pour K={num_topic})\")\n",
    "\n",
    "\n",
    "# --- Synthèse Finale ---\n",
    "print(\"\\n===== Synthèse Globale des Initialisations Custom (Cohérence via Gensim) =====\")\n",
    "\n",
    "# Afficher le DataFrame des résultats custom (avec cohérence Gensim)\n",
    "print(\"\\n--- Tableau Récapitulatif des Résultats Custom ---\")\n",
    "if not results_df.empty:\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    # Trier par topic, puis par erreur (ou cohérence Gensim si vous préférez)\n",
    "    results_df_sorted = results_df.sort_values(by=['num_topic', 'error'], ascending=[True, True])\n",
    "    # results_df_sorted = results_df.sort_values(by=['num_topic', 'coherence_npmi_gensim'], ascending=[True, False]) # Tri par cohérence (décroissant)\n",
    "    print(results_df_sorted.to_string(index=False, float_format='%.4f', na_rep='N/A'))\n",
    "\n",
    "    # Sauvegarder le DataFrame en CSV\n",
    "    try:\n",
    "        # Mettre à jour le nom de fichier pour refléter l'utilisation de Gensim\n",
    "        results_filename = os.path.join(output_dir, \"nmf_all_scenario_results_gensim_coherence.csv\")\n",
    "        results_df_sorted.to_csv(results_filename, index=False, sep=',', float_format='%.8f', na_rep='NaN')\n",
    "        print(f\"\\nTableau des résultats custom sauvegardé dans : '{results_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nATTENTION : Échec sauvegarde CSV des résultats custom : {e}\")\n",
    "else:\n",
    "    print(\"Aucun résultat à afficher ou sauvegarder pour les initialisations custom.\")\n",
    "\n",
    "\n",
    "# Afficher les paramètres du meilleur run global custom (basé sur l'ERREUR)\n",
    "print(\"\\n--- Meilleur Résultat Global Custom Trouvé (basé sur l'ERREUR de reconstruction) ---\")\n",
    "if overall_best_params:\n",
    "    print(\"Paramètres correspondants :\")\n",
    "    for key, val in overall_best_params.items():\n",
    "        if isinstance(val, float): print(f\"  {key}: {val:.4f}\")\n",
    "        else: print(f\"  {key}: {val}\")\n",
    "    # Note: overall_best_params contient aussi 'coherence_npmi_gensim'\n",
    "else:\n",
    "    print(\"Aucun run NMF custom réussi trouvé sur l'ensemble des configurations testées.\")\n",
    "\n",
    "\n",
    "# Rappel de la comparaison NNDSVD (avec cohérence Gensim)\n",
    "print(f\"\\n--- Rappel Comparaison Finale NNDSVD vs Meilleur Custom (par K, Cohérence Gensim w={COHERENCE_WINDOW_SIZE}) ---\")\n",
    "if nndsvd_results:\n",
    "    print(f\"(NNDSVD: rs={random_state_nndsvd}. Best Custom: basé sur min ERREUR pour ce K)\")\n",
    "    # Ajustement des largeurs de colonnes pour la cohérence\n",
    "    print(f\"{'K':<4} | {'NNDSVD Err':<12} | {'NNDSVD Coh(G)':<14} | {'Best Cust Err':<15} | {'Best Cust Coh(G)':<17}\")\n",
    "    print(\"-\" * (4 + 12 + 14 + 15 + 17 + 10)) # Ajuster la longueur du séparateur\n",
    "    for k in topic_list: # Itérer sur topic_list pour garder l'ordre\n",
    "        # Utiliser les résultats stockés pour NNDSVD\n",
    "        res_nndsvd = nndsvd_results.get(k, {'success': False, 'error': np.nan, 'coherence_npmi_gensim': np.nan})\n",
    "        # Utiliser les résultats stockés pour le meilleur Custom (par erreur)\n",
    "        best_cust_err = best_custom_errors.get(k, np.nan)\n",
    "        best_cust_coh = best_custom_coherence.get(k, np.nan) # Cohérence Gensim associée\n",
    "\n",
    "        # Formatage des chaînes de sortie\n",
    "        nndsvd_err_str = f\"{res_nndsvd['error']:.4f}\" if res_nndsvd['success'] else \"ÉCHEC\"\n",
    "        nndsvd_coh_str = f\"{res_nndsvd['coherence_npmi_gensim']:.4f}\" if not np.isnan(res_nndsvd['coherence_npmi_gensim']) else \"N/A\"\n",
    "        cust_err_str = f\"{best_cust_err:.4f}\" if not np.isnan(best_cust_err) else \"N/A\"\n",
    "        cust_coh_str = f\"{best_cust_coh:.4f}\" if not np.isnan(best_cust_coh) else \"N/A\"\n",
    "\n",
    "        # Affichage aligné\n",
    "        print(f\"{k:<4} | {nndsvd_err_str:<12} | {nndsvd_coh_str:<14} | {cust_err_str:<15} | {cust_coh_str:<17}\")\n",
    "\n",
    "else:\n",
    "    print(\"Aucun résultat NNDSVD n'a été calculé (la boucle a peut-être échoué).\")\n",
    "\n",
    "\n",
    "# --- Sauvegarde des Matrices H Formatées (Top Mots) ---\n",
    "# Cette partie reste identique car elle dépend des matrices H stockées et de tfidf_feature_names\n",
    "print(f\"\\n===== Sauvegarde des Détails des Topics (Top {n_top_words} Mots) =====\")\n",
    "print(f\"Les fichiers seront sauvegardés dans : '{topic_output_dir}'\")\n",
    "\n",
    "# Vérification finale avant sauvegarde (devrait être OK si le script est arrivé ici)\n",
    "feature_names_ok = ('tfidf_feature_names' in globals() and\n",
    "                    'tfidf' in globals() and\n",
    "                    len(tfidf_feature_names) == tfidf.shape[1])\n",
    "\n",
    "if not feature_names_ok:\n",
    "    print(\"Sauvegarde des détails des topics ANNULÉE en raison d'une incohérence détectée \"\n",
    "          \"entre tfidf et tfidf_feature_names.\")\n",
    "else:\n",
    "    local_feature_names = tfidf_feature_names # Utilisation sûre\n",
    "    os.makedirs(topic_output_dir, exist_ok=True) # Assurer que le dossier existe\n",
    "\n",
    "    for k in topic_list:\n",
    "        print(f\"\\n--- Sauvegarde Topics pour K={k} ---\")\n",
    "        saved_custom = False\n",
    "        saved_nndsvd = False\n",
    "\n",
    "        # Sauvegarde meilleur custom pour K (basé sur erreur)\n",
    "        if k in best_custom_H_matrices:\n",
    "            H_custom = best_custom_H_matrices[k]\n",
    "            filepath_custom = os.path.join(topic_output_dir, f\"topics_K{k}_best_custom_by_error.csv\")\n",
    "            try:\n",
    "                # Assurez-vous que save_topics_to_file est bien définie\n",
    "                save_topics_to_file(H_custom, local_feature_names, filepath_custom, n_top_words)\n",
    "                print(f\"  -> Fichier Best Custom (par erreur) : '{os.path.basename(filepath_custom)}'\")\n",
    "                saved_custom = True\n",
    "            except NameError:\n",
    "                 print(\"  ERREUR: La fonction 'save_topics_to_file' n'est pas définie!\")\n",
    "                 break # Inutile de continuer si la fonction manque\n",
    "            except Exception as e:\n",
    "                print(f\"  ERREUR sauvegarde topics best custom K={k}: {e}\")\n",
    "        else:\n",
    "            print(\"  (Pas de matrice H best custom trouvée pour K={k})\")\n",
    "\n",
    "        # Sauvegarde NNDSVD pour K\n",
    "        if k in nndsvd_H_matrices:\n",
    "            H_nndsvd = nndsvd_H_matrices[k]\n",
    "            filepath_nndsvd = os.path.join(topic_output_dir, f\"topics_K{k}_nndsvd_rs{random_state_nndsvd}.csv\")\n",
    "            try:\n",
    "                 # Assurez-vous que save_topics_to_file est bien définie\n",
    "                save_topics_to_file(H_nndsvd, local_feature_names, filepath_nndsvd, n_top_words)\n",
    "                print(f\"  -> Fichier NNDSVD                 : '{os.path.basename(filepath_nndsvd)}'\")\n",
    "                saved_nndsvd = True\n",
    "            except NameError:\n",
    "                 print(\"  ERREUR: La fonction 'save_topics_to_file' n'est pas définie!\")\n",
    "                 break # Inutile de continuer si la fonction manque\n",
    "            except Exception as e:\n",
    "                print(f\"  ERREUR sauvegarde topics NNDSVD K={k}: {e}\")\n",
    "        else:\n",
    "            print(f\"  (Pas de matrice H NNDSVD trouvée pour K={k})\")\n",
    "\n",
    "        if not saved_custom and not saved_nndsvd:\n",
    "                print(\"  (Aucune matrice H à sauvegarder pour ce K)\")\n",
    "\n",
    "\n",
    "# --- Calcul et Affichage des Poids Totaux des Topics et CV ---\n",
    "# Cette partie reste identique car elle utilise les modèles stockés\n",
    "print(\"\\n===== Poids Totaux des Topics et Coefficient de Variation (CV) =====\")\n",
    "\n",
    "if 'tfidf' not in globals():\n",
    "     print(\"\\nATTENTION CRITIQUE: Variable `tfidf` non définie. Calculs de W impossibles.\")\n",
    "else:\n",
    "    # Utiliser les modèles stockés\n",
    "    if 'best_custom_models' not in globals(): best_custom_models = {}\n",
    "    if 'nndsvd_models' not in globals(): nndsvd_models = {}\n",
    "\n",
    "    for k in topic_list:\n",
    "        print(f\"\\n--- Statistiques Poids pour K={k} ---\")\n",
    "        stats_calculated = False\n",
    "\n",
    "        # Stats pour le meilleur custom (par erreur)\n",
    "        if k in best_custom_models:\n",
    "            model_custom = best_custom_models[k]\n",
    "            try:\n",
    "                W_custom = model_custom.transform(tfidf)\n",
    "                custom_topic_weights = W_custom.sum(axis=0)\n",
    "                mean_weight = np.mean(custom_topic_weights)\n",
    "                cv_custom = np.std(custom_topic_weights) / mean_weight if mean_weight > 1e-9 else 0.0\n",
    "\n",
    "                print(\"  Statistiques Best Custom (par erreur) :\")\n",
    "                weights_str = [f\"{w:.3f}\" for w in custom_topic_weights]\n",
    "                print(f\"    Poids Totaux par Topic      : [{', '.join(weights_str)}]\")\n",
    "                print(f\"    Coefficient de Variation    : {cv_custom:.3f}\")\n",
    "                stats_calculated = True\n",
    "            except Exception as e:\n",
    "                print(f\"    Erreur calcul/transform W stats pour best custom (K={k}): {e}\")\n",
    "        else:\n",
    "            print(\"  (Pas de modèle 'best custom' trouvé pour K={k})\")\n",
    "\n",
    "        # Stats pour NNDSVD\n",
    "        if k in nndsvd_models:\n",
    "            model_nndsvd = nndsvd_models[k]\n",
    "            try:\n",
    "                W_nndsvd = model_nndsvd.transform(tfidf)\n",
    "                nndsvd_topic_weights = W_nndsvd.sum(axis=0)\n",
    "                mean_weight = np.mean(nndsvd_topic_weights)\n",
    "                cv_nndsvd = np.std(nndsvd_topic_weights) / mean_weight if mean_weight > 1e-9 else 0.0\n",
    "\n",
    "                print(\"  Statistiques NNDSVD                 :\")\n",
    "                weights_str = [f\"{w:.3f}\" for w in nndsvd_topic_weights]\n",
    "                print(f\"    Poids Totaux par Topic      : [{', '.join(weights_str)}]\")\n",
    "                print(f\"    Coefficient de Variation    : {cv_nndsvd:.3f}\")\n",
    "                stats_calculated = True\n",
    "            except Exception as e:\n",
    "                print(f\"    Erreur calcul/transform W stats pour NNDSVD (K={k}): {e}\")\n",
    "        else:\n",
    "            print(f\"  (Pas de modèle NNDSVD trouvé pour K={k})\")\n",
    "\n",
    "        if not stats_calculated:\n",
    "            print(\"  (Aucun modèle disponible pour calculer les statistiques de poids pour ce K)\")\n",
    "\n",
    "print(\"\\nCalcul des statistiques des topics terminé.\")\n",
    "print(\"\\nExploration NMF complète terminée (Cohérence via Gensim).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Créer les matrices d'initialisation\n",
    "current_H_init, current_W_init = None, None\n",
    "init_creation_success = False\n",
    "try:\n",
    "    current_H_init = create_decreasing_H(\n",
    "        num_topic, n_features, h_num_items_decay, h_power, h_noise_level, 42, epsilon\n",
    "    )\n",
    "    current_W_init = create_decreasing_W(\n",
    "        n_samples, num_topic, w_power, w_noise_level, 42, epsilon\n",
    "    )\n",
    "    # Vérifier les dimensions AVANT de lancer NMF\n",
    "    if current_W_init.shape != (n_samples, num_topic) or current_H_init.shape != (num_topic, n_features):\n",
    "            raise ValueError(f\"Dims init custom INCOHÉRENTES: W={current_W_init.shape} (attendu {(n_samples, num_topic)}), H={current_H_init.shape} (attendu {(num_topic, n_features)})\")\n",
    "    init_creation_success = True\n",
    "except Exception as e:\n",
    "    print(f\"\\n    ERREUR création init pour {scenario_name} (K={num_topic}): {e}\")\n",
    "\n",
    "# 2. Sauvegarder les matrices d'initialisation (Optionnel - désactivé ici)\n",
    "# if init_creation_success and init_csv_dir:\n",
    "#     # ... votre code de sauvegarde CSV ...\n",
    "#     pass\n",
    "\n",
    "# 3. Exécuter NMF\n",
    "model = None\n",
    "error = float('inf')\n",
    "coherence_score = np.nan # Initialiser score cohérence\n",
    "duration = 0\n",
    "success = False\n",
    "topic_words_custom = [] # Initialiser liste de mots\n",
    "\n",
    "if init_creation_success:\n",
    "    model = NMF(\n",
    "        n_components=num_topic, init='custom', solver='cd',\n",
    "        random_state=1, max_iter=max_iter,\n",
    "        alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio,\n",
    "        # tol=1e-4 # Peut aider parfois\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # NMF fit avec les matrices fournies\n",
    "        model.fit(tfidf, W=current_W_init.copy(), H=current_H_init.copy())\n",
    "        error = model.reconstruction_err_\n",
    "        duration = time.time() - start_time\n",
    "        success = True\n",
    "\n",
    "        # --- NOUVEAU: Calcul de la cohérence GENSIM APRÈS fit réussi ---\n",
    "        if success:\n",
    "            #print(f\"    NMF Custom {scenario_name} K={num_topic} réussi. Calcul cohérence (Gensim)...\")\n",
    "            # Extraire les mots clés\n",
    "            topic_words_custom = extract_top_words(model.components_, tfidf_feature_names, n_top_words)\n",
    "            if topic_words_custom and GENSIM_AVAILABLE: # Si extraction ok ET Gensim dispo\n",
    "                # Calculer la cohérence avec Gensim\n",
    "                    coherence_score = calculate_coherence_gensim( # Utiliser la nouvelle fonction\n",
    "                        topic_words_custom,\n",
    "                        tokenized_documents,\n",
    "                        gensim_dictionary,         # Passer le dictionnaire\n",
    "                        COHERENCE_WINDOW_SIZE      # Passer la window size\n",
    "                        # measure='c_npmi' # est la valeur par défaut dans la fonction\n",
    "                    )\n",
    "                    #print(f\"    -> Cohérence C_NPMI (Gensim): {coherence_score:.4f}\") # Log optionnel ici, fait dans comparaison\n",
    "            elif not topic_words_custom:\n",
    "                print(f\"    WARN: Échec extraction mots pour cohérence (custom {scenario_name} K={num_topic}).\")\n",
    "            # Si Gensim non dispo, coherence_score reste NaN (initialisé)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliothèque GENSIM pour la cohérence (CoherenceModel, Dictionary) trouvée.\n",
      "Début du test de robustesse ÉTENDU (avec c_NPMI via GENSIM) pour K=15 uniquement.\n",
      "Heure: 10:53:13 UTC, Lieu: Toulouse\n",
      "\n",
      "Utilisation du scénario custom fixe : K=15, h_power=1.0, w_power=0.7\n",
      "Nombre de runs de robustesse par méthode : 20\n",
      "Graine solveur fixe pour custom : 1\n",
      "Graines utilisées pour l'initialisation : 0 à 19\n",
      "Paramètres c_NPMI (GENSIM) : topk=20, window_size=30\n",
      "\n",
      "Vérification des prérequis...\n",
      "Prérequis vérifiés.\n",
      "Création du dictionnaire Gensim à partir de tokenized_documents...\n",
      "Dictionnaire Gensim créé avec 61062 tokens uniques.\n",
      "\n",
      "--- Test Robustesse: Scénario Custom Fixe (h=1.0, w=0.7) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f165ede76e694bf48f6153b7a6454e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Robustesse Custom (K=15):   0%|          | 0/20 [00:00<?, ?seed/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Robustesse: NNDSVD ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22798d2746e46649d3370565a78c7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Robustesse NNDSVD (K=15):   0%|          | 0/20 [00:00<?, ?seed/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ANALYSE DE ROBUSTESSE FINALE POUR K=15 (Cohérence via GENSIM) =====\n",
      "\n",
      "--- Résultats Robustesse Custom (K=15, h=1.0, w=0.7) ---\n",
      "  Erreur Reconstruction: Moy=8602.8513, Etd=0.6599 (sur 20/20 runs NMF valides)\n",
      "  Stabilité Topics (Sim Cos Moy): 0.8950 (basé sur 20 matrices H)\n",
      "  Cohérence c_NPMI (GENSIM, w=30): Moy=0.0887, Etd=0.0033 (sur 20/20 runs avec score valide)\n",
      "\n",
      "--- Résultats Robustesse NNDSVD (K=15) ---\n",
      "  Erreur Reconstruction: Moy=8602.2552, Etd=0.3247 (sur 20/20 runs NMF valides)\n",
      "  Stabilité Topics (Sim Cos Moy): 0.8869 (basé sur 20 matrices H)\n",
      "  Cohérence c_NPMI (GENSIM, w=30): Moy=0.0903, Etd=0.0017 (sur 20/20 runs avec score valide)\n",
      "\n",
      "--- Comparaison Robustesse K=15 (Cohérence via GENSIM, w=30) ---\n",
      "  Erreur Reconstruction:\n",
      "    -> Moyenne plus basse pour NNDSVD (ou égale)\n",
      "    -> Plus stable (Etd plus bas) pour NNDSVD (ou égale)\n",
      "  Stabilité Topics (Similarité Cos):\n",
      "    -> Plus stable (Sim Cos Moy plus haute) pour Custom\n",
      "  Cohérence c_NPMI (GENSIM, w=30):\n",
      "    -> Moyenne plus haute pour NNDSVD (ou égale)\n",
      "    -> Plus stable (Etd plus bas) pour NNDSVD (ou égale)\n",
      "\n",
      "===== FIN TEST DE ROBUSTESSE ÉTENDU pour K=15 (Cohérence via GENSIM) =====\n"
     ]
    }
   ],
   "source": [
    "# --- Imports présumés faits ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from scipy.optimize import linear_sum_assignment # Supposé non utilisé directement ici, mais peut l'être dans calculate_topic_stability\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm # Ou from tqdm import tqdm\n",
    "\n",
    "# --- NOUVEAUX IMPORTS NÉCESSAIRES (GENSIM) ---\n",
    "try:\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "    from gensim.corpora import Dictionary\n",
    "    print(\"Bibliothèque GENSIM pour la cohérence (CoherenceModel, Dictionary) trouvée.\")\n",
    "    GENSIM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"\\nATTENTION : La bibliothèque GENSIM (ou spécifiquement gensim.models.CoherenceModel\")\n",
    "    print(\"ou gensim.corpora.Dictionary) n'a pas pu être importée.\")\n",
    "    print(\"Assurez-vous que GENSIM est correctement installé (pip install gensim).\")\n",
    "    print(\"Le calcul de c_npmi sera désactivé.\")\n",
    "    CoherenceModel = None # Pour éviter les erreurs plus tard\n",
    "    Dictionary = None\n",
    "    GENSIM_AVAILABLE = False\n",
    "\n",
    "# --- Variables et Fonctions présumées définies ---\n",
    "# NECESSAIREMENT DEFINIS AVANT :\n",
    "# - tfidf: np.ndarray ou sparse matrix (n_samples, n_features)\n",
    "# - tfidf_feature_names: list ou np.ndarray de strings (n_features)\n",
    "# - tokenized_documents: list[list[str]] (Corpus tokenisé, PRÉREQUIS CRUCIAL pour c_npmi avec Gensim)\n",
    "# - alpha_W, alpha_H, l1_ratio: float (paramètres NMF)\n",
    "# - h_num_items_decay, h_noise_level, w_noise_level: float/int (params init custom)\n",
    "# - max_iter: int (paramètre NMF)\n",
    "# - epsilon: float (pour init custom)\n",
    "# - create_decreasing_H: function\n",
    "# - create_decreasing_W: function\n",
    "# - calculate_topic_stability: function (prend une liste de matrices H)\n",
    "# ---\n",
    "\n",
    "print(f\"Début du test de robustesse ÉTENDU (avec c_NPMI via GENSIM) pour K=15 uniquement.\")\n",
    "print(f\"Heure: {time.strftime('%H:%M:%S %Z')}, Lieu: Toulouse\")\n",
    "\n",
    "# --- Paramètres spécifiques pour ce test ---\n",
    "NUM_TOPIC_TO_TEST = 15\n",
    "N_ROBUSTNESS_RUNS = 20\n",
    "ROBUSTNESS_SEEDS = list(range(N_ROBUSTNESS_RUNS))\n",
    "FIXED_SOLVER_STATE_FOR_ROBUSTNESS = 1\n",
    "N_TOP_WORDS_FOR_COHERENCE = 10  # Nombre de mots top à extraire pour DÉFINIR les topics passés à Gensim\n",
    "WINDOW_SIZE_FOR_COHERENCE = 30 # Taille de la fenêtre glissante pour c_NPMI (Gensim)\n",
    "\n",
    "# --- Choix du Scénario Custom Fixe ---\n",
    "fixed_h_power_custom = 1.00\n",
    "fixed_w_power_custom = 0.70\n",
    "print(f\"\\nUtilisation du scénario custom fixe : K={NUM_TOPIC_TO_TEST}, h_power={fixed_h_power_custom}, w_power={fixed_w_power_custom}\")\n",
    "print(f\"Nombre de runs de robustesse par méthode : {N_ROBUSTNESS_RUNS}\")\n",
    "print(f\"Graine solveur fixe pour custom : {FIXED_SOLVER_STATE_FOR_ROBUSTNESS}\")\n",
    "print(f\"Graines utilisées pour l'initialisation : {ROBUSTNESS_SEEDS[0]} à {ROBUSTNESS_SEEDS[-1]}\")\n",
    "if GENSIM_AVAILABLE:\n",
    "    print(f\"Paramètres c_NPMI (GENSIM) : topk={N_TOP_WORDS_FOR_COHERENCE}, window_size={WINDOW_SIZE_FOR_COHERENCE}\")\n",
    "else:\n",
    "    print(\"Calcul de c_NPMI désactivé (GENSIM non trouvé ou import échoué).\")\n",
    "\n",
    "# --- Initialisation du stockage des résultats de robustesse ---\n",
    "robustness_results = {\n",
    "    'custom': {'errors': [], 'h_matrices': [], 'coherence_scores': []},\n",
    "    'nndsvd': {'errors': [], 'h_matrices': [], 'coherence_scores': []}\n",
    "}\n",
    "\n",
    "# --- Vérifications Initiales Critiques ---\n",
    "print(\"\\nVérification des prérequis...\")\n",
    "required_vars = ['tfidf', 'tfidf_feature_names', 'alpha_W', 'alpha_H', 'l1_ratio',\n",
    "                 'h_num_items_decay', 'h_noise_level', 'w_noise_level',\n",
    "                 'max_iter', 'epsilon']\n",
    "required_funcs = ['create_decreasing_H', 'create_decreasing_W', 'calculate_topic_stability']\n",
    "# Ajout de la vérification pour tokenized_documents si Gensim est disponible\n",
    "if GENSIM_AVAILABLE:\n",
    "    required_vars.append('tokenized_documents')\n",
    "\n",
    "missing_vars = [v for v in required_vars if v not in globals()]\n",
    "missing_funcs = [f for f in required_funcs if f not in globals() or not callable(globals()[f])]\n",
    "\n",
    "if missing_vars: raise NameError(f\"Variables manquantes nécessaires : {', '.join(missing_vars)}\")\n",
    "if missing_funcs: raise NameError(f\"Fonctions manquantes nécessaires : {', '.join(missing_funcs)}\")\n",
    "if GENSIM_AVAILABLE and 'tokenized_documents' not in globals():\n",
    "     raise NameError(\"Variable manquante nécessaire pour GENSIM : tokenized_documents (list[list[str]])\")\n",
    "if GENSIM_AVAILABLE and 'tokenized_documents' in globals():\n",
    "    if not isinstance(tokenized_documents, list) or not all(isinstance(doc, list) for doc in tokenized_documents):\n",
    "        raise TypeError(\"`tokenized_documents` doit être une liste de listes de strings pour GENSIM.\")\n",
    "\n",
    "n_samples, n_features = tfidf.shape\n",
    "if len(tfidf_feature_names) != n_features:\n",
    "    raise ValueError(f\"Incohérence de dimensions : {len(tfidf_feature_names)} feature_names vs {n_features} features dans tfidf.\")\n",
    "print(\"Prérequis vérifiés.\")\n",
    "\n",
    "# --- Préparation du Dictionnaire Gensim (une seule fois) ---\n",
    "gensim_dictionary = None\n",
    "if GENSIM_AVAILABLE:\n",
    "    print(\"Création du dictionnaire Gensim à partir de tokenized_documents...\")\n",
    "    try:\n",
    "        gensim_dictionary = Dictionary(tokenized_documents)\n",
    "        # Optionnel: Filtrer les extrêmes si beaucoup de documents/vocabulaire\n",
    "        # gensim_dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "        print(f\"Dictionnaire Gensim créé avec {len(gensim_dictionary)} tokens uniques.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERREUR lors de la création du dictionnaire Gensim : {e}\")\n",
    "        print(\"Le calcul de la cohérence c_npmi via Gensim sera désactivé.\")\n",
    "        GENSIM_AVAILABLE = False # Désactiver si le dictionnaire échoue\n",
    "        CoherenceModel = None\n",
    "        Dictionary = None\n",
    "\n",
    "# =============================================================================\n",
    "# TEST DE ROBUSTESSE POUR K = NUM_TOPIC_TO_TEST\n",
    "# =============================================================================\n",
    "num_topic = NUM_TOPIC_TO_TEST\n",
    "\n",
    "# --- Fonction utilitaire pour calculer la cohérence (version GENSIM) ---\n",
    "def calculate_run_coherence_gensim(nmf_H, feature_names, tokenized_corpus, gensim_dictionary, n_top_words, window_size):\n",
    "    \"\"\"Calcule le score c_npmi pour une matrice H donnée en utilisant Gensim.\"\"\"\n",
    "    if not GENSIM_AVAILABLE or gensim_dictionary is None: # Ne rien faire si Gensim n'est pas dispo ou dictionnaire absent\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        # 1. Extraire les top words pour chaque topic à partir de nmf_H\n",
    "        topics = []\n",
    "        for topic_idx in range(nmf_H.shape[0]): # nmf_H.shape[0] == num_topic\n",
    "            # Obtenir les indices des mots les plus importants pour ce topic\n",
    "            top_word_indices = np.argsort(nmf_H[topic_idx, :])[-n_top_words:]\n",
    "            # Récupérer les mots correspondants\n",
    "            topic_words = [feature_names[i] for i in top_word_indices[::-1]] # Inverser pour avoir le plus important en premier\n",
    "            topics.append(topic_words)\n",
    "\n",
    "        # 2. Vérifier que les mots extraits sont dans le dictionnaire (Gensim peut échouer sinon)\n",
    "        #    C'est une bonne pratique, bien que souvent non bloquant si les vocabulaires coïncident.\n",
    "        vocab_present = all(all(word in gensim_dictionary.token2id for word in topic) for topic in topics)\n",
    "        if not vocab_present:\n",
    "             # Optionnel : loguer les mots manquants si besoin de débugger\n",
    "             # missing_words = set(word for topic in topics for word in topic if word not in gensim_dictionary.token2id)\n",
    "             # print(f\"      Attention : Certains top words NMF ne sont pas dans le dictionnaire Gensim.\") # Ignorer pour le moment\n",
    "             pass # On tente quand même, Gensim gère parfois les mots absents\n",
    "\n",
    "        # 3. Initialiser et calculer la métrique de cohérence Gensim\n",
    "        cm = CoherenceModel(\n",
    "            topics=topics,                 # La liste des listes de mots top\n",
    "            texts=tokenized_corpus,        # Le corpus tokenisé (list[list[str]])\n",
    "            dictionary=gensim_dictionary,  # Le dictionnaire Gensim\n",
    "            coherence='c_npmi',            # La mesure de cohérence souhaitée\n",
    "            window_size=window_size,       # La taille de la fenêtre glissante\n",
    "            topn=n_top_words               # Gensim utilise aussi topn, ici redondant car on fournit 'topics'\n",
    "                                           # mais on le garde cohérent.\n",
    "        )\n",
    "        coherence_score = cm.get_coherence()\n",
    "        return coherence_score\n",
    "\n",
    "    except Exception as e:\n",
    "        # Afficher l'erreur spécifique de Gensim peut être utile\n",
    "        print(f\"\\n      Erreur pendant le calcul de c_NPMI avec GENSIM : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Imprime la trace complète pour aider au débogage\n",
    "        return np.nan\n",
    "\n",
    "# --- Robustesse Scénario A : Custom Fixe ---\n",
    "print(f\"\\n--- Test Robustesse: Scénario Custom Fixe (h={fixed_h_power_custom}, w={fixed_w_power_custom}) ---\")\n",
    "pbar_robust_custom = tqdm(ROBUSTNESS_SEEDS, desc=f\"  Robustesse Custom (K={num_topic})\", leave=True, unit=\"seed\")\n",
    "for seed in pbar_robust_custom:\n",
    "    pbar_robust_custom.set_postfix_str(f\"seed={seed}\", refresh=True)\n",
    "    nmf_success = False\n",
    "    current_H = None\n",
    "    current_error = np.nan\n",
    "    current_coherence = np.nan\n",
    "\n",
    "    # 1. Créer W_init, H_init\n",
    "    try:\n",
    "        current_H_init = create_decreasing_H(\n",
    "            num_topic, n_features, h_num_items_decay, fixed_h_power_custom,\n",
    "            h_noise_level, seed, epsilon\n",
    "        )\n",
    "        current_W_init = create_decreasing_W(\n",
    "            n_samples, num_topic, fixed_w_power_custom, w_noise_level,\n",
    "            seed, epsilon\n",
    "        )\n",
    "        init_success = True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n    Erreur création init custom (seed={seed}, K={num_topic}): {e}\")\n",
    "        init_success = False\n",
    "\n",
    "    # 2. Exécuter NMF si init réussie\n",
    "    if init_success:\n",
    "        model_robust_custom = NMF(\n",
    "            n_components=num_topic, init='custom', solver='cd',\n",
    "            random_state=FIXED_SOLVER_STATE_FOR_ROBUSTNESS,\n",
    "            max_iter=max_iter,\n",
    "            alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio,\n",
    "            # tol=1e-4 # Ajout d'une tolérance peut parfois aider la convergence\n",
    "        )\n",
    "        try:\n",
    "            if current_W_init.shape != (n_samples, num_topic) or current_H_init.shape != (num_topic, n_features):\n",
    "                 raise ValueError(f\"Dims init custom R: W={current_W_init.shape}, H={current_H_init.shape} vs attendu ({n_samples},{num_topic}), ({num_topic},{n_features})\")\n",
    "            model_robust_custom.fit(tfidf, W=current_W_init.copy(), H=current_H_init.copy())\n",
    "            current_error = model_robust_custom.reconstruction_err_\n",
    "            current_H = model_robust_custom.components_.copy()\n",
    "            nmf_success = True\n",
    "        except Exception as e:\n",
    "            print(f\"\\n    Erreur NMF custom (seed={seed}, K={num_topic}): {e}\")\n",
    "            nmf_success = False\n",
    "\n",
    "    # 3. Calculer la cohérence si NMF réussie (avec GENSIM)\n",
    "    if nmf_success and GENSIM_AVAILABLE:\n",
    "         current_coherence = calculate_run_coherence_gensim( # Appel de la fonction version GENSIM\n",
    "             current_H,\n",
    "             tfidf_feature_names,\n",
    "             tokenized_documents,\n",
    "             gensim_dictionary,         # Passe le dictionnaire Gensim\n",
    "             N_TOP_WORDS_FOR_COHERENCE, # Passe topk\n",
    "             WINDOW_SIZE_FOR_COHERENCE  # Passe window_size\n",
    "         )\n",
    "\n",
    "    # 4. Stocker les résultats du run\n",
    "    robustness_results['custom']['errors'].append(current_error)\n",
    "    robustness_results['custom']['h_matrices'].append(current_H)\n",
    "    robustness_results['custom']['coherence_scores'].append(current_coherence)\n",
    "\n",
    "# --- Robustesse Scénario B : NNDSVD ---\n",
    "print(f\"\\n--- Test Robustesse: NNDSVD ---\")\n",
    "pbar_robust_nndsvd = tqdm(ROBUSTNESS_SEEDS, desc=f\"  Robustesse NNDSVD (K={num_topic})\", leave=True, unit=\"seed\")\n",
    "for seed in pbar_robust_nndsvd:\n",
    "    pbar_robust_nndsvd.set_postfix_str(f\"seed={seed}\", refresh=True)\n",
    "    nmf_success = False\n",
    "    current_H = None\n",
    "    current_error = np.nan\n",
    "    current_coherence = np.nan\n",
    "\n",
    "    # 1. Exécuter NMF avec init NNDSVD\n",
    "    model_robust_nndsvd = NMF(\n",
    "        n_components=num_topic, init='nndsvd', solver='cd',\n",
    "        random_state=seed,\n",
    "        max_iter=max_iter,\n",
    "        alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio,\n",
    "        # tol=1e-4\n",
    "    )\n",
    "    try:\n",
    "        model_robust_nndsvd.fit(tfidf)\n",
    "        current_error = model_robust_nndsvd.reconstruction_err_\n",
    "        current_H = model_robust_nndsvd.components_.copy()\n",
    "        nmf_success = True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n    Erreur NMF NNDSVD (seed={seed}, K={num_topic}): {e}\")\n",
    "        nmf_success = False\n",
    "\n",
    "    # 2. Calculer la cohérence si NMF réussie (avec GENSIM)\n",
    "    if nmf_success and GENSIM_AVAILABLE:\n",
    "        current_coherence = calculate_run_coherence_gensim( # Appel de la fonction version GENSIM\n",
    "            current_H,\n",
    "            tfidf_feature_names,\n",
    "            tokenized_documents,\n",
    "            gensim_dictionary,         # Passe le dictionnaire Gensim\n",
    "            N_TOP_WORDS_FOR_COHERENCE, # Passe topk\n",
    "            WINDOW_SIZE_FOR_COHERENCE  # Passe window_size\n",
    "        )\n",
    "\n",
    "    # 3. Stocker les résultats du run\n",
    "    robustness_results['nndsvd']['errors'].append(current_error)\n",
    "    robustness_results['nndsvd']['h_matrices'].append(current_H)\n",
    "    robustness_results['nndsvd']['coherence_scores'].append(current_coherence)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYSE ET AFFICHAGE DES RÉSULTATS DE ROBUSTESSE pour K=15\n",
    "# =============================================================================\n",
    "print(f\"\\n===== ANALYSE DE ROBUSTESSE FINALE POUR K={num_topic} (Cohérence via GENSIM) =====\")\n",
    "\n",
    "final_analysis = {} # Stockage des stats calculées\n",
    "\n",
    "# --- Fonctions utilitaires pour l'analyse ---\n",
    "# (La fonction analyze_metric reste la même)\n",
    "def analyze_metric(scores, metric_name):\n",
    "    \"\"\"Calcule et retourne moyenne et écart-type pour une liste de scores.\"\"\"\n",
    "    valid_scores = [s for s in scores if s is not None and not np.isnan(s)]\n",
    "    num_valid = len(valid_scores)\n",
    "    if num_valid > 0:\n",
    "        mean_score = np.mean(valid_scores)\n",
    "        std_score = np.std(valid_scores) if num_valid >= 2 else 0.0\n",
    "        return mean_score, std_score, num_valid\n",
    "    else:\n",
    "        return np.nan, np.nan, 0\n",
    "\n",
    "# --- Analyse Custom ---\n",
    "valid_custom_h = [h for h in robustness_results['custom']['h_matrices'] if h is not None]\n",
    "print(f\"\\n--- Résultats Robustesse Custom (K={num_topic}, h={fixed_h_power_custom}, w={fixed_w_power_custom}) ---\")\n",
    "\n",
    "# 1. Erreur Reconstruction (Identique)\n",
    "mean_err_custom, std_err_custom, num_success_err_custom = analyze_metric(\n",
    "    robustness_results['custom']['errors'], 'Erreur Reconstruction'\n",
    ")\n",
    "final_analysis['custom_error_mean'] = mean_err_custom\n",
    "final_analysis['custom_error_std'] = std_err_custom\n",
    "if num_success_err_custom > 0:\n",
    "    print(f\"  Erreur Reconstruction: Moy={mean_err_custom:.4f}, Etd={std_err_custom:.4f} (sur {num_success_err_custom}/{N_ROBUSTNESS_RUNS} runs NMF valides)\")\n",
    "else:\n",
    "    print(\"  Erreur Reconstruction: Aucun run NMF réussi.\")\n",
    "\n",
    "# 2. Stabilité Topics (Similarité Cosinus) (Identique)\n",
    "num_success_h_custom = len(valid_custom_h)\n",
    "if num_success_h_custom >= 2:\n",
    "    # Assurez-vous que calculate_topic_stability est définie et fonctionne comme attendu\n",
    "    stability_custom = calculate_topic_stability(valid_custom_h)\n",
    "    print(f\"  Stabilité Topics (Sim Cos Moy): {stability_custom:.4f} (basé sur {num_success_h_custom} matrices H)\")\n",
    "    final_analysis['custom_topic_stability'] = stability_custom\n",
    "elif num_success_h_custom == 1:\n",
    "     print(f\"  Stabilité Topics: 1 run NMF réussi, stabilité = 1.0\")\n",
    "     final_analysis['custom_topic_stability'] = 1.0\n",
    "else:\n",
    "    print(f\"  Stabilité Topics: Pas assez de matrices H valides ({num_success_h_custom}) pour calculer.\")\n",
    "    final_analysis['custom_topic_stability'] = np.nan\n",
    "\n",
    "# 3. Cohérence c_NPMI (via GENSIM)\n",
    "if GENSIM_AVAILABLE:\n",
    "    mean_coh_custom, std_coh_custom, num_success_coh_custom = analyze_metric(\n",
    "        robustness_results['custom']['coherence_scores'], f'Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE})'\n",
    "    )\n",
    "    final_analysis['custom_coherence_mean'] = mean_coh_custom\n",
    "    final_analysis['custom_coherence_std'] = std_coh_custom\n",
    "    if num_success_coh_custom > 0:\n",
    "        # Affichage des scores de cohérence\n",
    "        print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}): Moy={mean_coh_custom:.4f}, Etd={std_coh_custom:.4f} (sur {num_success_coh_custom}/{N_ROBUSTNESS_RUNS} runs avec score valide)\")\n",
    "    else:\n",
    "        print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}): Aucun score valide calculé (vérifier erreurs Gensim ci-dessus si applicable).\")\n",
    "else:\n",
    "    print(\"  Cohérence c_NPMI (GENSIM): Calcul désactivé (import/dict échoué).\")\n",
    "    final_analysis['custom_coherence_mean'] = np.nan\n",
    "    final_analysis['custom_coherence_std'] = np.nan\n",
    "\n",
    "\n",
    "# --- Analyse NNDSVD ---\n",
    "valid_nndsvd_h = [h for h in robustness_results['nndsvd']['h_matrices'] if h is not None]\n",
    "print(f\"\\n--- Résultats Robustesse NNDSVD (K={num_topic}) ---\")\n",
    "\n",
    "# 1. Erreur Reconstruction (Identique)\n",
    "mean_err_nndsvd, std_err_nndsvd, num_success_err_nndsvd = analyze_metric(\n",
    "    robustness_results['nndsvd']['errors'], 'Erreur Reconstruction'\n",
    ")\n",
    "final_analysis['nndsvd_error_mean'] = mean_err_nndsvd\n",
    "final_analysis['nndsvd_error_std'] = std_err_nndsvd\n",
    "if num_success_err_nndsvd > 0:\n",
    "    print(f\"  Erreur Reconstruction: Moy={mean_err_nndsvd:.4f}, Etd={std_err_nndsvd:.4f} (sur {num_success_err_nndsvd}/{N_ROBUSTNESS_RUNS} runs NMF valides)\")\n",
    "else:\n",
    "    print(\"  Erreur Reconstruction: Aucun run NMF réussi.\")\n",
    "\n",
    "# 2. Stabilité Topics (Similarité Cosinus) (Identique)\n",
    "num_success_h_nndsvd = len(valid_nndsvd_h)\n",
    "if num_success_h_nndsvd >= 2:\n",
    "    stability_nndsvd = calculate_topic_stability(valid_nndsvd_h)\n",
    "    print(f\"  Stabilité Topics (Sim Cos Moy): {stability_nndsvd:.4f} (basé sur {num_success_h_nndsvd} matrices H)\")\n",
    "    final_analysis['nndsvd_topic_stability'] = stability_nndsvd\n",
    "elif num_success_h_nndsvd == 1:\n",
    "    print(f\"  Stabilité Topics: 1 run NMF réussi, stabilité = 1.0\")\n",
    "    final_analysis['nndsvd_topic_stability'] = 1.0\n",
    "else:\n",
    "    print(f\"  Stabilité Topics: Pas assez de matrices H valides ({num_success_h_nndsvd})\")\n",
    "    final_analysis['nndsvd_topic_stability'] = np.nan\n",
    "\n",
    "# 3. Cohérence c_NPMI (via GENSIM)\n",
    "if GENSIM_AVAILABLE:\n",
    "    mean_coh_nndsvd, std_coh_nndsvd, num_success_coh_nndsvd = analyze_metric(\n",
    "        robustness_results['nndsvd']['coherence_scores'], f'Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE})'\n",
    "    )\n",
    "    final_analysis['nndsvd_coherence_mean'] = mean_coh_nndsvd\n",
    "    final_analysis['nndsvd_coherence_std'] = std_coh_nndsvd\n",
    "    if num_success_coh_nndsvd > 0:\n",
    "        # Affichage des scores de cohérence\n",
    "        print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}): Moy={mean_coh_nndsvd:.4f}, Etd={std_coh_nndsvd:.4f} (sur {num_success_coh_nndsvd}/{N_ROBUSTNESS_RUNS} runs avec score valide)\")\n",
    "    else:\n",
    "        print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}): Aucun score valide calculé.\")\n",
    "else:\n",
    "    print(\"  Cohérence c_NPMI (GENSIM): Calcul désactivé.\")\n",
    "    final_analysis['nndsvd_coherence_mean'] = np.nan\n",
    "    final_analysis['nndsvd_coherence_std'] = np.nan\n",
    "\n",
    "\n",
    "# --- Comparaison finale de robustesse ---\n",
    "# (La logique de comparaison reste la même, mais utilise maintenant les valeurs calculées avec GENSIM)\n",
    "print(f\"\\n--- Comparaison Robustesse K={num_topic} (Cohérence via GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}) ---\")\n",
    "\n",
    "custom_valid_error = not np.isnan(final_analysis.get('custom_error_mean', np.nan))\n",
    "nndsvd_valid_error = not np.isnan(final_analysis.get('nndsvd_error_mean', np.nan))\n",
    "custom_valid_topic = not np.isnan(final_analysis.get('custom_topic_stability', np.nan))\n",
    "nndsvd_valid_topic = not np.isnan(final_analysis.get('nndsvd_topic_stability', np.nan))\n",
    "custom_valid_coh = not np.isnan(final_analysis.get('custom_coherence_mean', np.nan))\n",
    "nndsvd_valid_coh = not np.isnan(final_analysis.get('nndsvd_coherence_mean', np.nan))\n",
    "\n",
    "# Comparaison Erreur Reconstruction\n",
    "if custom_valid_error and nndsvd_valid_error:\n",
    "     print(\"  Erreur Reconstruction:\")\n",
    "     if final_analysis['custom_error_mean'] < final_analysis['nndsvd_error_mean']: print(\"    -> Moyenne plus basse pour Custom\")\n",
    "     else: print(\"    -> Moyenne plus basse pour NNDSVD (ou égale)\")\n",
    "     if final_analysis['custom_error_std'] < final_analysis['nndsvd_error_std']: print(\"    -> Plus stable (Etd plus bas) pour Custom\")\n",
    "     else: print(\"    -> Plus stable (Etd plus bas) pour NNDSVD (ou égale)\")\n",
    "elif custom_valid_error: print(\"  Erreur Reconstruction: Seul Custom a des résultats valides.\")\n",
    "elif nndsvd_valid_error: print(\"  Erreur Reconstruction: Seul NNDSVD a des résultats valides.\")\n",
    "else: print(\"  Erreur Reconstruction: Non évaluable.\")\n",
    "\n",
    "# Comparaison Stabilité Topics (Similarité Cosinus)\n",
    "if custom_valid_topic and nndsvd_valid_topic:\n",
    "    print(\"  Stabilité Topics (Similarité Cos):\")\n",
    "    if final_analysis['custom_topic_stability'] > final_analysis['nndsvd_topic_stability']: print(\"    -> Plus stable (Sim Cos Moy plus haute) pour Custom\")\n",
    "    else: print(\"    -> Plus stable (Sim Cos Moy plus haute) pour NNDSVD (ou égale)\")\n",
    "elif custom_valid_topic: print(\"  Stabilité Topics: Seul Custom a des résultats valides.\")\n",
    "elif nndsvd_valid_topic: print(\"  Stabilité Topics: Seul NNDSVD a des résultats valides.\")\n",
    "else: print(\"  Stabilité Topics: Non évaluable.\")\n",
    "\n",
    "# Comparaison Cohérence c_NPMI (GENSIM)\n",
    "if custom_valid_coh and nndsvd_valid_coh:\n",
    "     print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}):\")\n",
    "     if final_analysis['custom_coherence_mean'] > final_analysis['nndsvd_coherence_mean']: print(\"    -> Moyenne plus haute pour Custom\")\n",
    "     else: print(\"    -> Moyenne plus haute pour NNDSVD (ou égale)\")\n",
    "     if final_analysis['custom_coherence_std'] < final_analysis['nndsvd_coherence_std']: print(\"    -> Plus stable (Etd plus bas) pour Custom\")\n",
    "     else: print(\"    -> Plus stable (Etd plus bas) pour NNDSVD (ou égale)\")\n",
    "elif custom_valid_coh: print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}): Seul Custom a des résultats valides.\")\n",
    "elif nndsvd_valid_coh: print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}): Seul NNDSVD a des résultats valides.\")\n",
    "elif not GENSIM_AVAILABLE: print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}): Calcul désactivé (import/dict échoué).\")\n",
    "else: print(f\"  Cohérence c_NPMI (GENSIM, w={WINDOW_SIZE_FOR_COHERENCE}): Non évaluable (aucun run valide).\")\n",
    "\n",
    "\n",
    "print(f\"\\n===== FIN TEST DE ROBUSTESSE ÉTENDU pour K={num_topic} (Cohérence via GENSIM) =====\")\n",
    "\n",
    "# FIN DU SCRIPT PRINCIPAL (GENSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from optuna) (24.2)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.40-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.13.1)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "Downloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (649 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m649.1/649.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.10 alembic-1.15.2 colorlog-6.9.0 greenlet-3.1.1 optuna-4.2.1 sqlalchemy-2.0.40\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_words(H, feature_names, n_top_words):\n",
    "    \"\"\"Extrait les n_top_words de chaque topic à partir de la matrice H.\"\"\"\n",
    "    topics = []\n",
    "    num_topics, num_features = H.shape\n",
    "    if len(feature_names) != num_features:\n",
    "        print(f\"ERREUR extract_top_words: Nb features H ({num_features}) != Nb feature_names ({len(feature_names)})\")\n",
    "        return [] # Retourner vide pour indiquer l'échec\n",
    "\n",
    "    for t in range(num_topics):\n",
    "        # Indices triés par poids décroissant\n",
    "        top_word_indices = np.argsort(H[t])[:-n_top_words - 1:-1]\n",
    "        # Récupérer les mots correspondants\n",
    "        words = [feature_names[i] for i in top_word_indices]\n",
    "        topics.append(words)\n",
    "    return topics # Retourne une list[list[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 4.3.3\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: https://radimrehurek.com/gensim/\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: /usr/local/lib/python3.12/site-packages\n",
      "Requires: numpy, scipy, smart-open\n",
      "Required-by: octis\n"
     ]
    }
   ],
   "source": [
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenized_documents\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized_documents' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_documents = tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calculate_coherence_gensim(topics, tokenized_corpus, gensim_dictionary, window_size, measure='c_npmi'):\n",
    "    \"\"\"Calcule le score de cohérence en utilisant Gensim CoherenceModel.\"\"\"\n",
    "    if not GENSIM_AVAILABLE or gensim_dictionary is None: # Vérifier si Gensim est dispo et si le dictionnaire a été créé\n",
    "        print(\"Skipping coherence calculation: Gensim not available or dictionary creation failed.\")\n",
    "        return np.nan\n",
    "\n",
    "    # Vérifications basiques\n",
    "    if not topics:\n",
    "        print(\"WARN calculate_coherence_gensim: La liste de topics (top words) est vide.\")\n",
    "        return np.nan\n",
    "    if not tokenized_corpus:\n",
    "         print(\"WARN calculate_coherence_gensim: Le corpus tokenisé est vide.\")\n",
    "         return np.nan\n",
    "    if not isinstance(topics, list) or not all(isinstance(topic, list) for topic in topics):\n",
    "         print(\"WARN calculate_coherence_gensim: `topics` doit être une liste de listes de mots.\")\n",
    "         return np.nan\n",
    "\n",
    "    try:\n",
    "        # Initialiser le modèle de cohérence Gensim\n",
    "        cm = CoherenceModel(\n",
    "            topics=topics,                 # La liste de listes de mots top (pré-extraits)\n",
    "            texts=tokenized_corpus,        # Le corpus tokenisé complet (list[list[str]])\n",
    "            dictionary=gensim_dictionary,  # Le dictionnaire Gensim créé précédemment\n",
    "            coherence=measure,             # Type de cohérence ('c_npmi', 'c_v', etc.)\n",
    "            window_size=window_size,       # Taille de la fenêtre glissante pour c_npmi\n",
    "            topn=len(topics[0]) if topics else n_top_words # Nombre de mots top utilisés par la mesure interne (redondant si 'topics' est fourni, mais bonne pratique de le spécifier)\n",
    "        )\n",
    "        # Obtenir le score\n",
    "        score = cm.get_coherence()\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"ERREUR pendant le calcul de la cohérence ({measure}) avec GENSIM: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Pour plus de détails sur l'erreur Gensim\n",
    "        return np.nan # Retourner NaN en cas d'erreur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 11:01:05,888 - INFO - === Démarrage de l'optimisation Optuna pour 'NMF_Optuna_Optim' avec 200 essais (timeout=3600s) ===\n",
      "2025-04-12 11:01:05,893 - INFO - Trial 0: Début essai avec K=40, h_decay=160, h_pow=9.43, h_noise=0.0005, w_pow=5.76, w_noise=0.0035, intial_peak=8.19\n",
      "2025-04-12 11:06:32,134 - INFO - Trial 0: Fin essai. Score retourné: 0.17376070373743635\n",
      "2025-04-12 11:06:32,144 - INFO - Trial 1: Début essai avec K=19, h_decay=400, h_pow=4.12, h_noise=0.0001, w_pow=7.76, w_noise=0.0015, intial_peak=4.65\n",
      "2025-04-12 11:09:59,525 - INFO - Trial 1: Fin essai. Score retourné: 0.15560651880411336\n",
      "2025-04-12 11:09:59,527 - INFO - Trial 2: Début essai avec K=60, h_decay=468, h_pow=6.66, h_noise=0.3498, w_pow=2.68, w_noise=0.0007, intial_peak=1.37\n",
      "2025-04-12 11:12:10,506 - INFO - Trial 2: Fin essai. Score retourné: 0.18054932691624406\n",
      "2025-04-12 11:12:10,512 - INFO - Trial 3: Début essai avec K=24, h_decay=181, h_pow=9.04, h_noise=0.0084, w_pow=1.93, w_noise=0.0002, intial_peak=8.98\n",
      "2025-04-12 11:13:49,085 - INFO - Trial 3: Fin essai. Score retourné: 0.16529267811574838\n",
      "2025-04-12 11:13:49,088 - INFO - Trial 4: Début essai avec K=20, h_decay=451, h_pow=2.70, h_noise=0.0015, w_pow=6.99, w_noise=0.4769, intial_peak=6.55\n",
      "2025-04-12 11:16:16,945 - INFO - Trial 4: Fin essai. Score retourné: 0.16288571437206387\n",
      "2025-04-12 11:16:16,952 - INFO - Trial 5: Début essai avec K=37, h_decay=405, h_pow=4.02, h_noise=0.0979, w_pow=5.58, w_noise=0.0064, intial_peak=8.49\n",
      "2025-04-12 11:19:06,396 - INFO - Trial 5: Fin essai. Score retourné: 0.17971113841008327\n",
      "2025-04-12 11:19:06,414 - INFO - Trial 6: Début essai avec K=19, h_decay=174, h_pow=5.37, h_noise=0.0437, w_pow=1.00, w_noise=0.0048, intial_peak=4.07\n",
      "2025-04-12 11:20:41,279 - INFO - Trial 6: Fin essai. Score retourné: 0.15374725799749092\n",
      "2025-04-12 11:20:41,294 - INFO - Trial 7: Début essai avec K=47, h_decay=112, h_pow=9.84, h_noise=0.0082, w_pow=0.86, w_noise=0.0275, intial_peak=1.46\n",
      "2025-04-12 11:25:05,397 - INFO - Trial 7: Fin essai. Score retourné: 0.17710483202473837\n",
      "2025-04-12 11:25:05,411 - INFO - Trial 8: Début essai avec K=5, h_decay=265, h_pow=2.00, h_noise=0.0818, w_pow=6.11, w_noise=0.0024, intial_peak=1.89\n",
      "2025-04-12 11:26:19,361 - INFO - Trial 8: Fin essai. Score retourné: 0.08822230439443862\n",
      "2025-04-12 11:26:19,436 - INFO - Trial 9: Début essai avec K=27, h_decay=375, h_pow=5.96, h_noise=0.0036, w_pow=3.22, w_noise=0.0680, intial_peak=8.94\n",
      "[W 2025-04-12 11:29:32,677] Trial 9 failed with parameters: {'num_topic': 27, 'h_num_items_decay': 375, 'h_power': 5.960541797731634, 'h_noise_level': 0.003557264368317054, 'w_power': 3.221883145622838, 'w_noise_level': 0.06804698650544344, 'initial_peak': 8.939645761858664} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_77080/292682722.py\", line 178, in objective\n",
      "    nmf_model.fit(tfidf, W=W_fit, H=H_fit)\n",
      "  File \"/usr/local/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py\", line 1296, in fit\n",
      "    self.fit_transform(X, **params)\n",
      "  File \"/usr/local/lib/python3.12/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py\", line 1640, in fit_transform\n",
      "    W, H, n_iter = self._fit_transform(X, W=W, H=H)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py\", line 1708, in _fit_transform\n",
      "    W, H, n_iter = _fit_coordinate_descent(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py\", line 501, in _fit_coordinate_descent\n",
      "    violation += _update_coordinate_descent(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/sklearn/decomposition/_nmf.py\", line 388, in _update_coordinate_descent\n",
      "    XHt = safe_sparse_dot(X, Ht)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/sklearn/utils/extmath.py\", line 203, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "          ~~^~~\n",
      "  File \"/usr/local/lib/python3.12/site-packages/scipy/sparse/_base.py\", line 695, in __matmul__\n",
      "    return self._matmul_dispatch(other)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/scipy/sparse/_base.py\", line 595, in _matmul_dispatch\n",
      "    return self._matmul_multivector(other)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/scipy/sparse/_compressed.py\", line 504, in _matmul_multivector\n",
      "    fn(M, N, n_vecs, self.indptr, self.indices, self.data,\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-12 11:29:32,689] Trial 9 failed with value None.\n",
      "2025-04-12 11:29:32,690 - WARNING - Optimisation interrompue par l'utilisateur (KeyboardInterrupt).\n",
      "2025-04-12 11:29:32,690 - INFO - === Optimisation terminée ===\n",
      "2025-04-12 11:29:32,692 - INFO - Meilleur essai trouvé : Numéro 2\n",
      "2025-04-12 11:29:32,692 - INFO -   Meilleur score (cohérence) : 0.18055\n",
      "2025-04-12 11:29:32,693 - INFO -   Meilleurs hyperparamètres :\n",
      "2025-04-12 11:29:32,693 - INFO -     num_topic: 60\n",
      "2025-04-12 11:29:32,694 - INFO -     h_num_items_decay: 468\n",
      "2025-04-12 11:29:32,694 - INFO -     h_power: 6.657957702039263\n",
      "2025-04-12 11:29:32,695 - INFO -     h_noise_level: 0.3497989886292618\n",
      "2025-04-12 11:29:32,698 - INFO -     w_power: 2.6758042325650018\n",
      "2025-04-12 11:29:32,700 - INFO -     w_noise_level: 0.0006889932890464792\n",
      "2025-04-12 11:29:32,700 - INFO -     initial_peak: 1.369362709341334\n",
      "2025-04-12 11:29:32,705 - INFO - Génération des visualisations Optuna...\n",
      "2025-04-12 11:29:32,707 - WARNING - Bibliothèques de visualisation (plotly, matplotlib) non trouvées. Saut des graphiques.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.decomposition import NMF\n",
    "import optuna # Import Optuna\n",
    "\n",
    "# --- Configuration du Logging (Format simplifié) ---\n",
    "# Configurez le logging comme vous le souhaitez (ex: fichier, niveau)\n",
    "# Format simplifié pour éviter les erreurs avec les logs externes\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    force=True) # Ajout de force=True\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# <<< MODIFICATION 2 : Réduire la verbosité de Gensim >>>\n",
    "# Ne montrera que les warnings et erreurs de Gensim\n",
    "logging.getLogger('gensim').setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- Prérequis supposés définis ailleurs ---\n",
    "# Assurez-vous que ces variables existent et sont correctement initialisées AVANT ce bloc\n",
    "# tfidf = ... (votre matrice TF-IDF sparse ou dense)\n",
    "# tokenized_documents = ... (liste de listes de tokens, ex: [['mot1', 'mot2'], ['mot3']])\n",
    "# tfidf_feature_names = ... (liste des noms de features/mots du TF-IDF)\n",
    "# epsilon = ... (petite valeur, ex: 1e-6)\n",
    "# create_decreasing_H = ... (votre fonction)\n",
    "# create_decreasing_W = ... (votre fonction)\n",
    "# extract_top_words = ... (votre fonction)\n",
    "# calculate_coherence_gensim = ... (votre fonction, assurez-vous qu'elle accepte 'texts')\n",
    "\n",
    "# --- Dimensions des données ---\n",
    "try:\n",
    "    n_samples, n_features = tfidf.shape\n",
    "except NameError:\n",
    "    logging.error(\"La variable 'tfidf' n'est pas définie. Veuillez la charger ou la créer.\")\n",
    "    # Arrêter ou définir des valeurs par défaut si nécessaire\n",
    "    exit() # Ou gérer l'erreur autrement\n",
    "\n",
    "# --- Constantes et Prérequis (Définis EN DEHORS de l'objectif) ---\n",
    "RANDOM_SEED = 1\n",
    "INIT_RANDOM_SEED = 42\n",
    "NMF_SOLVER = 'cd'\n",
    "NMF_INIT_METHOD = 'custom'\n",
    "MAX_ITER = 10000          # Exemple: nombre max d'itérations pour NMF\n",
    "ALPHA_W = 0.0           # Exemple: régularisation L1/L2 pour W\n",
    "ALPHA_H = 0.0           # Exemple: régularisation L1/L2 pour H ('auto' souvent géré par solver 'cd')\n",
    "L1_RATIO = 0.0          # Exemple: ratio L1 (0=L2, 1=L1)\n",
    "N_TOP_WORDS = 10        # Exemple: nombre de mots clés à extraire par topic\n",
    "epsilon = 1e-6        # Déplacé vers les prérequis supposés\n",
    "SCENARIO_NAME = \"NMF_Optuna_Optim\" # Nom pour les logs\n",
    "\n",
    "# --- Vérification et Initialisation Gensim ---\n",
    "GENSIM_AVAILABLE = True\n",
    "\n",
    "COHERENCE_WINDOW_SIZE = 30 # Taille de fenêtre pour C_NPMI si Gensim utilisé\n",
    "\n",
    "# ==============================================================\n",
    "# FONCTION OBJECTIVE POUR OPTUNA (MODIFIÉE)\n",
    "# ==============================================================\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Fonction exécutée par Optuna pour chaque essai.\n",
    "    Teste une combinaison d'hyperparamètres et retourne le score de cohérence.\n",
    "    Retourne -float('inf') en cas d'échec pour permettre la maximisation.\n",
    "    \"\"\"\n",
    "    trial_num = trial.number # Récupérer le numéro de l'essai pour les logs\n",
    "\n",
    "    # 1. Suggérer les hyperparamètres à tester pour cet essai\n",
    "    num_topic = trial.suggest_int('num_topic', 5, 70) # K : Nombre de topics\n",
    "    h_num_items_decay = trial.suggest_int('h_num_items_decay', 50, 500) # Pour H init\n",
    "    h_power = trial.suggest_float('h_power', 0.5, 10.0)           # Pour H init\n",
    "    h_noise_level = trial.suggest_float('h_noise_level', 1e-4, 0.5, log=True) # Pour H init\n",
    "    w_power = trial.suggest_float('w_power', 0.5, 10.0)           # Pour W init\n",
    "    w_noise_level = trial.suggest_float('w_noise_level', 1e-4, 0.5, log=True) # Pour W init\n",
    "    initial_peak = trial.suggest_float('initial_peak', 0.5, 10.0) # Pour W init\n",
    "\n",
    "    logging.info(f\"Trial {trial_num}: Début essai avec K={num_topic}, h_decay={h_num_items_decay}, h_pow={h_power:.2f}, h_noise={h_noise_level:.4f}, w_pow={w_power:.2f}, w_noise={w_noise_level:.4f}, intial_peak={initial_peak:.2f}\")\n",
    "\n",
    "    # ==============================================================\n",
    "    # 1. Créer les matrices d'initialisation personnalisées H et W\n",
    "    # ==============================================================\n",
    "    current_H_init, current_W_init = None, None\n",
    "    expected_H_shape = (num_topic, n_features)\n",
    "    expected_W_shape = (n_samples, num_topic)\n",
    "\n",
    "   # logging.info(f\"Trial {trial_num}: Tentative de création des matrices d'initialisation H et W pour K={num_topic}...\")\n",
    "    try:\n",
    "        # Assurez-vous que les fonctions et 'epsilon' sont disponibles\n",
    "        current_H_init = create_decreasing_H(\n",
    "            num_topic, n_features, h_num_items_decay, h_power, h_noise_level, INIT_RANDOM_SEED, epsilon, initial_peak\n",
    "        )\n",
    "        current_W_init = create_decreasing_W(\n",
    "            n_samples, num_topic, w_power, w_noise_level, INIT_RANDOM_SEED, epsilon, initial_peak\n",
    "        )\n",
    "\n",
    "        # Vérification rigoureuse\n",
    "        if current_W_init is None or current_H_init is None:\n",
    "            raise ValueError(\"La création d'une matrice d'initialisation (H ou W) a échoué (résultat None).\")\n",
    "        if current_W_init.shape != expected_W_shape or current_H_init.shape != expected_H_shape:\n",
    "            raise ValueError(\n",
    "                f\"Dimensions initialisation INCOHÉRENTES: \"\n",
    "                f\"W={current_W_init.shape} (attendu {expected_W_shape}), \"\n",
    "                f\"H={current_H_init.shape} (attendu {expected_H_shape})\"\n",
    "            )\n",
    "\n",
    "   #     logging.info(f\"Trial {trial_num}: Initialisation personnalisée (H/W) créée avec succès pour K={num_topic}.\")\n",
    "        init_creation_success = True\n",
    "\n",
    "    except NameError as e_name:\n",
    "         logging.error(\n",
    "            f\"Trial {trial_num}: ERREUR - Variable ou fonction MANQUANTE lors de la création de l'initialisation (K={num_topic}): {e_name}. Assurez-vous que 'epsilon', 'create_decreasing_H', 'create_decreasing_W' sont définies.\",\n",
    "            exc_info=False\n",
    "        )\n",
    "         init_creation_success = False\n",
    "    except Exception as e_init:\n",
    "        logging.error(\n",
    "            f\"Trial {trial_num}: ERREUR lors de la création ou validation de l'initialisation (K={num_topic}): {e_init}\",\n",
    "            exc_info=False\n",
    "        )\n",
    "        init_creation_success = False\n",
    "\n",
    "    # Si l'initialisation échoue, cet essai est invalide.\n",
    "    if not init_creation_success:\n",
    "        logging.warning(f\"Trial {trial_num}: Échec de l'initialisation, retour de -inf.\")\n",
    "        return -float('inf') # Retourner une mauvaise valeur pour la maximisation\n",
    "\n",
    "    # ==============================================================\n",
    "    # 2. Sauvegarder les matrices d'initialisation (Optionnel)\n",
    "    # ==============================================================\n",
    "    # Décommenter et adapter si nécessaire (définir init_csv_dir)\n",
    "    # init_csv_dir = \"/chemin/optionnel/pour/sauvegarde\"\n",
    "    # if init_creation_success and 'init_csv_dir' in locals() and init_csv_dir:\n",
    "    #     try:\n",
    "    #         # ... Code pour sauvegarder H et W ...\n",
    "    #         logging.info(f\"Trial {trial_num}: Matrices d'initialisation K={num_topic} sauvegardées.\")\n",
    "    #     except Exception as e_save:\n",
    "    #         logging.error(f\"Trial {trial_num}: Échec sauvegarde matrices K={num_topic}: {e_save}\", exc_info=False)\n",
    "\n",
    "    # ==============================================================\n",
    "    # 3. Exécuter NMF et calculer la cohérence\n",
    "    # ==============================================================\n",
    "    nmf_model = None\n",
    "  #  reconstruction_error = float('inf')\n",
    "    coherence_score = -float('inf') # Initialiser à -inf pour maximisation\n",
    "    #duration = 0.0\n",
    "    nmf_fit_success = False\n",
    "    topic_words_custom = []\n",
    "\n",
    "   # logging.info(f\"Trial {trial_num}: Initialisation NMF (K={num_topic}) avec méthode '{NMF_INIT_METHOD}'...\")\n",
    "    try:\n",
    "        nmf_model = NMF(\n",
    "            n_components=num_topic,\n",
    "            init=NMF_INIT_METHOD,\n",
    "            solver=NMF_SOLVER,\n",
    "            random_state=RANDOM_SEED + trial_num, # Seed différent par essai\n",
    "            max_iter=MAX_ITER,\n",
    "            alpha_W=ALPHA_W,\n",
    "            alpha_H=ALPHA_H,\n",
    "            l1_ratio=L1_RATIO,\n",
    "            # tol=1e-4 # Vous pouvez aussi optimiser la tolérance\n",
    "        )\n",
    "    except Exception as e_nmf_init:\n",
    "         logging.error(f\"Trial {trial_num}: ERREUR lors de l'initialisation de sklearn.NMF (K={num_topic}): {e_nmf_init}\", exc_info=False)\n",
    "         logging.warning(f\"Trial {trial_num}: Échec init NMF, retour de -inf.\")\n",
    "         return -float('inf')\n",
    "\n",
    "\n",
    "    #start_time = time.time()\n",
    "   # logging.info(f\"Trial {trial_num}: Démarrage NMF.fit (K={num_topic})...\")\n",
    "    try:\n",
    "        # Utiliser .copy() car NMF modifie W et H en place avec init='custom'\n",
    "        W_fit = current_W_init.copy()\n",
    "        H_fit = current_H_init.copy()\n",
    "        # Assurez-vous que 'tfidf' est disponible\n",
    "        nmf_model.fit(tfidf, W=W_fit, H=H_fit)\n",
    "\n",
    "       # reconstruction_error = nmf_model.reconstruction_err_\n",
    "        nmf_fit_success = True\n",
    "       # logging.info(f\"Trial {trial_num}: NMF.fit (K={num_topic}) terminé. Erreur={reconstruction_error:.4f}\")\n",
    "\n",
    "    except NameError as e_name:\n",
    "        logging.error(f\"Trial {trial_num}: ERREUR - Variable 'tfidf' MANQUANTE durant NMF.fit (K={num_topic}): {e_name}\", exc_info=False)\n",
    "        nmf_fit_success = False\n",
    "    except Exception as e_fit:\n",
    "        logging.error(f\"Trial {trial_num}: ERREUR durant NMF.fit (K={num_topic}): {e_fit}\", exc_info=False)\n",
    "        nmf_fit_success = False # Marquer comme échoué\n",
    "\n",
    "  #  finally:\n",
    "      #  duration = time.time() - start_time\n",
    "      #  logging.info(f\"Trial {trial_num}: Temps écoulé pour NMF.fit (K={num_topic}): {duration:.2f}s\")\n",
    "\n",
    "    # Si NMF.fit a échoué, retourner une mauvaise valeur\n",
    "    if not nmf_fit_success:\n",
    "        logging.warning(f\"Trial {trial_num}: Échec NMF.fit, retour de -inf.\")\n",
    "        return -float('inf')\n",
    "\n",
    "    # --- Calcul de la cohérence et extraction des mots SEULEMENT SI NMF.fit a réussi ---\n",
    "   # logging.info(f\"Trial {trial_num}: Extraction mots clés et calcul cohérence (K={num_topic})...\")\n",
    "    try:\n",
    "        # 1. Extraire les mots clés\n",
    "        # Assurez-vous que 'tfidf_feature_names' et 'extract_top_words' sont disponibles\n",
    "        topic_words_custom = extract_top_words(nmf_model.components_, tfidf_feature_names, N_TOP_WORDS)\n",
    "\n",
    "        if not topic_words_custom:\n",
    "            logging.warning(f\"Trial {trial_num}: Aucun mot clé extrait pour K={num_topic}. Cohérence non calculée.\")\n",
    "            coherence_score = -float('inf') # Pas de cohérence si pas de mots\n",
    "\n",
    "        # 2. Calculer la cohérence si possible\n",
    "        elif GENSIM_AVAILABLE and gensim_dictionary:\n",
    "           # logging.info(f\"Trial {trial_num}: Calcul cohérence C_NPMI (Gensim) pour K={num_topic}...\")\n",
    "            try:\n",
    "                # Assurez-vous que 'tokenized_documents', 'gensim_dictionary',\n",
    "                # et 'calculate_coherence_gensim' sont disponibles et que\n",
    "                # calculate_coherence_gensim accepte bien le paramètre 'texts'\n",
    "                coherence_score_calc = calculate_coherence_gensim(\n",
    "                    topics=topic_words_custom,\n",
    "                    tokenized_corpus=tokenized_documents,    # Passez les documents tokenisés ici\n",
    "                    gensim_dictionary=gensim_dictionary,\n",
    "                    window_size=COHERENCE_WINDOW_SIZE\n",
    "                    # coherence_type='c_npmi' # si votre fonction le gère\n",
    "                )\n",
    "                # Vérifier si le calcul a retourné NaN\n",
    "                if np.isnan(coherence_score_calc):\n",
    "                     logging.warning(f\"Trial {trial_num}: Calcul cohérence a retourné NaN pour K={num_topic}. Retour de -inf.\")\n",
    "                     coherence_score = -float('inf')\n",
    "                else:\n",
    "                     coherence_score = coherence_score_calc # Assigner le score calculé\n",
    "                  #   logging.info(f\"Trial {trial_num}: Score cohérence C_NPMI (Gensim) pour K={num_topic}: {coherence_score:.4f}\")\n",
    "\n",
    "            except TypeError as e_coh_type:\n",
    "                 # Erreur spécifique si 'texts' n'est pas accepté\n",
    "                 if 'unexpected keyword argument \\'texts\\'' in str(e_coh_type):\n",
    "                      logging.error(f\"Trial {trial_num}: ERREUR - Votre fonction 'calculate_coherence_gensim' n'accepte pas l'argument 'texts'. Veuillez corriger sa définition. {e_coh_type}\", exc_info=False)\n",
    "                 else:\n",
    "                      logging.error(f\"Trial {trial_num}: ERREUR de type durant calcul cohérence Gensim K={num_topic}: {e_coh_type}\", exc_info=False)\n",
    "                 coherence_score = -float('inf') # Marquer l'échec\n",
    "            except NameError as e_name:\n",
    "                 logging.error(f\"Trial {trial_num}: ERREUR - Variable MANQUANTE durant calcul cohérence ({e_name}). Vérifiez 'tokenized_documents', 'gensim_dictionary', 'calculate_coherence_gensim'.\", exc_info=False)\n",
    "                 coherence_score = -float('inf') # Marquer l'échec\n",
    "            except Exception as e_coh:\n",
    "                logging.error(f\"Trial {trial_num}: ERREUR durant calcul cohérence Gensim K={num_topic}: {e_coh}\", exc_info=False)\n",
    "                coherence_score = -float('inf') # Marquer l'échec\n",
    "\n",
    "        else:\n",
    "            if not GENSIM_AVAILABLE:\n",
    "                 logging.warning(f\"Trial {trial_num}: Gensim non disponible. Score cohérence non calculé pour K={num_topic}. Retour de -inf.\")\n",
    "            elif not gensim_dictionary:\n",
    "                 logging.warning(f\"Trial {trial_num}: Dictionnaire Gensim non créé. Score cohérence non calculé pour K={num_topic}. Retour de -inf.\")\n",
    "            coherence_score = -float('inf') # Pas de calcul possible\n",
    "\n",
    "    except NameError as e_name:\n",
    "        logging.error(f\"Trial {trial_num}: ERREUR - Variable MANQUANTE extraction mots clés K={num_topic} ({e_name}). Vérifiez 'tfidf_feature_names', 'extract_top_words'.\", exc_info=False)\n",
    "        coherence_score = -float('inf')\n",
    "    except Exception as e_extract:\n",
    "        logging.error(f\"Trial {trial_num}: ERREUR extraction mots clés K={num_topic}: {e_extract}\", exc_info=False)\n",
    "        coherence_score = -float('inf') # Pas de cohérence si extraction échoue\n",
    "\n",
    "    # --- Retourner la métrique à optimiser ---\n",
    "    # coherence_score est soit la valeur calculée, soit -inf si une étape a échoué\n",
    "    logging.info(f\"Trial {trial_num}: Fin essai. Score retourné: {coherence_score}\")\n",
    "    # Gérer explicitement le cas où le score est -inf (même si Optuna le gère)\n",
    "    if coherence_score == -float('inf'):\n",
    "         # Vous pourriez lever optuna.TrialPruned() ici si vous voulez que le pruner agisse\n",
    "         # raise optuna.TrialPruned(\"Échec critique dans le calcul de l'objectif.\")\n",
    "         pass # Ou juste retourner -inf\n",
    "\n",
    "    # Vérifier une dernière fois si NaN (ne devrait pas arriver avec la logique ci-dessus, mais par sécurité)\n",
    "    if np.isnan(coherence_score):\n",
    "        logging.error(f\"Trial {trial_num}: Score final est NaN ! Retour de -inf.\")\n",
    "        return -float('inf')\n",
    "\n",
    "    return coherence_score\n",
    "\n",
    "# ==============================================================\n",
    "# OPTIMISATION AVEC OPTUNA\n",
    "# ==============================================================\n",
    "\n",
    "# Vérifier que les prérequis essentiels existent\n",
    "if 'tfidf' not in globals() or 'tokenized_documents' not in globals() or 'tfidf_feature_names' not in globals():\n",
    "        logging.critical(\"Variables essentielles (tfidf, tokenized_documents, tfidf_feature_names) non définies. Arrêt.\")\n",
    "        # exit()\n",
    "\n",
    "# Crée une étude Optuna.\n",
    "study = None\n",
    "try:\n",
    "    study = optuna.create_study(direction='maximize',\n",
    "                                study_name=SCENARIO_NAME,\n",
    "                                # Utiliser un pruner peut accélérer si certains essais sont très lents et peu prometteurs\n",
    "                                pruner=optuna.pruners.MedianPruner(n_warmup_steps=5, n_min_trials=5)) # Prune après 5 essais complets\n",
    "except Exception as e_study:\n",
    "    logging.critical(f\"Impossible de créer l'étude Optuna : {e_study}\", exc_info=True)\n",
    "    # exit()\n",
    "\n",
    "\n",
    "best_trial = None\n",
    "if study:\n",
    "    # Nombre d'essais à réaliser\n",
    "    n_trials = 200 # Adaptez ce nombre\n",
    "    timeout_seconds = 3600 # Adaptez ou mettez None\n",
    "\n",
    "    logging.info(f\"=== Démarrage de l'optimisation Optuna pour '{study.study_name}' avec {n_trials} essais (timeout={timeout_seconds}s) ===\")\n",
    "\n",
    "    try:\n",
    "        # Lance l'optimisation\n",
    "        study.optimize(objective, n_trials=n_trials, timeout=timeout_seconds)\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"Optimisation interrompue par l'utilisateur (KeyboardInterrupt).\")\n",
    "    except NameError as e_opt_name:\n",
    "        logging.error(f\"Une variable MANQUANTE a été détectée pendant study.optimize: {e_opt_name}. Vérifiez les dépendances globales.\", exc_info=True)\n",
    "    except Exception as e_opt:\n",
    "        logging.error(f\"Une erreur majeure est survenue durant l'optimisation: {e_opt}\", exc_info=True)\n",
    "\n",
    "    # --- Affichage des résultats ---\n",
    "    logging.info(\"=== Optimisation terminée ===\")\n",
    "\n",
    "    # Vérifier s'il y a un meilleur essai (pourrait ne pas exister si tout a échoué ou a été interrompu tôt)\n",
    "    try:\n",
    "        best_trial = study.best_trial\n",
    "        logging.info(f\"Meilleur essai trouvé : Numéro {best_trial.number}\")\n",
    "\n",
    "        best_value = best_trial.value\n",
    "        # Vérifier si la meilleure valeur est valide (pas -inf ou None)\n",
    "        if best_value is not None and best_value != -float('inf'):\n",
    "            logging.info(f\"  Meilleur score (cohérence) : {best_value:.5f}\")\n",
    "        elif best_value == -float('inf'):\n",
    "            logging.warning(\"  Tous les essais réussis ont retourné -inf (échec interne probable).\")\n",
    "        else:\n",
    "            logging.warning(\"  Aucun essai n'a produit un score de cohérence valide ou n'a pu être évalué.\")\n",
    "\n",
    "        logging.info(\"  Meilleurs hyperparamètres :\")\n",
    "        for key, value in best_trial.params.items():\n",
    "            logging.info(f\"    {key}: {value}\")\n",
    "\n",
    "    except ValueError:\n",
    "        logging.warning(\"Aucun essai terminé avec succès n'a été trouvé dans l'étude (study.best_trial a échoué).\")\n",
    "    except Exception as e_results:\n",
    "        logging.error(f\"Erreur lors de l'affichage des meilleurs résultats : {e_results}\")\n",
    "\n",
    "\n",
    "    # Optionnel : Visualisations Optuna\n",
    "    try:\n",
    "        # Vérifier si des essais ont été complétés avant de tenter de générer les graphiques\n",
    "        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        if completed_trials:\n",
    "            logging.info(\"Génération des visualisations Optuna...\")\n",
    "            # S'assurer que plotly est installé\n",
    "         #   import plotly # Mettre dans un try/except si optionnel\n",
    "\n",
    "            # Importance des hyperparamètres\n",
    "            fig1 = optuna.visualization.plot_param_importances(study)\n",
    "            fig1.show()\n",
    "            # fig1.write_html(\"param_importances.html\")\n",
    "\n",
    "            # Historique d'optimisation\n",
    "            fig2 = optuna.visualization.plot_optimization_history(study)\n",
    "            fig2.show()\n",
    "            # fig2.write_html(\"optimization_history.html\")\n",
    "\n",
    "            # Slice plot (choisir quelques paramètres pertinents)\n",
    "            # Vérifier quels paramètres sont effectivement dans l'étude\n",
    "            params_to_plot = [p for p in ['num_topic', 'h_noise_level', 'w_noise_level', 'h_power'] if p in study.best_params]\n",
    "            if params_to_plot:\n",
    "                fig3 = optuna.visualization.plot_slice(study, params=params_to_plot)\n",
    "                fig3.show()\n",
    "                # fig3.write_html(\"slice_plot.html\")\n",
    "            else:\n",
    "                logging.info(\"Aucun paramètre pertinent trouvé pour le slice plot.\")\n",
    "\n",
    "        else:\n",
    "            logging.warning(\"Aucun essai complété, impossible de générer les visualisations Optuna.\")\n",
    "\n",
    "    except ImportError:\n",
    "        logging.warning(\"Bibliothèques de visualisation (plotly, matplotlib) non trouvées. Saut des graphiques.\")\n",
    "    except Exception as e_vis:\n",
    "        logging.warning(f\"Impossible de générer les visualisations Optuna : {e_vis}\", exc_info=False) # Mettre True pour debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_topic': 7,\n",
       " 'h_num_items_decay': 169,\n",
       " 'h_power': 4.985014922686625,\n",
       " 'h_noise_level': 0.018616890492508073,\n",
       " 'w_power': 4.452134245034107,\n",
       " 'w_noise_level': 0.03383757092291506}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 15:05:07,342 - INFO - ============================================================\n",
      "2025-04-11 15:05:07,343 - INFO - ====== LANCEMENT DU RÉ-ENTRAÎNEMENT FINAL + COHÉRENCE ======\n",
      "2025-04-11 15:05:07,344 - INFO - Utilisation des meilleurs paramètres trouvés et de graines fixes.\n",
      "2025-04-11 15:05:07,345 - INFO - Meilleurs paramètres récupérés: K=5, h_decay=339, h_pow=6.45, ...\n",
      "2025-04-11 15:05:07,345 - INFO - -> 1. Recréation des matrices H/W pour K=5 avec seed_init=42...\n",
      "2025-04-11 15:05:07,518 - INFO -    Matrices d'initialisation finales H/W créées avec succès.\n",
      "2025-04-11 15:05:07,519 - INFO - -> 2. Initialisation du modèle NMF final (K=5) avec random_state=1...\n",
      "2025-04-11 15:05:07,519 - INFO -    Modèle NMF final initialisé.\n",
      "2025-04-11 15:05:07,520 - INFO - -> 3. Entraînement (fit) du modèle NMF final...\n",
      "2025-04-11 15:05:12,562 - INFO -    Modèle NMF final entraîné avec succès en 5.04s.\n",
      "2025-04-11 15:05:12,565 - INFO -    Erreur de reconstruction finale : 8703.4261\n",
      "2025-04-11 15:05:12,566 - INFO - -> 4. Stockage des résultats pour K=5...\n",
      "2025-04-11 15:05:12,566 - INFO -    Résultats (modèle, W, H) pour K=5 stockés avec succès.\n",
      "2025-04-11 15:05:12,567 - INFO - -> 5. Extraction des mots clés et calcul de la cohérence C_NPMI...\n",
      "2025-04-11 15:05:12,568 - INFO -    Extraction des 10 mots clés principaux...\n",
      "2025-04-11 15:05:12,586 - INFO -    Mots clés principaux du modèle NMF final (K=5) :\n",
      "2025-04-11 15:05:12,591 - INFO -      Topic 0: pouvoir, faire, travail, femme, jeune, euro, loi, ministre, permettre, devoir\n",
      "2025-04-11 15:05:12,591 - INFO -      Topic 1: résider, suisse, belgique, membre, famille, propriétaire, groupe, pdg, paul, hersant\n",
      "2025-04-11 15:05:12,592 - INFO -      Topic 2: france, europe, ukraine, guerre, devoir, russie, peuple, français, poutine, paix\n",
      "2025-04-11 15:05:12,595 - INFO -      Topic 3: macron, emmanuel, voter, candidat, tour, droite, avril, pen, élection, vote\n",
      "2025-04-11 15:05:12,595 - INFO -      Topic 4: pourcent, zemmour, eric, pen, marine, pecresse, sondage, reconquete, français, candidat\n",
      "2025-04-11 15:05:12,596 - INFO -    Calcul de la cohérence C_NPMI (Gensim) pour le modèle final K=5...\n",
      "2025-04-11 15:05:16,825 - INFO -    SCORE DE COHÉRENCE FINAL (C_NPMI) : 0.17018\n",
      "2025-04-11 15:05:16,825 - INFO - ============================================================\n",
      "2025-04-11 15:05:16,825 - INFO - ======= RÉ-ENTRAÎNEMENT ET CALCUL COHÉRENCE TERMINÉS =======\n",
      "2025-04-11 15:05:16,826 - INFO - ============================================================\n",
      "2025-04-11 15:05:16,826 - INFO - Score de cohérence C_NPMI final obtenu pour K=5 : 0.17018\n"
     ]
    }
   ],
   "source": [
    "best_params = best_trial.params\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "# Assurez-vous que toutes les dépendances nécessaires sont importées :\n",
    "# import optuna # Pas directement nécessaire ici, mais best_params vient de là\n",
    "# from sklearn.decomposition import NMF\n",
    "# from gensim.corpora import Dictionary # Si non déjà importé\n",
    "# from gensim.models.coherencemodel import CoherenceModel # Si non déjà importé\n",
    "\n",
    "# --- Configuration du Logging (Assurez-vous qu'il est configuré avant) ---\n",
    "# Exemple:\n",
    "# logging.basicConfig(level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "#                     force=True)\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# --- Assurez-vous que ces variables sont définies AVANT ce bloc ---\n",
    "# ----- Issues d'Optuna -----\n",
    "# best_params: dict # Dictionnaire contenant les meilleurs hyperparamètres trouvés\n",
    "# ----- Données & Pré-traitement -----\n",
    "# tfidf: np.ndarray ou sparse matrix # Matrice TF-IDF des données\n",
    "# n_samples: int # Nombre de documents\n",
    "# n_features: int # Nombre de features (mots)\n",
    "# tfidf_feature_names: list[str] # Liste des noms des features\n",
    "# tokenized_documents: list[list[str]] # Corpus tokenisé (pour Gensim)\n",
    "# ----- Paramètres NMF -----\n",
    "# NMF_INIT_METHOD, NMF_SOLVER: str # Paramètres NMF\n",
    "# MAX_ITER: int # Max iterations pour NMF\n",
    "# ALPHA_W, ALPHA_H, L1_RATIO: float # Paramètres de régularisation NMF\n",
    "# N_TOP_WORDS: int # Nombre de mots clés à afficher par topic\n",
    "# epsilon: float # Petite valeur pour éviter les zéros\n",
    "# RANDOM_SEED, INIT_RANDOM_SEED: int # Graines aléatoires\n",
    "# ----- Fonctions Utilitaires -----\n",
    "# create_decreasing_H, create_decreasing_W: functions # Fonctions d'initialisation\n",
    "# extract_top_words: function # Fonction pour extraire les mots clés\n",
    "# ----- Prérequis pour la Cohérence Gensim -----\n",
    "# calculate_coherence_gensim: function # Fonction calculant la cohérence (acceptant 'texts')\n",
    "# gensim_dictionary: gensim.corpora.Dictionary # Dictionnaire Gensim créé sur tokenized_documents\n",
    "# GENSIM_AVAILABLE: bool # Drapeau indiquant si Gensim est disponible/utilisable\n",
    "# COHERENCE_WINDOW_SIZE: int # Taille de fenêtre pour la cohérence\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Dictionnaires globaux (supposés définis en dehors ou au début du script/notebook)\n",
    "# Ces dictionnaires seront remplis par ce bloc\n",
    "all_nmf_H = {}\n",
    "all_nmf_W = {}\n",
    "nmf_models = {}\n",
    "\n",
    "# Variable pour stocker le score de cohérence final (initialisée)\n",
    "final_coherence_score = None\n",
    "\n",
    "# <<< --- DÉBUT DU BLOC DE RÉ-ENTRAÎNEMENT FINAL ET CALCUL DE COHÉRENCE --- >>>\n",
    "logging.info(\"=\" * 60)\n",
    "logging.info(\" LANCEMENT DU RÉ-ENTRAÎNEMENT FINAL + COHÉRENCE \".center(60, \"=\"))\n",
    "logging.info(f\"Utilisation des meilleurs paramètres trouvés et de graines fixes.\")\n",
    "\n",
    "# Définir les graines fixes pour le ré-entraînement\n",
    "final_model_seed = RANDOM_SEED       # Graine pour le solveur sklearn NMF\n",
    "final_init_seed = INIT_RANDOM_SEED   # Graine pour les fonctions create_decreasing_H/W\n",
    "\n",
    "# Extraire les meilleurs hyperparamètres nécessaires\n",
    "try:\n",
    "    # Récupération de la clé (nombre de topics) et autres paramètres\n",
    "    best_num_topic = best_params['num_topic']\n",
    "    best_h_num_items_decay = best_params['h_num_items_decay']\n",
    "    best_h_power = best_params['h_power']\n",
    "    best_h_noise_level = best_params['h_noise_level']\n",
    "    best_w_power = best_params['w_power']\n",
    "    best_w_noise_level = best_params['w_noise_level']\n",
    "    # S'assurer que 'initial_peak' est récupéré si utilisé par vos fonctions create_\n",
    "    # (Il manquait dans votre snippet original, ajout au cas où)\n",
    "    best_initial_peak = best_params.get('initial_peak', 1.0) # Utiliser une valeur par défaut si non trouvé\n",
    "\n",
    "    logging.info(f\"Meilleurs paramètres récupérés: K={best_num_topic}, h_decay={best_h_num_items_decay}, h_pow={best_h_power:.2f}, ...\")\n",
    "\n",
    "except KeyError as e_key:\n",
    "    logging.error(f\"Erreur: Hyperparamètre clé manquant dans best_params: {e_key}. Arrêt.\")\n",
    "    # Peut-être sortir ou lever une exception plus spécifique\n",
    "    raise SystemExit(f\"Paramètre manquant: {e_key}\")\n",
    "except NameError:\n",
    "    logging.error(\"Erreur: La variable 'best_params' n'est pas définie. Avez-vous lancé Optuna avant ? Arrêt.\")\n",
    "    # Peut-être sortir ou lever une exception plus spécifique\n",
    "    raise SystemExit(\"Variable 'best_params' non définie.\")\n",
    "\n",
    "\n",
    "# --- Vérification des prérequis critiques avant de continuer ---\n",
    "critical_vars = ['tfidf', 'n_samples', 'n_features', 'tfidf_feature_names',\n",
    "                 'tokenized_documents', 'create_decreasing_H', 'create_decreasing_W',\n",
    "                 'extract_top_words', 'calculate_coherence_gensim', 'gensim_dictionary',\n",
    "                 'epsilon', 'NMF_INIT_METHOD', 'NMF_SOLVER', 'MAX_ITER',\n",
    "                 'ALPHA_W', 'ALPHA_H', 'L1_RATIO', 'N_TOP_WORDS', 'RANDOM_SEED',\n",
    "                 'INIT_RANDOM_SEED', 'GENSIM_AVAILABLE', 'COHERENCE_WINDOW_SIZE']\n",
    "missing_vars = [var for var in critical_vars if var not in globals()]\n",
    "if missing_vars:\n",
    "    logging.error(f\"Erreur: Variables/Fonctions critiques MANQUANTES : {', '.join(missing_vars)}. Arrêt.\")\n",
    "    raise SystemExit(f\"Variables/Fonctions manquantes : {', '.join(missing_vars)}\")\n",
    "\n",
    "\n",
    "# 1. Recréer les matrices d'initialisation H et W\n",
    "final_H_init, final_W_init = None, None\n",
    "expected_H_shape = (best_num_topic, n_features)\n",
    "expected_W_shape = (n_samples, best_num_topic)\n",
    "logging.info(f\"-> 1. Recréation des matrices H/W pour K={best_num_topic} avec seed_init={final_init_seed}...\")\n",
    "try:\n",
    "    final_H_init = create_decreasing_H(\n",
    "        best_num_topic, n_features, best_h_num_items_decay,\n",
    "        best_h_power, best_h_noise_level,\n",
    "        final_init_seed, epsilon, best_initial_peak # Ajout de initial_peak\n",
    "    )\n",
    "    final_W_init = create_decreasing_W(\n",
    "        n_samples, best_num_topic, best_w_power,\n",
    "        best_w_noise_level, final_init_seed, epsilon, best_initial_peak # Ajout de initial_peak\n",
    "    )\n",
    "\n",
    "    if final_W_init is None or final_H_init is None:\n",
    "        raise ValueError(\"Échec de la création des matrices d'init finales (résultat None).\")\n",
    "    if final_W_init.shape != expected_W_shape or final_H_init.shape != expected_H_shape:\n",
    "        raise ValueError(f\"Dimensions initialisation finales INCOHÉRENTES: W={final_W_init.shape} attendu {expected_W_shape}, H={final_H_init.shape} attendu {expected_H_shape}\")\n",
    "    logging.info(\"   Matrices d'initialisation finales H/W créées avec succès.\")\n",
    "\n",
    "except Exception as e_init_final:\n",
    "    logging.error(f\"   ERREUR lors de la création/validation des matrices d'initialisation finales : {e_init_final}\", exc_info=True)\n",
    "    raise # Arrêter le processus si l'initialisation échoue\n",
    "\n",
    "# 2. Initialiser le modèle NMF final\n",
    "logging.info(f\"-> 2. Initialisation du modèle NMF final (K={best_num_topic}) avec random_state={final_model_seed}...\")\n",
    "final_nmf_model = None # Initialiser à None avant le try\n",
    "try:\n",
    "    # Utilisation de la graine fixe RANDOM_SEED pour le random_state de NMF\n",
    "    final_nmf_model = NMF(\n",
    "        n_components=best_num_topic,\n",
    "        init=NMF_INIT_METHOD, # 'custom'\n",
    "        solver=NMF_SOLVER,\n",
    "        random_state=final_model_seed, # <--- GRAINE FIXE ICI\n",
    "        max_iter=MAX_ITER,             # Vous pourriez augmenter max_iter ici si désiré\n",
    "        alpha_W=ALPHA_W,\n",
    "        alpha_H=ALPHA_H,\n",
    "        l1_ratio=L1_RATIO,\n",
    "        # tol=1e-4 # Vous pouvez définir une tolérance fixe ici aussi\n",
    "    )\n",
    "    logging.info(\"   Modèle NMF final initialisé.\")\n",
    "except Exception as e_nmf_final_init:\n",
    "    logging.error(f\"   ERREUR lors de l'initialisation du modèle NMF final: {e_nmf_final_init}\", exc_info=True)\n",
    "    raise # Arrêter le processus\n",
    "\n",
    "# 3. Entraîner (fit) le modèle NMF final\n",
    "logging.info(\"-> 3. Entraînement (fit) du modèle NMF final...\")\n",
    "start_final_fit = time.time()\n",
    "W_final_fit = None # Initialiser à None avant le try/catch\n",
    "H_final_fit = None # Initialiser à None avant le try/catch\n",
    "training_successful = False\n",
    "try:\n",
    "    # Important : utiliser .copy() car NMF(init='custom') modifie W et H en place\n",
    "    W_final_fit = final_W_init.copy()\n",
    "    H_final_fit = final_H_init.copy()\n",
    "\n",
    "    # Entraînement du modèle. W_final_fit et H_final_fit seront modifiés en place.\n",
    "    final_nmf_model.fit(tfidf, W=W_final_fit, H=H_final_fit)\n",
    "\n",
    "    duration_final_fit = time.time() - start_final_fit\n",
    "    logging.info(f\"   Modèle NMF final entraîné avec succès en {duration_final_fit:.2f}s.\")\n",
    "    logging.info(f\"   Erreur de reconstruction finale : {final_nmf_model.reconstruction_err_:.4f}\")\n",
    "    training_successful = True\n",
    "\n",
    "    # Stockage des résultats si le fit a réussi\n",
    "    logging.info(f\"-> 4. Stockage des résultats pour K={best_num_topic}...\")\n",
    "    all_nmf_H[best_num_topic] = final_nmf_model.components_\n",
    "    all_nmf_W[best_num_topic] = W_final_fit # W a été modifié en place\n",
    "    nmf_models[best_num_topic] = final_nmf_model\n",
    "    logging.info(f\"   Résultats (modèle, W, H) pour K={best_num_topic} stockés avec succès.\")\n",
    "\n",
    "except Exception as e_fit_final:\n",
    "    logging.error(f\"   ERREUR durant l'entraînement (fit) du modèle NMF final: {e_fit_final}\", exc_info=True)\n",
    "    # Pas de stockage si le fit échoue\n",
    "\n",
    "# 5. Afficher les mots clés et calculer la cohérence (SEULEMENT si l'entraînement a réussi)\n",
    "if training_successful and best_num_topic in nmf_models:\n",
    "    logging.info(f\"-> 5. Extraction des mots clés et calcul de la cohérence C_NPMI...\")\n",
    "    try:\n",
    "        # Récupérer le modèle et la matrice H depuis les dictionnaires/variables\n",
    "        stored_model = nmf_models[best_num_topic]\n",
    "        stored_H = all_nmf_H[best_num_topic] # Ou utiliser stored_model.components_\n",
    "\n",
    "        # Extraction des mots clés\n",
    "        logging.info(f\"   Extraction des {N_TOP_WORDS} mots clés principaux...\")\n",
    "        final_topic_words = extract_top_words(stored_H, tfidf_feature_names, N_TOP_WORDS)\n",
    "\n",
    "        if not final_topic_words:\n",
    "             logging.warning(\"   Aucun mot clé n'a été extrait pour le modèle final. Impossible d'afficher ou de calculer la cohérence.\")\n",
    "        else:\n",
    "            # Affichage des mots clés\n",
    "            logging.info(f\"   Mots clés principaux du modèle NMF final (K={best_num_topic}) :\")\n",
    "            for i, words in enumerate(final_topic_words):\n",
    "                logging.info(f\"     Topic {i}: {', '.join(words)}\") # Affichage plus lisible\n",
    "\n",
    "            # Calcul de la cohérence C_NPMI sur le modèle final\n",
    "            logging.info(f\"   Calcul de la cohérence C_NPMI (Gensim) pour le modèle final K={best_num_topic}...\")\n",
    "            if GENSIM_AVAILABLE and gensim_dictionary:\n",
    "                try:\n",
    "                    # Appel de la fonction de calcul de cohérence\n",
    "                    calculated_score = calculate_coherence_gensim(\n",
    "                        topics=final_topic_words,\n",
    "                        tokenized_corpus=tokenized_documents,    # Corpus tokenisé\n",
    "                        gensim_dictionary=gensim_dictionary, # Dictionnaire Gensim\n",
    "                        window_size=COHERENCE_WINDOW_SIZE    # Taille de fenêtre\n",
    "                        # coherence_type='c_npmi' # si votre fonction le gère\n",
    "                    )\n",
    "\n",
    "                    # Vérification du résultat\n",
    "                    if np.isnan(calculated_score):\n",
    "                        logging.warning(\"   Le calcul de cohérence a retourné NaN.\")\n",
    "                        final_coherence_score = np.nan # Ou None\n",
    "                    else:\n",
    "                        final_coherence_score = calculated_score\n",
    "                        logging.info(f\"   SCORE DE COHÉRENCE FINAL (C_NPMI) : {final_coherence_score:.5f}\")\n",
    "\n",
    "                except TypeError as e_coh_type:\n",
    "                     # Gérer spécifiquement l'erreur si 'texts'/'tokenized_corpus' n'est pas le bon nom d'argument\n",
    "                     logging.error(f\"   ERREUR de type durant calcul cohérence final : {e_coh_type}. Vérifiez les arguments de 'calculate_coherence_gensim'.\", exc_info=False)\n",
    "                except NameError as e_name:\n",
    "                     logging.error(f\"   ERREUR - Variable MANQUANTE durant calcul cohérence final ({e_name}).\", exc_info=False)\n",
    "                except Exception as e_coh:\n",
    "                    logging.error(f\"   ERREUR durant calcul cohérence final : {e_coh}\", exc_info=False)\n",
    "            else:\n",
    "                if not GENSIM_AVAILABLE:\n",
    "                    logging.warning(\"   Gensim non disponible. Score de cohérence final non calculé.\")\n",
    "                elif not gensim_dictionary:\n",
    "                    logging.warning(\"   Dictionnaire Gensim non disponible/créé. Score de cohérence final non calculé.\")\n",
    "\n",
    "    except NameError as e_name_final:\n",
    "        logging.warning(f\"   Attention : Variable MANQUANTE lors de l'extraction/calcul final pour K={best_num_topic}: {e_name_final}\")\n",
    "    except Exception as e_final_process:\n",
    "        logging.warning(f\"   Attention : Erreur lors de l'extraction/affichage/calcul final pour K={best_num_topic}: {e_final_process}\")\n",
    "else:\n",
    "    if not training_successful:\n",
    "        logging.warning(\"Le modèle final n'a pas été entraîné avec succès. Impossible d'extraire les mots clés ou de calculer la cohérence.\")\n",
    "    else: # Implique que best_num_topic n'est pas dans nmf_models, ce qui ne devrait pas arriver si training_successful est True\n",
    "         logging.error(\"Incohérence : Entraînement marqué comme réussi, mais modèle non trouvé dans le dictionnaire.\")\n",
    "\n",
    "logging.info(\"=\" * 60)\n",
    "logging.info(\" RÉ-ENTRAÎNEMENT ET CALCUL COHÉRENCE TERMINÉS \".center(60, \"=\"))\n",
    "logging.info(\"=\" * 60)\n",
    "\n",
    "# Affichage final du score (s'il a été calculé)\n",
    "if final_coherence_score is not None and not np.isnan(final_coherence_score):\n",
    "    logging.info(f\"Score de cohérence C_NPMI final obtenu pour K={best_num_topic} : {final_coherence_score:.5f}\")\n",
    "elif np.isnan(final_coherence_score):\n",
    "    logging.warning(\"Le calcul de cohérence final a résulté en NaN.\")\n",
    "else:\n",
    "    logging.warning(\"Le score de cohérence final n'a pas pu être calculé (voir logs précédents pour la raison).\")\n",
    "\n",
    "# Optionnel: Vérifier que les dictionnaires ont été remplis (si training_successful)\n",
    "if training_successful:\n",
    "    logging.debug(f\"Clés dans nmf_models: {list(nmf_models.keys())}\")\n",
    "    logging.debug(f\"Clés dans all_nmf_W: {list(all_nmf_W.keys())}\")\n",
    "    logging.debug(f\"Clés dans all_nmf_H: {list(all_nmf_H.keys())}\")\n",
    "    if best_num_topic in all_nmf_H:\n",
    "         logging.debug(f\"Shape H pour K={best_num_topic}: {all_nmf_H[best_num_topic].shape}\")\n",
    "    if best_num_topic in all_nmf_W:\n",
    "         logging.debug(f\"Shape W pour K={best_num_topic}: {all_nmf_W[best_num_topic].shape}\")\n",
    "# <<< --- FIN DU BLOC --- >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 4.3.3\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: https://radimrehurek.com/gensim/\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: /usr/local/lib/python3.12/site-packages\n",
      "Requires: numpy, scipy, smart-open\n",
      "Required-by: octis\n"
     ]
    }
   ],
   "source": [
    "!python -m pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0xfffe974e7b00>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP_k5UM8lyID"
   },
   "source": [
    "##**NMF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "261f2cae5c8a4622b7d4ad0d206dc27f",
      "c69192f9e8af45bda4e4fdcd7010da96",
      "966f1728dd0440fba83052119019a784",
      "b262477dba9c4aef9cf44db10544ca4f",
      "10ca8c888aab4e32a0f2937c3feeaaf0",
      "300611a2ba2e420b9811e3221f5d3300",
      "2ea5ee687ed947c2b92135749ed74b39",
      "e5c9192050e440f0ba82630612efe4cd",
      "072cce547935406fb348e5d49f8d216b",
      "f21e890af6fc4c449429189c9ce8c44f",
      "34b6531480074258ae4491bfa645de64"
     ]
    },
    "executionInfo": {
     "elapsed": 152243,
     "status": "ok",
     "timestamp": 1741771117812,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "QGZjdjA6WMUO",
    "outputId": "4920f716-9c43-4f99-dcdf-b742fc99950b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeef7b55abec4b3798ea2bf07ec3711a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSUS DES TOPICS:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialisation des variables globales utilisées pour stocker les résultats de la factorisation NMF et les données associées.\n",
    "\n",
    "# all_nmf_H : Dictionnaire global où les matrices H (composantes thématiques) générées par le modèle NMF\n",
    "# pour chaque nombre de topics sont stockées. La clé correspond au nombre de topics (e.g., 5, 10, etc.),\n",
    "# et la valeur est la matrice H générée pour ce nombre de topics.\n",
    "all_nmf_H = {}\n",
    "\n",
    "# all_nmf_W : Dictionnaire global où les matrices W (représentant les documents dans l'espace des topics)\n",
    "# sont stockées. Chaque clé correspond à un nombre de topics, et chaque valeur est la matrice W associée.\n",
    "all_nmf_W = {}\n",
    "\n",
    "# coherence_scores : Dictionnaire global associant à chaque nombre de topics un score de cohérence\n",
    "# calculé par une métrique de type “fenêtre glissante” (par exemple c_npmi ou c_uci).\n",
    "# Nous utilisons ici NMF, car il est fréquent qu'un même document soit associé à plusieurs topics.\n",
    "# Dans cette situation, mesurer la qualité à l'échelle d'un document entier (doc-based) pourrait\n",
    "# masquer des co-occurrences thématiques plus fines. Une approche en fenêtre glissante s'avère\n",
    "# donc plus appropriée pour évaluer la cohérence locale des mots-clés liés à chaque topic.\n",
    "coherence_scores = {}\n",
    "\n",
    "# Appel de la fonction determine_nmf avec une liste de nombres de topics à tester ([10, 15]),\n",
    "# en spécifiant les nouveaux paramètres alpha_W, alpha_H et l1_ratio.\n",
    "# La fonction accepte également n_top_words et window_size si besoin (avec des valeurs par défaut de 15 et 100).\n",
    "# Exemple d’appel pour tester respectivement 10 et 15 topics :\n",
    "# determine_nmf([5, 7, 10, 12, 15, 20], alpha_W=0.3, alpha_H=0.3, l1_ratio=0.0, n_top_words=15, window_size=100)\n",
    "nmf_models = determine_nmf([7, 12, 15], alpha_W=0.0, alpha_H=0.0, l1_ratio=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10349.582406870564\n",
      "10297.523126394293\n",
      "10271.301325119022\n"
     ]
    }
   ],
   "source": [
    "for nb_topics in nmf_models:\n",
    "    print(nmf_models[nb_topics].reconstruction_err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1741771995816,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "0HFAjz00hQv4",
    "outputId": "3c656b77-0271-46fa-9477-e65a7eb5270a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7: 0.12059232985642639, 12: 0.11454644725085544, 15: 0.1106162920574208}\n"
     ]
    }
   ],
   "source": [
    "print(coherence_scores) # IL FAUT LES METTRE SUR LE DISQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aller chopper automatiquement le max c_npmi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These texts specifically originate from American right-wing YouTube content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprompt = \"Ces textes proviennent de comptes politiques actifs sur les réseaux sociaux français pendant la campagne de l'élection présidentielle de 2022.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "R_DwxX4qYdQD"
   },
   "outputs": [],
   "source": [
    "# Ce script est conçu pour aider les analystes à extraire des phrases représentatives\n",
    "# après l'entraînement d'un modèle NMF. L’objectif global est de sélectionner,\n",
    "# pour chaque thème (ou topic) identifié par le NMF, un ensemble de phrases\n",
    "# pertinentes et peu redondantes. Pour cela, le code met à jour des listes de\n",
    "# termes unigrams, génère une matrice TF-IDF de l’ensemble des phrases, puis\n",
    "# utilise les scores NMF pour repérer les phrases les plus discriminantes de chaque\n",
    "# topic, en filtrant celles qui sont trop similaires entre elles. Les sorties,\n",
    "# notamment les phrases et leurs scores de pertinence, permettent ensuite\n",
    "# de réaliser des synthèses qualitatives sur les topics détectés.\n",
    "topic_labels_by_config = extract_relevant_sentences_and_titles(nmf_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eCDROO8sYdNd"
   },
   "outputs": [],
   "source": [
    "# Ce code enregistre sur le disque l'esnsemble des matrices H (topics-termes) et\n",
    "# des matrices W (documents-topics) sur le disque. Il y a donc une matrice H et\n",
    "# une matrice W par configuration (nombre de topics). Ce sont les seuls objets\n",
    "# dont nous aurons besoin pour les analyses, en plus des données du corpus.\n",
    "with open(results_path + base_name + '_RAW/all_nmf_W.pkl', 'wb') as f:\n",
    "    pickle.dump(all_nmf_W, f)\n",
    "\n",
    "with open(results_path + base_name + '_RAW/all_nmf_H.pkl', 'wb') as f:\n",
    "    pickle.dump(all_nmf_H, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMoaOSrjGMRN"
   },
   "source": [
    "#**PHASE D'ANALYSES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Wft1gUeCar-"
   },
   "source": [
    "## **RÉCUPÉRATION DES MATRICES H ET W ENREGISTRÉES SUR LE DISQUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "error",
     "timestamp": 1741727225821,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "gpGLrUbJlZ33",
    "outputId": "a1c528b5-d366-4d98-e396-63ce214790c9"
   },
   "outputs": [],
   "source": [
    "# Récupération des matrices W et H depuis le disque.\n",
    "with open(results_path + base_name + '_RAW/all_nmf_W.pkl', 'rb') as f:\n",
    "    all_nmf_W = pickle.load(f)\n",
    "\n",
    "with open(results_path + base_name + '_RAW/all_nmf_H.pkl', 'rb') as f:\n",
    "    all_nmf_H = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhzO_mUDmG5y"
   },
   "source": [
    "##**EXTRACTION DES PHRASES CARACTÉRISTIQUES**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CHxO_MFwYd52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacee7c073ea4e318a1a6219e4b12d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ÉCRITURE DES FICHIERS SUR LE DISQUE:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "write_documents_infos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYC2GAShpeHU"
   },
   "source": [
    "##**DYNAMIQUE DES TOPICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30942,
     "status": "ok",
     "timestamp": 1739878654428,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "RRwDK00oJjds",
    "outputId": "625cdf53-7223-414e-a97f-dfd925beb10c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ab7a3d395b43078170e6b8a46842ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CONFIGURATIONS PROCESSÉES:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca53b5685cd47688a95f71e4cdf5c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CONFIGURATIONS PROCESSÉES:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc2bbf93131409c9d8457f9f3850dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CONFIGURATIONS PROCESSÉES:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6004af555b6749d3a7ccfdf7895951aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CONFIGURATIONS PROCESSÉES:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Le paramètre `sigma` détermine le niveau de lissage de la distribution des scores des topics.\n",
    "# Lorsque `sigma='auto'`, la fonction tente de déterminer automatiquement un lissage \"optimal\"\n",
    "# en se basant sur la moyenne des écarts-types des valeurs dans chaque période temporelle.\n",
    "# Cependant, cette approche n'est pas toujours idéale, et il peut être nécessaire d'expérimenter\n",
    "# avec différentes valeurs de `sigma`. Si `sigma=1`, aucun lissage n'est appliqué, laissant\n",
    "# les valeurs brutes du DataFrame intactes.\n",
    "#\n",
    "# Il est important de noter qu'il n'existe pas de lissage \"optimal\" universel. Le lissage permet\n",
    "# de rendre les dynamiques des topics plus intelligibles visuellement, en supprimant les variations\n",
    "# erratiques et en montrant les tendances générales des topics au fil du temps. Par exemple,\n",
    "# cela peut aider à mettre en évidence un topic dominant au début ou à la fin de la période analysée.\n",
    "#\n",
    "# Cependant, un lissage trop fort peut masquer des variations significatives au sein des données\n",
    "# et donner une impression erronée de stabilité. Une fois lissée, il devient plus difficile\n",
    "# de distinguer si une zone de forte intensité est due à une présence soutenue du topic tout au\n",
    "# long de la période ou à un événement ponctuel particulièrement intense.\n",
    "for vertical_normalization in [False, True]:\n",
    "    for horizontal_normalization in [False, True]:\n",
    "        create_chrono_topics(sigma='auto', \n",
    "                             apply_vertical_normalization=vertical_normalization,\n",
    "                             apply_horizontal_normalization=horizontal_normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekryLTvSM_P1"
   },
   "source": [
    "##**DYNAMIQUE DE GROUPES**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739878654442,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "4gc1HIh2oQyY",
    "outputId": "acf113f4-669f-44f9-c39e-25e2f2929aa4"
   },
   "outputs": [],
   "source": [
    "# Le paramètre `sigma` détermine le niveau de lissage de la distribution des scores des journaux ou autres entités au fil du temps.\n",
    "# Lorsque `sigma='auto'`, la fonction tente de déterminer automatiquement un lissage \"optimal\" basé sur la moyenne\n",
    "# des écarts-types des valeurs dans chaque période. Cependant, cette méthode n'est pas toujours efficace, et il\n",
    "# peut être utile d'expérimenter avec différentes valeurs de `sigma`.\n",
    "#\n",
    "# Par exemple, si `sigma=1`, aucun lissage n'est appliqué et les valeurs brutes sont utilisées, laissant ainsi\n",
    "# apparaître toutes les fluctuations, même mineures. Si `sigma=30`, cela signifie que l'on applique un fort lissage\n",
    "# qui va aplanir considérablement les données, en supprimant une bonne partie des variations quotidiennes et\n",
    "# en montrant plutôt des tendances à plus long terme. Plus `sigma` est grand, plus la courbe est \"étalée\" et\n",
    "# lissée, ce qui permet de voir des tendances générales, mais au détriment de la détection des pics ponctuels.\n",
    "#\n",
    "# Il est important de souligner qu'il n'existe pas de lissage \"optimal\" universel. Le lissage permet de rendre\n",
    "# les dynamiques de la publication des journaux plus intelligibles en supprimant les fluctuations erratiques\n",
    "# et en exposant les tendances générales des journaux au fil du temps. Par exemple, cela permet de visualiser\n",
    "# les périodes où certains journaux dominent la couverture médiatique.\n",
    "#\n",
    "# Cependant, un lissage trop important peut atténuer des variations cruciales dans les données et nuire à\n",
    "# l'analyse des événements ponctuels ou des périodes d'intensité médiatique accrue. Une fois lissée,\n",
    "# il devient difficile de savoir si une zone de forte intensité est le résultat d'une couverture continue\n",
    "# ou d'un événement médiatique exceptionnellement intense.\n",
    "#\n",
    "# Le paramètre `group_column` est pertinent uniquement lorsque `source_type='csv'`. Dans ce cas, il s'agit du\n",
    "# nom (string) d'une colonne catégorielle sur laquelle regrouper les données, similaire à la colonne \"journal\"\n",
    "# dans le cas de Istex ou de Europresse. Par exemple, si votre CSV contient une colonne \"publisher\" qui répertorie le nom de\n",
    "# l'éditeur ou de la source du document, vous pouvez utiliser `group_column='publisher'` pour agréger\n",
    "# temporellement les données par éditeur. De même, si vous avez une colonne \"theme\" pour catégoriser\n",
    "# les articles par sujet, `group_column='theme'` permettra de visualiser l'évolution temporelle\n",
    "# par sujet. Si `group_column` n'est pas fourni ou n'existe pas dans `columns_dict`, un message\n",
    "# d'erreur est affiché et la fonction s'interrompt. Ce mécanisme permet de créer une agrégation temporelle\n",
    "# par catégorie, facilitant l'analyse par groupes définis (par exemple, par titre de publication, thème, pays, etc.).\n",
    "for vertical_normalization in [False, True]:\n",
    "    for horizontal_normalization in [False, True]:\n",
    "        create_chrono_group_column(group_column='political_party', \n",
    "                                   sigma='auto', \n",
    "                                   apply_vertical_normalization=vertical_normalization,\n",
    "                                   apply_horizontal_normalization=horizontal_normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QTDTgkFatMT"
   },
   "source": [
    "## **SPATIALISATION DES TOPICS PAR PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-P1ytfCYH0Zl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impossible d'obtenir un renderer valide initialement (FontProperties.__init__() got an unexpected keyword argument '_internal.classic_mode'), forçage d'un dessin...\n",
      "[Passe 1] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 1] Lancement de la résolution...\n",
      "[Passe 1] Résolution terminée avec le statut : 0\n",
      "[Passe 1] Distance minimale trouvée (approximative) : 0.0\n",
      "[Passe 2] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 2] Lancement de la résolution...\n",
      "[Passe 2] Résolution terminée avec le statut : 0\n",
      "[2passes] Solution finale extraite avec 7 labels placés.\n",
      "Impossible d'obtenir un renderer valide initialement (FontProperties.__init__() got an unexpected keyword argument '_internal.classic_mode'), forçage d'un dessin...\n",
      "[Passe 1] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 1] Lancement de la résolution...\n",
      "[Passe 1] Résolution terminée avec le statut : 0\n",
      "[Passe 1] Distance minimale trouvée (approximative) : 0.08376\n",
      "[Passe 2] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 2] Lancement de la résolution...\n",
      "[Passe 2] Résolution terminée avec le statut : 0\n",
      "[2passes] Solution finale extraite avec 12 labels placés.\n",
      "Impossible d'obtenir un renderer valide initialement (FontProperties.__init__() got an unexpected keyword argument '_internal.classic_mode'), forçage d'un dessin...\n",
      "[Passe 1] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 1] Lancement de la résolution...\n",
      "[Passe 1] Résolution terminée avec le statut : 0\n",
      "[Passe 1] Distance minimale trouvée (approximative) : 0.0\n",
      "[Passe 2] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 2] Lancement de la résolution...\n",
      "[Passe 2] Résolution terminée avec le statut : 0\n",
      "[2passes] Solution finale extraite avec 15 labels placés.\n",
      "Impossible d'obtenir un renderer valide initialement (FontProperties.__init__() got an unexpected keyword argument '_internal.classic_mode'), forçage d'un dessin...\n",
      "[Passe 1] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 1] Lancement de la résolution...\n",
      "[Passe 1] Résolution terminée avec le statut : 0\n",
      "[Passe 1] Distance minimale trouvée (approximative) : 0.0\n",
      "[Passe 2] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 2] Lancement de la résolution...\n",
      "[Passe 2] Résolution terminée avec le statut : 0\n",
      "[2passes] Solution finale extraite avec 7 labels placés.\n",
      "Impossible d'obtenir un renderer valide initialement (FontProperties.__init__() got an unexpected keyword argument '_internal.classic_mode'), forçage d'un dessin...\n",
      "[Passe 1] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 1] Lancement de la résolution...\n",
      "[Passe 1] Résolution terminée avec le statut : 0\n",
      "[Passe 1] Distance minimale trouvée (approximative) : 0.0\n",
      "[Passe 2] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 2] Lancement de la résolution...\n",
      "[Passe 2] Résolution terminée avec le statut : 0\n",
      "[2passes] Solution finale extraite avec 12 labels placés.\n",
      "Impossible d'obtenir un renderer valide initialement (FontProperties.__init__() got an unexpected keyword argument '_internal.classic_mode'), forçage d'un dessin...\n",
      "[Passe 1] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 1] Lancement de la résolution...\n",
      "[Passe 1] Résolution terminée avec le statut : 0\n",
      "[Passe 1] Distance minimale trouvée (approximative) : 0.0\n",
      "[Passe 2] Application d'une limite de temps de 600000 ms.\n",
      "[Passe 2] Lancement de la résolution...\n",
      "[Passe 2] Résolution terminée avec le statut : 0\n",
      "[2passes] Solution finale extraite avec 15 labels placés.\n"
     ]
    }
   ],
   "source": [
    "# Cette fonction vise à analyser et visualiser les relations entre topics en appliquant une PCA sur les matrices\n",
    "# produites par la factorisation NMF (matrices W et H), chaque projection ayant un objectif distinct :\n",
    "#\n",
    "# 1) **PCA sur W** :\n",
    "#    - La matrice W décrit comment les topics sont distribués à travers l'ensemble des documents.\n",
    "#    - En appliquant une PCA sur W, nous cherchons à rapprocher les topics qui partagent une distribution\n",
    "#      similaire sur les documents, c'est-à-dire ceux qui apparaissent de manière conjointe dans les mêmes\n",
    "#      ensembles documentaires. Cela fournit une vision \"macro\" des relations entre topics dans l'espace documentaire.\n",
    "#\n",
    "# 2) **PCA sur H** :\n",
    "#    - La matrice H décrit comment les mots contribuent à la définition de chaque topic.\n",
    "#    - En appliquant une PCA sur H, nous cherchons à rapprocher les topics ayant un vocabulaire similaire,\n",
    "#      c'est-à-dire ceux qui partagent des mots-clés ou des caractéristiques linguistiques proches.\n",
    "#      Cette analyse met en évidence des proximités sémantiques entre les topics.\n",
    "#\n",
    "# Ces deux visualisations permettent d'explorer les relations entre topics sous deux angles complémentaires :\n",
    "# - La PCA sur W éclaire les co-apparitions thématiques dans les documents.\n",
    "# - La PCA sur H révèle les similitudes lexicales ou sémantiques entre topics.\n",
    "#\n",
    "# En somme, cette étape enrichit l'interprétation des topics NMF en offrant une vue synthétique de leurs\n",
    "# relations documentaires et lexicales.\n",
    "plot_pca('W')\n",
    "plot_pca('H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPgCltOL9NtP"
   },
   "source": [
    "## **ANALYSE DE SENTIMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 68448,
     "status": "error",
     "timestamp": 1739878726218,
     "user": {
      "displayName": "Jérémie Garrigues",
      "userId": "14912292466158867073"
     },
     "user_tz": -60
    },
    "id": "mNMRH_5cqxB6",
    "outputId": "bb120e9f-82fb-49b3-f554-f861abfe194c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a7d0262ec5495c86317f9e4632690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Documents:   0%|          | 0/86338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f30b775b7cc43c890f40c55820afc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cd0d8c4cf3473aba10fb64a4ac42d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be3e09bb13948d5851b18d523716120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a812757311a742e39287e488d26a9d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24488f2b9b604d1eb8bad773848f7d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8c42ea70cc4febaf91b613c6ff1a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cette fonction effectue une analyse de sentiments sur une liste de documents textuels, en s’appuyant\n",
    "# sur un modèle BERT multilingue (nlptown/bert-base-multilingual-uncased-sentiment). Elle parcourt chaque texte,\n",
    "# le tokenize et le passe au pipeline d’analyse afin d’obtenir une note d’étoiles (de 1 à 5) reflétant\n",
    "# le sentiment.\n",
    "#\n",
    "# Attention : par défaut, le modèle BERT utilisé est limité à 512 tokens. Si le texte dépasse cette limite,\n",
    "# il sera automatiquement tronqué aux 512 premiers tokens par le tokenizer et le pipeline. Par conséquent,\n",
    "# seuls ces 512 premiers tokens seront pris en compte dans le calcul du sentiment, ce qui peut potentiellement\n",
    "# biaiser l’analyse pour les textes très longs. Si un traitement plus complet est nécessaire, il conviendra\n",
    "# de segmenter le texte en plusieurs morceaux de taille inférieure ou égale à 512 tokens et d’analyser\n",
    "# chaque segment individuellement.\n",
    "#\n",
    "# Après l’analyse, la fonction calcule une moyenne des scores des documents, transforme les dates,\n",
    "# puis génère des graphiques de sentiment dans le temps et sous forme de cartes thermiques (heatmaps)\n",
    "# en fonction de différents paramètres.\n",
    "process_sentiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO-u3ChN4fno"
   },
   "source": [
    "##**FORÊTS ALÉATOIRES AVEC ANALYSE DES RÉSIDUS NORMALISÉS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "tELAclevPNiV"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f957ad7b0c423c80d14b139e59f267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing topics:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a95e2eda4140a496523572dc627d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing topics:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45056e94e02c4f1da4def60ac7b53401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing topics:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_box_plots(group_column='political_party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "mj373B1wAfWF"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b418b927620449d98420212a7022a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RÉGRESSIONS : ANALYSE DES RÉSIDUS:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036b194f450a463d85bac6d0165e7470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RÉGRESSIONS : ANALYSE DES RÉSIDUS:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180d101451c247cf82b0f62fd2d39752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RÉGRESSIONS : ANALYSE DES RÉSIDUS:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_forests_residuals_analysis(group_column='political_party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtJaMURad-S-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad2-cMA0d-Qn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7GCclCvd-OE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLNDddPFzYlQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "072cce547935406fb348e5d49f8d216b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0b05464c072c435fa7dc4d80172a633e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c4c95e25a114e10beeab3b484b61cd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0dc48acef57f428f972e75ebfa50338e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbdfb3a58d8f4388bfd1b771bab3bb9b",
      "placeholder": "​",
      "style": "IPY_MODEL_257f2a9bb9a7453dbb59546de453ad76",
      "value": "DOCUMENTS PROCESSÉS: 100%"
     }
    },
    "10ca8c888aab4e32a0f2937c3feeaaf0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "199d5c7aad2b4945b43d4459d594a2c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "257f2a9bb9a7453dbb59546de453ad76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "261f2cae5c8a4622b7d4ad0d206dc27f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c69192f9e8af45bda4e4fdcd7010da96",
       "IPY_MODEL_966f1728dd0440fba83052119019a784",
       "IPY_MODEL_b262477dba9c4aef9cf44db10544ca4f"
      ],
      "layout": "IPY_MODEL_10ca8c888aab4e32a0f2937c3feeaaf0"
     }
    },
    "284ec8e15a2a4c839f436d8d20c8268c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b5fb742bc2f42fcbdaa176eb3194b62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e43333161254325a19e8285edbb93ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_284ec8e15a2a4c839f436d8d20c8268c",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f477fbb55f140e9856f82f4d38ed312",
      "value": 3
     }
    },
    "2ea5ee687ed947c2b92135749ed74b39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "300611a2ba2e420b9811e3221f5d3300": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34b6531480074258ae4491bfa645de64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40bbe11df03b4da3b011249554f5a4af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5245b6f1cf074219aba13aadb968246f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "556e919d627c4e13a3e4192a85cdf6e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "592368db4fcb41849664dc13fe64be77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b05464c072c435fa7dc4d80172a633e",
      "placeholder": "​",
      "style": "IPY_MODEL_b6bf255c06a9413d8060331dddf1ccf5",
      "value": " 128623/128623 [00:01&lt;00:00, 116810.84it/s]"
     }
    },
    "5945ec1d12e84726bb59acd843647bba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "634033e7fbe345c58d7dfd276a2f2789": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "660a0dda97ca416e8b97ce48adf066ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a52320dd96d4e0c89f9b16d75772ff0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5ac99fc9e7747808608e466b3fb6172",
      "max": 128623,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c4c95e25a114e10beeab3b484b61cd3",
      "value": 128623
     }
    },
    "6c798065d3d34f33a5cb4ef3a86352a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4f4c50162da4955884c79c8bf78db39",
      "placeholder": "​",
      "style": "IPY_MODEL_5945ec1d12e84726bb59acd843647bba",
      "value": " 1/1 [01:09&lt;00:00, 30.80s/it]"
     }
    },
    "6d411e848df049acba679d8f6ac021f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b951c23a57b24298bbfc84a946219bd5",
       "IPY_MODEL_ff08f96f6c68498699fa5aa86e5c3f50",
       "IPY_MODEL_6c798065d3d34f33a5cb4ef3a86352a3"
      ],
      "layout": "IPY_MODEL_813a0096c0a14544bf8587197047d519"
     }
    },
    "6ed24d4ab2b34844ac740f610899f716": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bdd47d4986834f368168d34776965aa5",
       "IPY_MODEL_2e43333161254325a19e8285edbb93ce",
       "IPY_MODEL_8247fe49192a47c88da052388b309c9a"
      ],
      "layout": "IPY_MODEL_993e44343c394e46996719dc596d9468"
     }
    },
    "6f477fbb55f140e9856f82f4d38ed312": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "72bf281e72104988b8af544dcef9cbe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_199d5c7aad2b4945b43d4459d594a2c7",
      "max": 128623,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_660a0dda97ca416e8b97ce48adf066ed",
      "value": 128623
     }
    },
    "77968bf9e2d245959a8baaa74ac969ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "813a0096c0a14544bf8587197047d519": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8247fe49192a47c88da052388b309c9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_634033e7fbe345c58d7dfd276a2f2789",
      "placeholder": "​",
      "style": "IPY_MODEL_2b5fb742bc2f42fcbdaa176eb3194b62",
      "value": " 3/3 [00:56&lt;00:00, 18.07s/it]"
     }
    },
    "8e0f122d9b38442586646c7d54f2b905": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "966f1728dd0440fba83052119019a784": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5c9192050e440f0ba82630612efe4cd",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_072cce547935406fb348e5d49f8d216b",
      "value": 4
     }
    },
    "96e1596583854dd28f6f45d8018127b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "993e44343c394e46996719dc596d9468": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a91c1a4c25049049eca8b933b191941": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a24d702ff6494dac8b6ed48a464c7e06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5ac99fc9e7747808608e466b3fb6172": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b262477dba9c4aef9cf44db10544ca4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f21e890af6fc4c449429189c9ce8c44f",
      "placeholder": "​",
      "style": "IPY_MODEL_34b6531480074258ae4491bfa645de64",
      "value": " 4/4 [02:24&lt;00:00, 45.35s/it]"
     }
    },
    "b6bf255c06a9413d8060331dddf1ccf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b951c23a57b24298bbfc84a946219bd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a24d702ff6494dac8b6ed48a464c7e06",
      "placeholder": "​",
      "style": "IPY_MODEL_40bbe11df03b4da3b011249554f5a4af",
      "value": "DOCUMENTS PROCESSÉS: 100%"
     }
    },
    "bbdfb3a58d8f4388bfd1b771bab3bb9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdd47d4986834f368168d34776965aa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77968bf9e2d245959a8baaa74ac969ce",
      "placeholder": "​",
      "style": "IPY_MODEL_c0501456c0034c30984d2f2b2e6b0195",
      "value": "Mise à jour des unigrams: 100%"
     }
    },
    "c0501456c0034c30984d2f2b2e6b0195": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4f4c50162da4955884c79c8bf78db39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c69192f9e8af45bda4e4fdcd7010da96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_300611a2ba2e420b9811e3221f5d3300",
      "placeholder": "​",
      "style": "IPY_MODEL_2ea5ee687ed947c2b92135749ed74b39",
      "value": "PROCESSUS DES TOPICS: 100%"
     }
    },
    "c998fc39c42743e38f86e2b5ebae80d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d10a1681a9e44f499be3a68f9e638f94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d29f1fc02d964c8e8600308bf6d09dec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0dc48acef57f428f972e75ebfa50338e",
       "IPY_MODEL_6a52320dd96d4e0c89f9b16d75772ff0",
       "IPY_MODEL_e10268dc4c3e4c4e978c057af32147c2"
      ],
      "layout": "IPY_MODEL_5245b6f1cf074219aba13aadb968246f"
     }
    },
    "d95819ac1f024d1f825f373852b4f848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2cb881e86a84b9d97fdd96ebd8509c8",
       "IPY_MODEL_72bf281e72104988b8af544dcef9cbe5",
       "IPY_MODEL_592368db4fcb41849664dc13fe64be77"
      ],
      "layout": "IPY_MODEL_556e919d627c4e13a3e4192a85cdf6e4"
     }
    },
    "e10268dc4c3e4c4e978c057af32147c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c998fc39c42743e38f86e2b5ebae80d9",
      "placeholder": "​",
      "style": "IPY_MODEL_ff9c6da282f442359847c81b69cb8243",
      "value": " 128623/128623 [19:50&lt;00:00, 131.43it/s]"
     }
    },
    "e5c9192050e440f0ba82630612efe4cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f21e890af6fc4c449429189c9ce8c44f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2cb881e86a84b9d97fdd96ebd8509c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d10a1681a9e44f499be3a68f9e638f94",
      "placeholder": "​",
      "style": "IPY_MODEL_9a91c1a4c25049049eca8b933b191941",
      "value": "Filtrage des stopwords: 100%"
     }
    },
    "ff08f96f6c68498699fa5aa86e5c3f50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96e1596583854dd28f6f45d8018127b0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e0f122d9b38442586646c7d54f2b905",
      "value": 1
     }
    },
    "ff9c6da282f442359847c81b69cb8243": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQvwxAFguEeo"
   },
   "outputs": [],
   "source": [
    "def plot_clustered_heatmap(\n",
    "    pivot_df,\n",
    "    num_topic=20,\n",
    "    scale_axis=1,\n",
    "    safe_pad_px=5  # marge de sécurité supplémentaire (en pixels) de chaque côté\n",
    "):\n",
    "    # -----------------------------\n",
    "    # 1) Normalisation min–max\n",
    "    # -----------------------------\n",
    "    df_scaled = pivot_df.apply(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()),\n",
    "        axis=scale_axis\n",
    "    )\n",
    "\n",
    "    df_scaled.columns = (\n",
    "        df_scaled.columns\n",
    "        .str.split(':')\n",
    "        .str[0]\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    if scale_axis == 1:\n",
    "        df_scaled = df_scaled.T\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Ordre des lignes et colonnes\n",
    "    # -----------------------------\n",
    "    row_order = (\n",
    "        df_scaled\n",
    "        .sum(axis=1)\n",
    "        .sort_values()\n",
    "        .index\n",
    "    )\n",
    "    col_order = (\n",
    "        df_scaled\n",
    "        .sum(axis=0)\n",
    "        .sort_values()\n",
    "        .index\n",
    "    )\n",
    "    df_scaled = df_scaled.loc[row_order, col_order]\n",
    "\n",
    "    # ======================================================\n",
    "    # PASSE 1 : figure temporaire pour mesurer\n",
    "    # ======================================================\n",
    "    temp_height_inch = 6\n",
    "    temp_fig, temp_ax = plt.subplots(\n",
    "        figsize=(FIGURE_WIDTH_INCH, temp_height_inch),\n",
    "        dpi=DPI\n",
    "    )\n",
    "\n",
    "    sns.heatmap(\n",
    "        df_scaled,\n",
    "        ax=temp_ax,\n",
    "        cbar=False,\n",
    "        cmap=\"YlGnBu\",\n",
    "        square=False,\n",
    "        linewidths=1,\n",
    "        linecolor='white'\n",
    "    )\n",
    "    temp_ax.tick_params(\n",
    "        axis='x',\n",
    "        pad=2,\n",
    "        length=0,\n",
    "        labeltop=True,\n",
    "        labelbottom=True,\n",
    "        top=True,\n",
    "        bottom=True,\n",
    "        labelrotation=90\n",
    "    )\n",
    "    temp_ax.tick_params(axis='y', pad=2, length=0)\n",
    "\n",
    "    temp_fig.canvas.draw()\n",
    "    renderer = temp_fig.canvas.get_renderer()\n",
    "\n",
    "    # Mesure bounding box Xlabels\n",
    "    xlabels = temp_ax.get_xticklabels()\n",
    "    xlabels_bboxes = [lbl.get_window_extent(renderer=renderer) for lbl in xlabels]\n",
    "    max_label_height_px = max(bbox.height for bbox in xlabels_bboxes) if xlabels_bboxes else 0\n",
    "\n",
    "    # Mesure bounding box Ylabels\n",
    "    ylabels = temp_ax.get_yticklabels()\n",
    "    ylabels_bboxes = [lbl.get_window_extent(renderer=renderer) for lbl in ylabels]\n",
    "    max_label_width_px = max(bbox.width for bbox in ylabels_bboxes) if ylabels_bboxes else 0\n",
    "\n",
    "    plt.close(temp_fig)\n",
    "\n",
    "    # ======================================================\n",
    "    # CALCUL DES DIMENSIONS FINALES\n",
    "    # ======================================================\n",
    "    nb_lignes = len(df_scaled)\n",
    "    nb_colonnes = len(df_scaled.columns)\n",
    "\n",
    "    # Taille en pixels de la heatmap (strictement)\n",
    "    heatmap_cell_size_px = PX_PER_TOPIC\n",
    "    heatmap_height_px = nb_lignes * heatmap_cell_size_px\n",
    "    heatmap_width_px = nb_colonnes * heatmap_cell_size_px\n",
    "\n",
    "    # Marges\n",
    "    margin_top_px = max_label_height_px + safe_pad_px\n",
    "    margin_bottom_px = max_label_height_px + safe_pad_px\n",
    "    margin_left_px = max_label_width_px + safe_pad_px\n",
    "    margin_right_px = safe_pad_px\n",
    "\n",
    "    total_height_px = heatmap_height_px + margin_top_px + margin_bottom_px\n",
    "    total_width_px = heatmap_width_px + margin_left_px + margin_right_px\n",
    "\n",
    "    figure_height_inch = total_height_px / DPI\n",
    "    figure_width_inch = total_width_px / DPI\n",
    "\n",
    "    left_margin_fraction = margin_left_px / total_width_px\n",
    "    right_margin_fraction = 1.0 - (margin_right_px / total_width_px)\n",
    "    bottom_margin_fraction = margin_bottom_px / total_height_px\n",
    "    top_margin_fraction = 1.0 - (margin_top_px / total_height_px)\n",
    "\n",
    "    # ======================================================\n",
    "    # PASSE 2 : figure finale\n",
    "    # ======================================================\n",
    "    fig = plt.figure(\n",
    "        figsize=(figure_width_inch, figure_height_inch),\n",
    "        dpi=DPI\n",
    "    )\n",
    "    ax = sns.heatmap(\n",
    "        df_scaled,\n",
    "        cbar=False,\n",
    "        square=False,\n",
    "        cmap=\"YlGnBu\",\n",
    "        linewidths=1,\n",
    "        linecolor='white'\n",
    "    )\n",
    "\n",
    "    # Force les cellules carrées\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ax.tick_params(\n",
    "        axis='x',\n",
    "        pad=2,\n",
    "        length=0,\n",
    "        labeltop=True,\n",
    "        labelbottom=True,\n",
    "        top=True,\n",
    "        bottom=True,\n",
    "        labelrotation=90\n",
    "    )\n",
    "    ax.tick_params(axis='y', pad=2, length=0)\n",
    "\n",
    "    # Ajustement des marges\n",
    "    fig.subplots_adjust(\n",
    "        left=left_margin_fraction,\n",
    "        right=right_margin_fraction,\n",
    "        bottom=bottom_margin_fraction,\n",
    "        top=top_margin_fraction\n",
    "    )\n",
    "\n",
    "    # Nettoyage\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    plt.title(\"\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Sauvegarde\n",
    "    # ======================================================\n",
    "    if scale_axis == 1:\n",
    "        output_filename = (\n",
    "            f\"{results_path}{base_name}_RANDOM_FORESTS_RESIDUALS_ANALYSIS/\"\n",
    "            f\"{base_name}_random_forests_residual_analysis_topic_normalized_heatmap_{num_topic}tc_\"\n",
    "            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_\"\n",
    "            f\"_correlationaveragech.png\"\n",
    "        )\n",
    "    else:\n",
    "        output_filename = (\n",
    "            f\"{results_path}{base_name}_RANDOM_FORESTS_RESIDUALS_ANALYSIS/\"\n",
    "            f\"{base_name}_random_forests_residual_analysis_group_normalized_heatmap_{num_topic}tc_\"\n",
    "            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_\"\n",
    "            f\"_correlationaveragech.png\"\n",
    "        )\n",
    "\n",
    "    plt.savefig(\n",
    "        output_filename,\n",
    "        dpi=DPI,\n",
    "        pad_inches=0,\n",
    "        bbox_inches=None\n",
    "    )\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZArllw9IRLO"
   },
   "outputs": [],
   "source": [
    "def random_forests_residuals_analysis(group_column=None):\n",
    "    if group_column == None:\n",
    "        print('group_column est None')\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(f\"{results_path}{base_name}_RANDOM_FORESTS_RESIDUALS_ANALYSIS/\"):\n",
    "        os.makedirs(f\"{results_path}{base_name}_RANDOM_FORESTS_RESIDUALS_ANALYSIS/\")\n",
    "\n",
    "\n",
    "    for num_topic in all_nmf_W:\n",
    "        rows = []\n",
    "        W_matrix = all_nmf_W[num_topic]\n",
    "\n",
    "        # On itère sur les \"lignes\" de la matrice, i.e. chaque article\n",
    "        for doc_idx, topic_scores in enumerate(W_matrix):\n",
    "            # === Récupération du nom du journal selon la source ===\n",
    "            # doc_idx (au lieu de num_article)\n",
    "            if source_type == 'europresse':\n",
    "                header = all_soups[doc_idx].header\n",
    "                journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "                journal_text = normalize_journal(journal_text)\n",
    "\n",
    "            elif source_type == 'istex':\n",
    "                journal_text = columns_dict['journal'][doc_idx]\n",
    "\n",
    "            elif source_type == 'csv':\n",
    "                if group_column not in columns_dict:\n",
    "                    print(f\"La colonne '{group_column}' n'a pas été trouvée dans le fichier CSV.\")\n",
    "                    return\n",
    "                journal_text = columns_dict[group_column][doc_idx]\n",
    "\n",
    "            # Construire un dictionnaire pour cette ligne\n",
    "            row_dict = {\n",
    "                'doc_idx': doc_idx,\n",
    "                'Journal': journal_text\n",
    "            }\n",
    "            # Ajouter les scores de tous les topics\n",
    "            # topic_scores est un array de taille k\n",
    "            for t_idx, score in enumerate(topic_scores):\n",
    "                row_dict[f'Topic{t_idx}'] = score\n",
    "\n",
    "            rows.append(row_dict)\n",
    "\n",
    "\n",
    "        # 1) Compter la fréquence de chaque journal dans rows\n",
    "        journal_counts = Counter(row['Journal'] for row in rows)\n",
    "\n",
    "        # 2) Filtrer : ne conserver que les rows dont le journal apparaît au moins threshold fois\n",
    "        rows = [row for row in rows if journal_counts[row['Journal']] >= threshold]\n",
    "\n",
    "        unique_journals = {row['Journal'] for row in rows}\n",
    "\n",
    "        if len(unique_journals) == 1:\n",
    "            print(\"Il n'y a qu'un seul groupe\")\n",
    "            return\n",
    "\n",
    "        df_wide = pd.DataFrame(rows)\n",
    "\n",
    "        k = W_matrix.shape[1]  # nombre de topics\n",
    "\n",
    "        residuals_df = df_wide.copy()  # On duplique pour y stocker les résidus\n",
    "\n",
    "        for j in tqdm(range(k), desc='RÉGRESSIONS : ANALYSE DES RÉSIDUS'):\n",
    "            # Nom de la colonne cible\n",
    "            col_target = f'Topic{j}'\n",
    "\n",
    "            # Features = tous les topics sauf le j-ème\n",
    "            feature_cols = [f'Topic{x}' for x in range(k) if x != j]\n",
    "\n",
    "            X = df_wide[feature_cols]\n",
    "            y = df_wide[col_target]\n",
    "\n",
    "            # Entraîner un modèle de régression\n",
    "            rf = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "            rf.fit(X, y)\n",
    "\n",
    "            # Prédire\n",
    "            y_pred = rf.predict(X)\n",
    "\n",
    "            # Calcul du résidu \"brut\"\n",
    "            resid = y - y_pred\n",
    "\n",
    "            # Stocker le résidu brut (optionnel)\n",
    "            residuals_df[f'Resid_Topic{j}'] = resid\n",
    "\n",
    "            # Standardisation (z-scoring) du résidu\n",
    "            mu = resid.mean()\n",
    "            sigma = resid.std()  # ou np.std(resid, ddof=1) pour l'échantillon\n",
    "            if sigma == 0:\n",
    "                # Éventuellement, gérer le cas où le résidu est toujours identique (très rare)\n",
    "                resid_z = resid  # ou resid_z = 0\n",
    "            else:\n",
    "                resid_z = (resid - mu) / sigma\n",
    "\n",
    "            # Stocker le résidu normalisé\n",
    "            residuals_df[f'ResidZ_Topic{j}'] = resid_z\n",
    "\n",
    "        # 1) Préparer un dictionnaire de renommage\n",
    "        rename_map = {}\n",
    "        for j in range(k):\n",
    "            old_col = f\"ResidZ_Topic{j}\"\n",
    "            # Récupérer le vrai nom du topic\n",
    "            # Exemple : \"Politique\", \"Économie\", etc.\n",
    "            real_name = topic_labels_by_config[num_topic][j]\n",
    "            new_col = f\"ResidZ_{real_name}\"\n",
    "            rename_map[old_col] = new_col\n",
    "\n",
    "        # 2) Renommer les colonnes dans un nouveau DataFrame\n",
    "        residuals_df_renamed = residuals_df.rename(columns=rename_map)\n",
    "\n",
    "        # 3) Faire le melt : on sélectionne les nouvelles colonnes 'Resid_<topic_name>'\n",
    "        value_vars_list = list(rename_map.values())  # ex: ['Resid_Politique', 'Resid_Sport', ...]\n",
    "\n",
    "        table_resid = residuals_df_renamed.melt(\n",
    "            id_vars=['Journal'],         # on garde la colonne 'Journal' telle quelle\n",
    "            value_vars=value_vars_list,  # on fait fondre les colonnes résidu renommées\n",
    "            var_name='Topic',            # le nom de la colonne contenant l'ancien nom de variable\n",
    "            value_name='Resid'           # la valeur numérique du résidu\n",
    "        )\n",
    "\n",
    "        # Maintenant, 'Topic' sera de la forme 'Resid_<NomDuTopic>'\n",
    "        # On peut, si on veut, enlever le préfixe 'Resid_' pour un affichage plus clair :\n",
    "        table_resid['Topic'] = table_resid['Topic'].str.replace('ResidZ_', '', regex=False)\n",
    "\n",
    "        # 4) Calculer la moyenne des résidus par (Journal, Topic), puis faire un pivot\n",
    "        pivot = table_resid.groupby(['Journal','Topic'])['Resid'].mean().unstack(fill_value=0)\n",
    "\n",
    "        # ===================================================================\n",
    "        # Exemple d’utilisation pour générer deux heatmaps :\n",
    "        #   - l’une avec normalisation min-max par ligne\n",
    "        #   - l’autre avec normalisation min-max par colonne\n",
    "        # ===================================================================\n",
    "\n",
    "        for num_topic in all_nmf_W:\n",
    "            # Heatmap avec normalisation par colonne\n",
    "            plot_clustered_heatmap(\n",
    "                pivot_df=pivot,\n",
    "                scale_axis=0,\n",
    "                num_topic=num_topic\n",
    "            )\n",
    "\n",
    "            # Heatmap avec normalisation par ligne\n",
    "            plot_clustered_heatmap(\n",
    "                pivot_df=pivot,\n",
    "                scale_axis=1,\n",
    "                num_topic=num_topic\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJdSGESIXdYQ"
   },
   "outputs": [],
   "source": [
    "def go_tfidf_vectorization_sentences(\n",
    "    count_vectorizer,       # CountVectorizer déjà \"fit\"\n",
    "    tfidf_transformer,      # TfidfTransformer (ou pipeline) déjà \"fit\"\n",
    "    all_sentence_pos        # liste de données (phrases + infos POS, etc.) à tokeniser\n",
    "):\n",
    "    # Sans `tokenize_and_stem`\n",
    "    tokenized_documents = []\n",
    "    for atb in all_sentence_pos:\n",
    "        tokenized_document = [t[0] for t in atb if t[0] in unigrams]\n",
    "        tokenized_documents.append(tokenized_document)\n",
    "\n",
    "    # 4) Filtrer les stop words via spaCy\n",
    "    spacy_stopwords = nlp_pipeline.Defaults.stop_words\n",
    "    # On itère avec tqdm sur tokenized_documents\n",
    "    filtered_docs = []\n",
    "    for doc in tokenized_documents:\n",
    "        filtered_doc = [token for token in doc if token.lower() not in spacy_stopwords]\n",
    "        filtered_docs.append(filtered_doc)\n",
    "    tokenized_documents = filtered_docs\n",
    "\n",
    "    # Exemple : transformation batch pour profiter de tqdm (optionnel)\n",
    "    word_count = count_vectorizer.transform(tokenized_documents)\n",
    "\n",
    "    # Transformation TF-IDF\n",
    "    X_sentences = tfidf_transformer.transform(word_count)\n",
    "\n",
    "    # 7) Retourner la matrice TF-IDF et la version tokenisée\n",
    "    return X_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6llBCJlXdVp"
   },
   "outputs": [],
   "source": [
    "def write_sentences_results(topic_num, final_top_ngrams_per_topic):\n",
    "    with open(\n",
    "        f\"{results_path}{base_name}_EXPLORE_TOPICS/\"\n",
    "        f\"{base_name}_topic_modeling_sentences_{topic_num}tc_\"\n",
    "        f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "        f\"{go_remove_duplicates}dup.csv\",\n",
    "        \"w\",\n",
    "        encoding='utf-8'\n",
    "    ) as file_object:\n",
    "        writer = csv.writer(file_object)\n",
    "\n",
    "        # Écrire les en-têtes si nécessaire\n",
    "        headers = []\n",
    "        for i in range(len(final_top_ngrams_per_topic)):\n",
    "            headers.extend([f'{i}_sentences', f'{i}_scores'])\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        # Écrire les données\n",
    "        for i in range(20):\n",
    "            row = []\n",
    "            for sub_array in final_top_ngrams_per_topic:\n",
    "                if i < len(sub_array):\n",
    "                    row.extend(sub_array[i])\n",
    "                else:\n",
    "                    row.extend(('', ''))\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplace par ta clé réelle\n",
    "client = openai.OpenAI(api_key='')\n",
    "\n",
    "def call_gpt4o_mini(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  \n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=30\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except OpenAIError as e:\n",
    "        print(f\"Erreur OpenAI : {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qoa0SYGuXdNN"
   },
   "outputs": [],
   "source": [
    "# On choisit l'encoding \"o200k_base\" pour le comptage des tokens\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "# File pour stocker (timestamp, nb_tokens) et calculer la conso sur 60s\n",
    "tokens_history = deque()\n",
    "\n",
    "def count_tokens(prompt: str) -> int:\n",
    "    \"\"\"Compte le nombre de tokens dans la chaîne 'prompt'.\"\"\"\n",
    "    return len(tokenizer.encode(prompt))\n",
    "\n",
    "def check_and_wait_if_needed(current_tokens: int, max_tokens_per_minute: int = 180_000):\n",
    "    \"\"\"\n",
    "    Vérifie la consommation de tokens sur la dernière minute.\n",
    "    Si l’ajout de current_tokens dépasse max_tokens_per_minute,\n",
    "    on attend le temps nécessaire pour repasser sous la limite.\n",
    "    \"\"\"\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # On enlève de l'historique tout ce qui date de plus de 60s\n",
    "    while tokens_history and (current_time - tokens_history[0][0] > 60):\n",
    "        tokens_history.popleft()\n",
    "    \n",
    "    # On calcule le total de tokens sur la dernière minute\n",
    "    tokens_last_minute = sum(tokens for _, tokens in tokens_history)\n",
    "    \n",
    "    # Si l'ajout de 'current_tokens' dépasse la limite\n",
    "    if tokens_last_minute + current_tokens > max_tokens_per_minute:\n",
    "        first_timestamp = tokens_history[0][0]\n",
    "        time_to_wait = 60 - (current_time - first_timestamp)\n",
    "        if time_to_wait > 0:\n",
    "            time.sleep(time_to_wait)\n",
    "    \n",
    "    # On ajoute le (timestamp, current_tokens) une fois qu'on a éventuellement attendu\n",
    "    tokens_history.append((time.time(), current_tokens))\n",
    "\n",
    "\n",
    "def extract_relevant_sentences_and_titles(nmf_models):\n",
    "    \"\"\"\n",
    "    This function extracts the top N diverse sentences from each topic,\n",
    "    then calls gpt4o-mini to generate a single, crisp title for each topic\n",
    "    in the format \"X: Y\" (where X is a singular main title, and Y is an\n",
    "    explanatory phrase).\n",
    "\n",
    "    NOUVEAU COMPORTEMENT :\n",
    "      - À chaque nouveau titre demandé, on injecte dans le prompt tous les\n",
    "        titres déjà créés pour éviter toute duplication.\n",
    "      - On interdit les titres identiques (\"X: Y\" complet) ou même une partie X déjà utilisée.\n",
    "\n",
    "    NOTE: On suppose l'existence de `call_gpt4o_mini(prompt: str) -> str`\n",
    "    pour faire l'appel au modèle. À adapter selon votre code.\n",
    "    \"\"\"\n",
    "    titles_per_num_topic = {}\n",
    "    # Stocke tous les titres produits sous forme \"X: Y\"\n",
    "    all_previous_titles = []\n",
    "\n",
    "    for num_topic in nmf_models:\n",
    "        X_sentences = go_tfidf_vectorization_sentences(\n",
    "            tfidf_vectorizer,\n",
    "            tfidf_transformer,\n",
    "            all_sentence_pos\n",
    "        )\n",
    "\n",
    "        score_phrases = nmf_models[num_topic].transform(X_sentences)\n",
    "\n",
    "        final_top_ngrams_per_topic = []\n",
    "        top_n = 20\n",
    "        candidate_size = 100\n",
    "        similarity_threshold = 0.8\n",
    "        n_topics = nmf_models[num_topic].n_components\n",
    "\n",
    "        # On identifie les top_n phrases pour chaque topic\n",
    "        for topic_idx in range(n_topics):\n",
    "            topic_scores = score_phrases[:, topic_idx]\n",
    "            top_indices_candidate = np.argsort(topic_scores)[::-1][:candidate_size]\n",
    "\n",
    "            candidate_vectors = X_sentences[top_indices_candidate]\n",
    "            candidate_phrases = [sentences_norms[i] for i in top_indices_candidate]\n",
    "\n",
    "            selected_indices = []\n",
    "            for i, vec_i in enumerate(candidate_vectors):\n",
    "                if not selected_indices:\n",
    "                    selected_indices.append(i)\n",
    "                    continue\n",
    "\n",
    "                is_similar_to_selected = False\n",
    "                for j in selected_indices:\n",
    "                    sim_ij = cosine_similarity(vec_i, candidate_vectors[j])\n",
    "                    if sim_ij[0, 0] >= similarity_threshold:\n",
    "                        is_similar_to_selected = True\n",
    "                        break\n",
    "\n",
    "                if not is_similar_to_selected:\n",
    "                    selected_indices.append(i)\n",
    "\n",
    "                if len(selected_indices) >= top_n:\n",
    "                    break\n",
    "\n",
    "            sub_array = []\n",
    "            for idx_in_candidates in selected_indices[:top_n]:\n",
    "                phrase_brute = candidate_phrases[idx_in_candidates]\n",
    "                score_value = topic_scores[top_indices_candidate[idx_in_candidates]]\n",
    "                sub_array.append((phrase_brute, round(score_value, 4)))\n",
    "\n",
    "            final_top_ngrams_per_topic.append(sub_array)\n",
    "\n",
    "        # Écriture ou log des phrases si nécessaire\n",
    "        write_sentences_results(num_topic, final_top_ngrams_per_topic)\n",
    "\n",
    "        # Génération des titres par topic\n",
    "        topic_titles = []\n",
    "        for topic_idx, sub_array in enumerate(final_top_ngrams_per_topic):\n",
    "            hierarchical_sentences = \"\\n\".join(\n",
    "                f\"- {item[0]} (score: {item[1]})\" for item in sub_array\n",
    "            )\n",
    "\n",
    "            # On formate la liste de tous les titres déjà connus\n",
    "            previous_titles_text = \"\\n\".join(f\"- {t}\" for t in all_previous_titles) or \"(none so far)\"\n",
    "\n",
    "            if language == 'fr':\n",
    "                prompt = f\"\"\"{preprompt}\n",
    "                Nous avons déjà créé ces titres (X: Y).\n",
    "                NE répète PAS un titre entier déjà existant.\n",
    "                NE réutilise PAS un « X » déjà présent dans les titres existants.\n",
    "\n",
    "                Titres déjà utilisés :\n",
    "                {previous_titles_text}\n",
    "\n",
    "                Voici maintenant des phrases regroupées thématiquement :\n",
    "\n",
    "                {hierarchical_sentences}\n",
    "\n",
    "                Ta tâche :\n",
    "                    • Génère UN SEUL titre court, unique et complet, au format « X: Y ».\n",
    "                    • Le « X » doit précisément saisir la spécificité ou le sujet distinctif présent dans ces propos de droite américaine. Évite autant que possible les termes abstraits, généraux ou vagues.\n",
    "                    • Le « Y » doit brièvement apporter un contexte ou une nuance clarifiant le sens de « X ».\n",
    "                    • Le titre complet ne doit pas dépasser 18 mots au total.\n",
    "                    • Reflète fidèlement les aspects idéologiques ou rhétoriques propres à ces phrases.\n",
    "                    • Fournis UNIQUEMENT le titre final, sans texte supplémentaire ni commentaire.\n",
    "                    • La première lettre du premier mot du titre est en majuscule.\n",
    "                    • Tous les autres mots sont en minuscules, y compris la première lettre de la partie « Y » si elle n'est pas au début du titre.\n",
    "                    • Exception : les noms propres (pays, villes, prénoms, etc.) conservent leur majuscule, quelle que soit leur position dans le titre.\n",
    "                    • Le titre ne se termine pas par un point.\n",
    "                    • UNE FOIS ENCORE : NE réutilise PAS un « X » déjà présent dans les titres existants !!\n",
    "                \"\"\"\n",
    "            else:\n",
    "                prompt = f\"\"\"{preprompt}\n",
    "                We have already created these titles (X: Y).\n",
    "                Do NOT repeat any entire title.\n",
    "                Do NOT reuse any ‘X’ that appears in the existing titles.\n",
    "\n",
    "                Already used titles:\n",
    "                {previous_titles_text}\n",
    "\n",
    "                Now, you are given the following grouped, thematically relevant sentences:\n",
    "\n",
    "                {hierarchical_sentences}\n",
    "\n",
    "                Your task:\n",
    "                    •\tGenerate ONE short, single, and complete title in the format “X: Y”.\n",
    "                    •\t“X” must precisely capture the specificity or distinctive topic found in these American right-wing statements. Avoid abstract, general, or vague terms unless absolutely necessary.\n",
    "                    •\t“Y” should briefly add context or nuance to clarify the meaning of “X”.\n",
    "                    •\tThe complete title must not exceed 18 words in total.\n",
    "                    •\tAccurately reflect the unique ideological or rhetorical aspects present in these sentences.\n",
    "                    •\tProvide ONLY the final title, without extra text or commentary.\n",
    "                    •\tONCE AGAIN: Do NOT reuse any ‘X’ that appears in the existing titles!!\n",
    "                \"\"\"\n",
    "\n",
    "            # 1) Compter les tokens\n",
    "            token_count = count_tokens(prompt)\n",
    "            # 2) Vérifier la limite de 180k tokens/minute\n",
    "            check_and_wait_if_needed(token_count)\n",
    "\n",
    "            # 3) Appel effectif au modèle\n",
    "            new_title = call_gpt4o_mini(prompt).strip()\n",
    "            topic_titles.append(new_title)\n",
    "\n",
    "            # On enregistre ce nouveau titre dans la liste globale\n",
    "            all_previous_titles.append(new_title)\n",
    "\n",
    "        titles_per_num_topic[num_topic] = topic_titles\n",
    "\n",
    "    return titles_per_num_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kdQJK-l_Fto"
   },
   "outputs": [],
   "source": [
    "def detecter_date(chaine, jour_en_premier=True):\n",
    "    try:\n",
    "        return parse(chaine, dayfirst=jour_en_premier)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21wCcYU5_HUj"
   },
   "outputs": [],
   "source": [
    "def formater_date(date):\n",
    "    return date.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MZQ4gTe_IhB"
   },
   "outputs": [],
   "source": [
    "def formater_liste_dates(liste_dates, jour_en_premier=True):\n",
    "    return [formater_date(detecter_date(date_str, jour_en_premier)) for date_str in liste_dates if detecter_date(date_str, jour_en_premier)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsZyVjo9WtG7"
   },
   "outputs": [],
   "source": [
    "def truncate_texts(texts, max_length=30):\n",
    "    # 1. Vérifier si tous les textes contiennent \":\"\n",
    "    if all(':' in text for text in texts):\n",
    "        # Si oui, on ne garde que la partie avant le premier \":\" (trim)\n",
    "        return [text.split(':', 1)[0].strip() for text in texts]\n",
    "\n",
    "    # 2. Sinon, on applique la logique de troncature initiale\n",
    "    truncated_texts = []\n",
    "    for text in texts:\n",
    "        if len(text) <= max_length:\n",
    "            truncated_texts.append(text)\n",
    "            continue\n",
    "\n",
    "        # Trouve le dernier espace avant max_length\n",
    "        last_space_index = text.rfind(' ', 0, max_length)\n",
    "        if last_space_index == -1:\n",
    "            # S'il n'y a pas d'espace, on coupe jusqu'à max_length et on ajoute \"...\"\n",
    "            truncated_texts.append(text[:max_length] + \"...\")\n",
    "        else:\n",
    "            # Sinon, on coupe jusqu'au dernier espace et on ajoute \"...\"\n",
    "            truncated_texts.append(text[:last_space_index] + \"...\")\n",
    "\n",
    "    return truncated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_label_placement_force_directed(\n",
    "    ax,\n",
    "    positions_and_labels,\n",
    "    x_min, x_max, y_min, y_max,\n",
    "    # Les paramètres offsets_*, possible_* ne sont pas utilisés ici\n",
    "    # Paramètres spécifiques à l'algorithme de forces :\n",
    "    n_iterations=200,\n",
    "    k_attraction=0.01,\n",
    "    k_repulsion=0.05,\n",
    "    k_boundary=0.1,\n",
    "    damping=0.85,\n",
    "    timestep=1.0,\n",
    "    convergence_threshold=1e-4,\n",
    "    fixed_ha='center',\n",
    "    fixed_va='center',\n",
    "    repulsion_margin=1.0 # Petite marge pour la répulsion pour compenser l'approx.\n",
    "):\n",
    "    \"\"\"\n",
    "    -------------------------------------------------------------------------\n",
    "    SOLVEUR PAR FORCES DIRIGÉES :\n",
    "    Place les étiquettes en simulant un système physique où elles sont\n",
    "    attirées vers leur point d'origine et se repoussent entre elles\n",
    "    et par rapport aux limites du cadre.\n",
    "\n",
    "    positions_and_labels : liste de ((x_i, y_i), label_text).\n",
    "    x_min, x_max, y_min, y_max : cadre à ne pas dépasser.\n",
    "    ax : Axes Matplotlib (utilisé pour calculer la taille initiale).\n",
    "\n",
    "    Retourne : [(i, X, Y, ha, va, bbox, distance_reelle), ...]\n",
    "       Format identique à la version MILP, mais avec ha/va fixés.\n",
    "    -------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    num_labels = len(positions_and_labels)\n",
    "    if num_labels == 0:\n",
    "        return []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- 1. Initialisation ---\n",
    "    anchors = []\n",
    "    texts = []\n",
    "    current_pos = []\n",
    "    label_dims = [] # Stocker (width, height)\n",
    "\n",
    "    print(\"[Force Directed] Initialisation et calcul des tailles...\")\n",
    "    for i, ((x_i, y_i), label_text) in enumerate(positions_and_labels):\n",
    "        anchors.append((x_i, y_i))\n",
    "        texts.append(label_text)\n",
    "        current_pos.append([x_i, y_i]) # Position initiale = ancre\n",
    "\n",
    "        # Calculer la taille une seule fois (approximation)\n",
    "        # On utilise la fonction existante mais on ne garde que la taille\n",
    "        try:\n",
    "            # Mettre des coordonnées temporaires non nulles pour éviter\n",
    "            # des problèmes si l'ancre est à (0,0) selon l'échelle.\n",
    "            temp_x, temp_y = 1, 1\n",
    "            bbx_min, bbx_max, bby_min, bby_max = bounding_box_with_patch(\n",
    "                ax, label_text, temp_x, temp_y, ha=fixed_ha, va=fixed_va\n",
    "                # Le style de bbox n'est pas critique ici, juste pour la taille\n",
    "            )\n",
    "            width = bbx_max - bbx_min\n",
    "            height = bby_max - bby_min\n",
    "            # Gérer les cas où la taille serait nulle ou négative (peu probable)\n",
    "            if width <= 0 or height <= 0:\n",
    "                 print(f\"Warning: Label {i} text '{label_text}' has non-positive dimensions ({width}, {height}). Using fallback size.\")\n",
    "                 # Fallback: essayer de mesurer directement le texte ? Ou mettre une taille par défaut.\n",
    "                 # Pour simplifier, mettons une petite taille par défaut.\n",
    "                 width = abs(width) if width != 0 else 0.1 * (x_max - x_min)\n",
    "                 height = abs(height) if height != 0 else 0.1 * (y_max - y_min)\n",
    "\n",
    "            label_dims.append((width, height))\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du calcul de la taille pour le label {i} ('{label_text}'): {e}\")\n",
    "            print(\"Utilisation d'une taille par défaut.\")\n",
    "            # Fournir une taille par défaut raisonnable si le calcul échoue\n",
    "            default_width = 0.1 * (x_max - x_min)\n",
    "            default_height = 0.05 * (y_max - y_min)\n",
    "            label_dims.append((default_width, default_height))\n",
    "\n",
    "    print(f\"[Force Directed] Tailles calculées en {time.time() - start_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- 2. Simulation ---\n",
    "    print(f\"[Force Directed] Début de la simulation ({n_iterations} itérations max)...\")\n",
    "    velocities = [[0.0, 0.0] for _ in range(num_labels)] # Pour un amortissement plus réaliste\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        total_movement = 0.0\n",
    "        forces = [[0.0, 0.0] for _ in range(num_labels)]\n",
    "\n",
    "        # --- Calcul des forces ---\n",
    "        for i in range(num_labels):\n",
    "            xi, yi = current_pos[i]\n",
    "            anchor_x, anchor_y = anchors[i]\n",
    "            width_i, height_i = label_dims[i]\n",
    "\n",
    "            # a) Force d'attraction vers l'ancre\n",
    "            dx_attr = anchor_x - xi\n",
    "            dy_attr = anchor_y - yi\n",
    "            forces[i][0] += k_attraction * dx_attr\n",
    "            forces[i][1] += k_attraction * dy_attr\n",
    "\n",
    "            # b) Force de répulsion par les autres étiquettes\n",
    "            for j in range(num_labels):\n",
    "                if i == j: continue\n",
    "\n",
    "                xj, yj = current_pos[j]\n",
    "                width_j, height_j = label_dims[j]\n",
    "\n",
    "                # Calculer les BBox approximatives basées sur pos et dims\n",
    "                # (en supposant ha='center', va='center' pour la simulation)\n",
    "                bbox_i = (xi - width_i/2, xi + width_i/2, yi - height_i/2, yi + height_i/2)\n",
    "                bbox_j = (xj - width_j/2, xj + width_j/2, yj - height_j/2, yj + height_j/2)\n",
    "\n",
    "                if overlap(bbox_i, bbox_j): #, margin=repulsion_margin):\n",
    "                    # Calcul simple de la direction de répulsion (centre à centre)\n",
    "                    dx_rep = xi - xj\n",
    "                    dy_rep = yi - yj\n",
    "                    dist_sq = dx_rep**2 + dy_rep**2\n",
    "\n",
    "                    if dist_sq < 1e-9: # Eviter division par zéro si superposés exactement\n",
    "                        dx_rep = 0.1 * width_i # Petite poussée aléatoire ou fixe\n",
    "                        dy_rep = 0.0\n",
    "                        dist_sq = dx_rep**2\n",
    "\n",
    "                    dist = math.sqrt(dist_sq)\n",
    "\n",
    "                    # Force proportionnelle à l'inverse de la distance (ou 1/dist^2 ?)\n",
    "                    # et plus forte si plus proche. Utilisons 1/dist.\n",
    "                    # On peut aussi la moduler par l'aire de chevauchement, mais c'est plus complexe.\n",
    "                    repulsion_strength = k_repulsion / dist\n",
    "\n",
    "                    forces[i][0] += repulsion_strength * (dx_rep / dist)\n",
    "                    forces[i][1] += repulsion_strength * (dy_rep / dist)\n",
    "                    # forces[j][0] -= repulsion_strength * (dx_rep / dist) # Action = Réaction (calculé quand j est traité)\n",
    "\n",
    "            # c) Force de répulsion par les limites\n",
    "            if xi - width_i / 2 < x_min: forces[i][0] += k_boundary * (x_min - (xi - width_i / 2))\n",
    "            if xi + width_i / 2 > x_max: forces[i][0] += k_boundary * (x_max - (xi + width_i / 2))\n",
    "            if yi - height_i / 2 < y_min: forces[i][1] += k_boundary * (y_min - (yi - height_i / 2))\n",
    "            if yi + height_i / 2 > y_max: forces[i][1] += k_boundary * (y_max - (yi + height_i / 2))\n",
    "\n",
    "\n",
    "        # --- Mise à jour des positions ---\n",
    "        new_pos = []\n",
    "        for i in range(num_labels):\n",
    "            # Mise à jour de la vélocité (avec amortissement)\n",
    "            velocities[i][0] = (velocities[i][0] + forces[i][0] * timestep) * damping\n",
    "            velocities[i][1] = (velocities[i][1] + forces[i][1] * timestep) * damping\n",
    "\n",
    "            # Mise à jour de la position\n",
    "            next_x = current_pos[i][0] + velocities[i][0]\n",
    "            next_y = current_pos[i][1] + velocities[i][1]\n",
    "            new_pos.append([next_x, next_y])\n",
    "\n",
    "            # Calculer le mouvement pour la convergence\n",
    "            movement = math.sqrt(velocities[i][0]**2 + velocities[i][1]**2)\n",
    "            total_movement += movement\n",
    "\n",
    "        current_pos = new_pos # Mettre à jour toutes les positions en même temps\n",
    "\n",
    "        # Critère de convergence\n",
    "        if total_movement < convergence_threshold * num_labels:\n",
    "            print(f\"[Force Directed] Convergence atteinte à l'itération {iteration+1}\")\n",
    "            break\n",
    "\n",
    "    if iteration == n_iterations - 1:\n",
    "         print(f\"[Force Directed] Nombre maximum d'itérations ({n_iterations}) atteint.\")\n",
    "\n",
    "    print(f\"[Force Directed] Simulation terminée en {time.time() - start_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- 3. Finalisation et Formatage de la sortie ---\n",
    "    solution = []\n",
    "    print(\"[Force Directed] Calcul des BBox finales précises...\")\n",
    "    final_bbox_calculation_start = time.time()\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        final_x, final_y = current_pos[i]\n",
    "        anchor_x, anchor_y = anchors[i]\n",
    "        label_text = texts[i]\n",
    "\n",
    "        # Calcul de la BBox finale PRÉCISE avec la fonction d'origine\n",
    "        try:\n",
    "             bbx_min, bbx_max, bby_min, bby_max = bounding_box_with_patch(\n",
    "                 ax, label_text, final_x, final_y, ha=fixed_ha, va=fixed_va)\n",
    "             bbox_tup = (bbx_min, bbx_max, bby_min, bby_max)\n",
    "\n",
    "             # Vérification finale (optionnelle) : la bbox finale est-elle bien dans les clous ?\n",
    "             # Elle pourrait dépasser légèrement à cause de l'approximation pendant la simu.\n",
    "             # On pourrait la \"clipper\" ou la ramener si nécessaire.\n",
    "             # Pour l'instant, on la retourne telle quelle.\n",
    "             if not (bbx_min >= x_min - 1e-6 and bbx_max <= x_max + 1e-6 and \\\n",
    "                     bby_min >= y_min - 1e-6 and bby_max <= y_max + 1e-6):\n",
    "                 print(f\"Warning: Label {i} ('{label_text}') BBox finale {bbox_tup} dépasse légèrement les limites.\")\n",
    "                 # Option: \"clipper\" les coordonnées de la bbox ou la position finale?\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Erreur lors du calcul de la BBox finale pour le label {i} ('{label_text}'): {e}\")\n",
    "             # Utiliser une bbox basée sur l'approximation si le calcul final échoue\n",
    "             width_i, height_i = label_dims[i]\n",
    "             bbox_tup = (final_x - width_i/2, final_x + width_i/2, final_y - height_i/2, final_y + height_i/2)\n",
    "\n",
    "\n",
    "        # Calcul de la distance réelle finale\n",
    "        dist_reelle = math.dist((anchor_x, anchor_y), (final_x, final_y))\n",
    "\n",
    "        solution.append((i, final_x, final_y, fixed_ha, fixed_va, bbox_tup, dist_reelle))\n",
    "\n",
    "    print(f\"[Force Directed] BBox finales calculées en {time.time() - final_bbox_calculation_start:.2f}s\")\n",
    "    print(f\"[Force Directed] Terminé en {time.time() - start_time:.2f}s au total.\")\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bbox_style=dict(facecolor='white', edgecolor='black', alpha=0.1, boxstyle='square,pad=0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_label_placement_matplotlib_2passes(\n",
    "    ax,\n",
    "    positions_and_labels,\n",
    "    x_min, x_max, y_min, y_max,\n",
    "    offsets_x, offsets_y,\n",
    "    possible_ha=('left','center','right'),\n",
    "    possible_va=('bottom','center','top'),\n",
    "    time_limit_ms_pass1=None, # Nouvelle option : limite en ms pour passe 1\n",
    "    time_limit_ms_pass2=None  # Nouvelle option : limite en ms pour passe 2\n",
    "):\n",
    "    \"\"\"\n",
    "    -------------------------------------------------------------------------\n",
    "    SOLVEUR EN DEUX PASSES (méthode \"lexicographique\") en MILP :\n",
    "      1) Minimiser la somme des distances (distance * 100000).\n",
    "      2) À distance minimale égale, maximiser le nombre de labels\n",
    "         en (ha='center', va='center').\n",
    "\n",
    "    positions_and_labels : liste de ((x_i, y_i), label_text).\n",
    "    x_min, x_max, y_min, y_max : cadre à ne pas dépasser.\n",
    "    offsets_x, offsets_y : listes des offsets qu'on souhaite tester.\n",
    "    time_limit_ms_pass1 / time_limit_ms_pass2: Limite de temps en ms pour chaque passe.\n",
    "\n",
    "    Retourne : [(i, X, Y, ha, va, bbox, distance_reelle), ...]\n",
    "       - i = indice du label\n",
    "       - (X, Y) = position choisie\n",
    "       - ha, va = alignements\n",
    "       - bbox = (xmin, xmax, ymin, ymax)\n",
    "       - distance_reelle = distance euclidienne (en \"data coords\") entre\n",
    "         (x_i, y_i) et (X, Y)\n",
    "    -------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #  1) GÉNÉRATION DE TOUTES LES POSITIONS CANDIDATES (OPTIMISÉE)\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    candidate_positions = []\n",
    "    label_size_cache = {} # <--- Cache pour stocker (W, H) par label_text\n",
    "\n",
    "    print(\"Génération des positions candidates (avec cache de taille)...\")\n",
    "    num_potential_candidates = 0\n",
    "    num_valid_candidates = 0\n",
    "\n",
    "    for i, ((x_i, y_i), label_text) in enumerate(positions_and_labels):\n",
    "\n",
    "        # --- Calcul ou récupération de la taille de la BBox ---\n",
    "        if label_text not in label_size_cache:\n",
    "            try:\n",
    "                # Calculer une seule fois, p.ex. à l'origine ou position initiale\n",
    "                # Utiliser des alignements simples pour le calcul de taille\n",
    "                ref_bbox = bounding_box_with_patch(\n",
    "                    ax, label_text, x_i, y_i, # Ou 0, 0 si préféré\n",
    "                    ha='left', va='bottom'\n",
    "                )\n",
    "                ref_xmin, ref_xmax, ref_ymin, ref_ymax = ref_bbox\n",
    "                W = ref_xmax - ref_xmin\n",
    "                H = ref_ymax - ref_ymin\n",
    "                label_size_cache[label_text] = (W, H)\n",
    "                # print(f\"Cache calculé pour '{label_text}': W={W:.2f}, H={H:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERREUR lors du calcul de la taille pour le label '{label_text}': {e}\")\n",
    "                # Optionnel: ignorer ce label ou utiliser une taille par défaut?\n",
    "                # Pour l'instant, on ne pourra pas générer de candidats pour lui.\n",
    "                candidate_positions.append([]) # Ajouter une liste vide pour ce label\n",
    "                continue # Passer au label suivant\n",
    "\n",
    "        W, H = label_size_cache[label_text]\n",
    "        if W <= 0 or H <= 0:\n",
    "             print(f\"Warning: Taille invalide (W={W}, H={H}) pour le label '{label_text}'. Impossible de placer.\")\n",
    "             candidate_positions.append([])\n",
    "             continue\n",
    "\n",
    "\n",
    "        cands_for_i = []\n",
    "        for dx in offsets_x:\n",
    "            for dy in offsets_y:\n",
    "                X = x_i + dx\n",
    "                Y = y_i + dy\n",
    "\n",
    "                for ha in possible_ha:\n",
    "                    for va in possible_va:\n",
    "                        num_potential_candidates += 1\n",
    "\n",
    "                        # --- Calcul de la BBox à partir de W, H, X, Y, ha, va ---\n",
    "                        if ha == 'left':\n",
    "                            bbx_min = X\n",
    "                            bbx_max = X + W\n",
    "                        elif ha == 'center':\n",
    "                            bbx_min = X - W / 2.0\n",
    "                            bbx_max = X + W / 2.0\n",
    "                        else: # ha == 'right'\n",
    "                            bbx_min = X - W\n",
    "                            bbx_max = X\n",
    "\n",
    "                        if va == 'bottom':\n",
    "                            bby_min = Y\n",
    "                            bby_max = Y + H\n",
    "                        elif va == 'center':\n",
    "                            bby_min = Y - H / 2.0\n",
    "                            bby_max = Y + H / 2.0\n",
    "                        else: # va == 'top'\n",
    "                            bby_min = Y - H\n",
    "                            bby_max = Y\n",
    "\n",
    "                        # --- Vérification des limites du cadre ---\n",
    "                        EPS = 1e-9 # Tolérance pour les comparaisons flottantes\n",
    "                        if (\n",
    "                            bbx_min >= x_min - EPS and\n",
    "                            bbx_max <= x_max + EPS and\n",
    "                            bby_min >= y_min - EPS and\n",
    "                            bby_max <= y_max + EPS\n",
    "                        ):\n",
    "                            dist = math.dist((x_i, y_i), (X, Y))\n",
    "                            # dist_int = int(round(dist * 100000)) # On garde pour l'objectif MILP\n",
    "                            dist_int = int(dist * 100000 + 0.5) # Arrondi plus robuste\n",
    "                            bbox_tup = (bbx_min, bbx_max, bby_min, bby_max)\n",
    "                            cands_for_i.append((X, Y, ha, va, bbox_tup, dist_int))\n",
    "                            num_valid_candidates += 1\n",
    "\n",
    "        candidate_positions.append(cands_for_i)\n",
    "        # print(f\"  Label {i} ('{label_text}'): {len(cands_for_i)} candidats valides générés.\")\n",
    "\n",
    "    total_candidates = sum(len(c) for c in candidate_positions)\n",
    "    print(f\"Génération terminée. Total candidats potentiels: {num_potential_candidates}, Total candidats valides: {total_candidates}\")\n",
    "    if total_candidates == 0 and len(positions_and_labels) > 0:\n",
    "        print(\"ATTENTION : Aucun candidat valide n'a été généré. Vérifiez les limites (x/y_min/max) et les offsets.\")\n",
    "        # return [] # Peut-être retourner vide ici si aucun candidat n'est possible\n",
    "\n",
    "\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #  2) PREMIÈRE PASSE : MINIMISER LA SOMME DES DISTANCES (MILP)\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    solver_1 = pywraplp.Solver.CreateSolver('CBC')\n",
    "    if not solver_1:\n",
    "        print(\"Erreur: Impossible de créer le solveur CBC.\")\n",
    "        return [] # Ou gérer l'erreur autrement\n",
    "\n",
    "    # *** AJOUT DE LA LIMITE DE TEMPS POUR LA PASSE 1 ***\n",
    "    if time_limit_ms_pass1 is not None and time_limit_ms_pass1 > 0:\n",
    "        print(f\"[Passe 1] Application d'une limite de temps de {time_limit_ms_pass1} ms.\")\n",
    "        solver_1.SetTimeLimit(time_limit_ms_pass1)\n",
    "\n",
    "    # Variables booléennes z1_{i,p}\n",
    "    z1_vars = {}\n",
    "    # ... (création des variables z1 identique) ...\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        for p, cand_p in enumerate(cands_i):\n",
    "            z1_vars[(i, p)] = solver_1.BoolVar(f\"z1_{i}_{p}\")\n",
    "\n",
    "\n",
    "    # Contraintes (identiques)\n",
    "    # ... (contrainte \"un seul p par i\" identique) ...\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        solver_1.Add(sum(z1_vars[(i, p)] for p in range(len(cands_i))) == 1)\n",
    "    # ... (contrainte \"non-chevauchement\" identique) ...\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        for p, cand_p in enumerate(cands_i):\n",
    "            bbox_p = cand_p[4]\n",
    "            for j in range(i+1, len(candidate_positions)):\n",
    "                for q, cand_q in enumerate(candidate_positions[j]):\n",
    "                    bbox_q = cand_q[4]\n",
    "                    # On suppose que overlap est défini ailleurs\n",
    "                    if overlap(bbox_p, bbox_q):\n",
    "                         solver_1.Add(z1_vars[(i, p)] + z1_vars[(j, q)] <= 1)\n",
    "\n",
    "\n",
    "    # Objectif 1 (identique)\n",
    "    distance_expr_1 = solver_1.Sum(\n",
    "        cand_p[5] * z1_vars[(i,p)]\n",
    "        for i, cands_i in enumerate(candidate_positions)\n",
    "        for p, cand_p in enumerate(cands_i)\n",
    "    )\n",
    "    solver_1.Minimize(distance_expr_1)\n",
    "\n",
    "    # Résolution de la première passe\n",
    "    print(\"[Passe 1] Lancement de la résolution...\")\n",
    "    status_1 = solver_1.Solve()\n",
    "    print(f\"[Passe 1] Résolution terminée avec le statut : {status_1}\") \n",
    "          # ({pywraplp.Solver.StatusName(status_1)})\") # CORRIGÉ\n",
    "\n",
    "\n",
    "    # Vérification du statut après résolution (IMPORTANT si limite de temps)\n",
    "    # Si le temps est écoulé, le statut peut être FEASIBLE mais pas OPTIMAL\n",
    "    if status_1 != pywraplp.Solver.OPTIMAL and status_1 != pywraplp.Solver.FEASIBLE:\n",
    "        print(\"[2passes] Aucune solution (ou solution non optimale/réalisable) trouvée lors de la première passe.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    dist_min_float = solver_1.Objective().Value()\n",
    "   # dist_min_int = int(round(dist_min_float))\n",
    "    print(f\"[Passe 1] Distance minimale trouvée (approximative) : {dist_min_float / 100000.0}\")\n",
    "\n",
    "    # ... (calcul sum_of_dist_int identique) ...\n",
    "    sum_of_dist_int = 0\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        for p, cand_p in enumerate(cands_i):\n",
    "            if z1_vars[(i,p)].solution_value() > 0.5:\n",
    "                 sum_of_dist_int += cand_p[5]\n",
    "\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #  3) DEUXIÈME PASSE : distance = dist_min, maximiser #center\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    solver_2 = pywraplp.Solver.CreateSolver('CBC')\n",
    "    if not solver_2:\n",
    "        print(\"Erreur: Impossible de créer le solveur CBC pour la passe 2.\")\n",
    "        # Vous pourriez retourner la solution de la passe 1 si elle existe\n",
    "        # Ou retourner la solution originale\n",
    "        return [] # Simplifié ici\n",
    "\n",
    "    # *** AJOUT DE LA LIMITE DE TEMPS POUR LA PASSE 2 ***\n",
    "    if time_limit_ms_pass2 is not None and time_limit_ms_pass2 > 0:\n",
    "        print(f\"[Passe 2] Application d'une limite de temps de {time_limit_ms_pass2} ms.\")\n",
    "        solver_2.SetTimeLimit(time_limit_ms_pass2)\n",
    "\n",
    "    # Variables booléennes z2_{i,p}\n",
    "    z2_vars = {}\n",
    "    # ... (création des variables z2 identique) ...\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        for p, cand_p in enumerate(cands_i):\n",
    "            z2_vars[(i, p)] = solver_2.BoolVar(f\"z2_{i}_{p}\")\n",
    "\n",
    "    # Contraintes (identiques)\n",
    "    # ... (contrainte \"un seul p par i\" identique) ...\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        solver_2.Add(sum(z2_vars[(i, p)] for p in range(len(cands_i))) == 1)\n",
    "    # ... (contrainte \"non-chevauchement\" identique) ...\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        for p, cand_p in enumerate(cands_i):\n",
    "            bbox_p = cand_p[4]\n",
    "            for j in range(i+1, len(candidate_positions)):\n",
    "                for q, cand_q in enumerate(candidate_positions[j]):\n",
    "                    bbox_q = cand_q[4]\n",
    "                    if overlap(bbox_p, bbox_q):\n",
    "                         solver_2.Add(z2_vars[(i,p)] + z2_vars[(j,q)] <= 1)\n",
    "\n",
    "    # Contrainte sur la distance (identique)\n",
    "    distance_expr_2 = solver_2.Sum(\n",
    "        cand_p[5] * z2_vars[(i,p)]\n",
    "        for i, cands_i in enumerate(candidate_positions)\n",
    "        for p, cand_p in enumerate(cands_i)\n",
    "    )\n",
    "    # Utiliser dist_min_int calculé ou sum_of_dist_int qui est plus précis\n",
    "    solver_2.Add(distance_expr_2 == sum_of_dist_int) # Préférable\n",
    "\n",
    "\n",
    "    # Objectif 2 (identique)\n",
    "    center_expr_terms = []\n",
    "    # ... (calcul de center_expr_terms identique) ...\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        for p, cand_p in enumerate(cands_i):\n",
    "            (_, _, ha, va, _, _) = cand_p\n",
    "            center_score = 0\n",
    "            if ha == 'center': center_score += 1\n",
    "            if va == 'center': center_score += 1\n",
    "            if center_score > 0:\n",
    "                center_expr_terms.append(center_score * z2_vars[(i,p)])\n",
    "\n",
    "    center_expr = solver_2.Sum(center_expr_terms)\n",
    "    solver_2.Maximize(center_expr)\n",
    "\n",
    "\n",
    "    # Résolution de la 2ᵉ passe\n",
    "    print(\"[Passe 2] Lancement de la résolution...\")\n",
    "    status_2 = solver_2.Solve()\n",
    "    print(f\"[Passe 2] Résolution terminée avec le statut : {status_2}\")# ({pywraplp.Solver.StatusName(status_2)})\") # CORRIGÉ\n",
    "\n",
    "\n",
    "    # Vérification du statut après résolution (IMPORTANT si limite de temps)\n",
    "    if status_2 != pywraplp.Solver.OPTIMAL and status_2 != pywraplp.Solver.FEASIBLE:\n",
    "        print(\"[2passes] Aucune solution (ou solution non optimale/réalisable) trouvée lors de la deuxième passe.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #  4) EXTRACTION FINALE DE LA SOLUTION (depuis la Passe 2)\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    solution = []\n",
    "    # ... (code d'extraction identique, basé sur z2_vars) ...\n",
    "    for i, cands_i in enumerate(candidate_positions):\n",
    "        for p, cand_p in enumerate(cands_i):\n",
    "            # Utiliser une tolérance pour la vérification de la valeur de solution\n",
    "            if z2_vars[(i,p)].solution_value() > 0.5:\n",
    "                (X, Y, ha, va, bbox_tup, dist_int) = cand_p\n",
    "                # Utiliser la distance réelle calculée lors de la génération\n",
    "                dist_reelle = math.dist(positions_and_labels[i][0], (X,Y))\n",
    "                # Ou recalculer si nécessaire: dist_reelle = dist_int / 100000.0\n",
    "                solution.append((i, X, Y, ha, va, bbox_tup, dist_reelle))\n",
    "                break\n",
    "\n",
    "    print(f\"[2passes] Solution finale extraite avec {len(solution)} labels placés.\")\n",
    "    return solution\n",
    "\n",
    "\n",
    "def overlap(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Teste si deux bounding boxes se chevauchent strictement.\n",
    "    bbox = (xmin, xmax, ymin, ymax)\n",
    "    \"\"\"\n",
    "    return not (\n",
    "        bbox1[1] < bbox2[0] or  # bbox1.xmax < bbox2.xmin\n",
    "        bbox1[0] > bbox2[1] or  # bbox1.xmin > bbox2.xmax\n",
    "        bbox1[3] < bbox2[2] or  # bbox1.ymax < bbox2.ymin\n",
    "        bbox1[2] > bbox2[3]     # bbox1.ymin > bbox2.ymax\n",
    "    )\n",
    "\n",
    "def bounding_box_with_patch(ax,\n",
    "                            label_text,\n",
    "                            x, \n",
    "                            y,\n",
    "                            ha='left', \n",
    "                            va='center'):\n",
    "    \"\"\"\n",
    "    Crée *temporairement* un texte invisible,\n",
    "    avec EXACTEMENT le bbox dict(...) que vous utiliserez pour l’affichage,\n",
    "    puis récupère la bbox de ce patch, en coordonnées data (ax).\n",
    "\n",
    "    Retourne (xmin, xmax, ymin, ymax).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) On crée un objet Text, invisible (color='none'),\n",
    "    #    MAIS avec le même bbox que l'affichage final\n",
    "    t = ax.text(\n",
    "        x, y, label_text,\n",
    "        ha=ha, va=va,\n",
    "        color='none',\n",
    "        bbox=main_bbox_style\n",
    "    )\n",
    "\n",
    "    # 2) Forcer le dessin pour que le patch soit calculé\n",
    "    ax.figure.canvas.draw()\n",
    "\n",
    "    # 3) Récupérer la bounding box du patch (le cadre gris)\n",
    "    patch = t.get_bbox_patch()\n",
    "    if patch is not None:\n",
    "        bbox = patch.get_window_extent()\n",
    "    else:\n",
    "        # fallback, au cas où (rare)\n",
    "        bbox = t.get_window_extent()\n",
    "\n",
    "    # 4) Convertir la bbox en coords \"data\"\n",
    "    bbox_data = bbox.transformed(ax.transData.inverted())\n",
    "\n",
    "    # 5) Supprimer le texte temporaire\n",
    "    t.remove()\n",
    "\n",
    "    # 6) Retour (xmin, xmax, ymin, ymax)\n",
    "    return (bbox_data.x0, bbox_data.x1, bbox_data.y0, bbox_data.y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assurez-vous que les autres imports nécessaires sont présents ailleurs\n",
    "\n",
    "def get_renderer(fig):\n",
    "    \"\"\"\n",
    "    Fonction utilitaire pour obtenir une instance de rendu (renderer).\n",
    "    Essaye d'obtenir un renderer sans forcer un dessin si possible.\n",
    "    Parfois, un premier dessin peut être nécessaire pour initialiser le backend.\n",
    "    \"\"\"\n",
    "    # Si le canvas n'a jamais été dessiné, get_renderer peut retourner None ou\n",
    "    # un renderer invalide. On peut forcer un dessin initial si nécessaire.\n",
    "    # Note : Idéalement, ce premier dessin (si requis) devrait être fait UNE SEULE fois\n",
    "    # AVANT la boucle qui appelle N*C fois fast_bounding_box_estimate.\n",
    "    # Exemple (à mettre avant la boucle de génération des candidats) :\n",
    "    # try:\n",
    "    #     renderer = fig.canvas.get_renderer()\n",
    "    #     # Test rapide pour voir si le renderer semble valide\n",
    "    #     _ = renderer.get_text_width_height_descent(\"test\", plt.rcParams, ismath=False)\n",
    "    # except AttributeError: # Ou autre exception selon le backend si non initialisé\n",
    "    #     print(\"Renderer non valide, tentative de dessin initial...\")\n",
    "    #     fig.canvas.draw() # Force UN dessin initial\n",
    "    #     renderer = fig.canvas.get_renderer()\n",
    "\n",
    "    # Pour simplifier ici, on suppose qu'un renderer valide est disponible.\n",
    "    # Une gestion plus robuste peut être nécessaire.\n",
    "    return fig.canvas.get_renderer()\n",
    "\n",
    "\n",
    "def fast_bounding_box_estimate(ax,\n",
    "                               renderer, # Le renderer doit être passé en argument\n",
    "                               label_text,\n",
    "                               x, y,\n",
    "                               ha='left', va='center'): # Passer le style utilisé pour l'affichage final\n",
    "    \"\"\"\n",
    "    Estime la bounding box d'un objet texte SANS appeler draw().\n",
    "    Nécessite une instance de renderer valide.\n",
    "\n",
    "    Retourne (xmin, xmax, ymin, ymax) en coordonnées data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Vérification préliminaire\n",
    "    if renderer is None:\n",
    "        # Gérer l'erreur : Le renderer est indispensable ici\n",
    "        # On pourrait essayer d'en obtenir un, mais cela ralentirait potentiellement.\n",
    "        # Il est préférable de l'obtenir une fois à l'extérieur et de le passer.\n",
    "        raise ValueError(\"Un renderer valide doit être fourni à fast_bounding_box_estimate.\")\n",
    "\n",
    "    # 1) Créer l'objet Text temporaire, mais le rendre invisible\n",
    "    #    Utiliser visible=False est plus propre que color='none'.\n",
    "    t = ax.text(\n",
    "        x, y, label_text,\n",
    "        ha=ha, va=va,\n",
    "        bbox=main_bbox_style,\n",
    "        visible=False,  # Rendre invisible\n",
    "        # Important: S'assurer que la police, la taille, etc., sont les mêmes que pour l'affichage final\n",
    "        # fontproperties=... # si nécessaire\n",
    "    )\n",
    "\n",
    "    # 2) Obtenir l'étendue (extent) en utilisant le renderer fourni\n",
    "    #    On essaie d'abord avec le patch (cadre) car il inclut le padding.\n",
    "    bbox_disp = None\n",
    "    try:\n",
    "        patch = t.get_bbox_patch()\n",
    "        if patch:\n",
    "            # Obtenir l'étendue du patch en coordonnées d'affichage (pixels)\n",
    "            bbox_disp = patch.get_window_extent(renderer=renderer)\n",
    "        else:\n",
    "            bbox_disp = t.get_window_extent(renderer=renderer)\n",
    "            # Note: Le padding du boxstyle pourrait être manquant ici.\n",
    "\n",
    "    except Exception as e:\n",
    "        # Une erreur ici peut indiquer un problème avec le renderer ou le texte\n",
    "        print(f\"Erreur lors de get_window_extent pour '{label_text}': {e}\")\n",
    "        # Important : Supprimer le texte temporaire même en cas d'erreur\n",
    "        t.remove()\n",
    "        # Que retourner ? On peut lever l'erreur, ou retourner une estimation invalide/par défaut\n",
    "        # Retourner une boîte dégénérée au point d'ancrage pour signaler le problème\n",
    "        return (x, x, y, y)\n",
    "\n",
    "\n",
    "    # 3) Convertir la bbox de coordonnées d'affichage en coordonnées \"data\"\n",
    "    if bbox_disp:\n",
    "        bbox_data = bbox_disp.transformed(ax.transData.inverted())\n",
    "        result = (bbox_data.x0, bbox_data.x1, bbox_data.y0, bbox_data.y1)\n",
    "    else:\n",
    "        # Si bbox_disp n'a pas pu être obtenu (cf. bloc try/except)\n",
    "        # On retourne la boîte dégénérée\n",
    "        print(f\"Avertissement: N'a pas pu obtenir bbox_disp pour '{label_text}'.\")\n",
    "        result = (x, x, y, y)\n",
    "\n",
    "\n",
    "    # 4) Supprimer le texte temporaire du graphique\n",
    "    t.remove()\n",
    "\n",
    "    # 5) Retourner les coordonnées data (xmin, xmax, ymin, ymax)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHy81u94W7OV"
   },
   "outputs": [],
   "source": [
    "def plot_pca(matrix_type='W'):\n",
    "    \"\"\"\n",
    "    all_nmf_H : dict[ int -> ndarray ]\n",
    "        Dictionnaire où all_nmf_H[topic_count] est une matrice H (shape = (k, m))\n",
    "        k = nombre de topics, m = taille du vocabulaire.\n",
    "    all_nmf_W : iterable\n",
    "        Liste (ou clés du dict) indiquant les différents topic_count disponibles.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\"):\n",
    "        os.makedirs(f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\")\n",
    "\n",
    "    for topic_count in all_nmf_H:\n",
    "        if matrix_type == 'W':\n",
    "            M = all_nmf_W[topic_count].T  # Matrice (k x m)\n",
    "        else:\n",
    "            M = all_nmf_H[topic_count]\n",
    "\n",
    "        # ---------------------------\n",
    "        # 1) Normalisation L2 par topic (chaque ligne)\n",
    "        # ---------------------------\n",
    "        # On calcule la norme L2 de chaque ligne (axis=1)\n",
    "        norms = np.linalg.norm(M, axis=1, keepdims=True)\n",
    "        # Pour éviter la division par zéro si une ligne est totalement nulle\n",
    "        norms[norms == 0] = 1e-16\n",
    "\n",
    "        M_norm = M / norms  # Division élément par élément\n",
    "\n",
    "        # ---------------------------\n",
    "        # 2) PCA sur H normalisé\n",
    "        # ---------------------------\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_result = pca.fit_transform(M_norm)  # shape = (k, 2)\n",
    "\n",
    "\n",
    "        # 2) Préparation de la figure/axe\n",
    "        _, ax = plt.subplots(\n",
    "            figsize=(FIGURE_WIDTH_INCH, FIGURE_WIDTH_INCH),\n",
    "            dpi=DPI\n",
    "        )\n",
    "\n",
    "        # 3) Construire la liste (positions, labels)\n",
    "        labels = [f'{i}' for i in range(len(pca_result))]\n",
    "        truncated_texts = truncate_texts(topic_labels_by_config[topic_count])\n",
    "\n",
    "        # On associe chaque point PCA à un label\n",
    "        positions_and_labels = [\n",
    "            (tuple(coords), truncated_texts[int(lbl)])\n",
    "            for coords, lbl in zip(pca_result, labels)\n",
    "        ]\n",
    "\n",
    "        # 4) Calculer le min/max pour x et y (cadre à ne pas dépasser)\n",
    "        all_x = [pos_lbl[0][0] for pos_lbl in positions_and_labels]\n",
    "        all_y = [pos_lbl[0][1] for pos_lbl in positions_and_labels]\n",
    "        x_min, x_max = min(all_x), max(all_x)\n",
    "        y_min, y_max = min(all_y), max(all_y)\n",
    "\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.autoscale(False)\n",
    "\n",
    "        # 5) Paramètres pour solve_label_placement_matplotlib\n",
    "        # 1) Calcul de l'étendue (range) de l'axe\n",
    "        range_x = x_max - x_min\n",
    "        range_y = y_max - y_min\n",
    "\n",
    "        # 2) Choix du nombre d'offsets\n",
    "        num_offsets = 5\n",
    "\n",
    "        # 3) Génération des pourcentages entre -5% et +5% (en 10 pas)\n",
    "        #    => np.linspace(-0.05, 0.05, num_offsets)\n",
    "        #    sera par exemple [-0.05, -0.0388, ..., 0.05]\n",
    "\n",
    "        percentages_x = np.linspace(-0.05, 0.05, num_offsets)\n",
    "        percentages_y = np.linspace(-0.05, 0.05, num_offsets)\n",
    "\n",
    "        # 4) Conversion de ces pourcentages en offsets dans les coordonnées du graphique\n",
    "        offsets_x = [p * range_x for p in percentages_x]\n",
    "        offsets_y = [p * range_y for p in percentages_y]\n",
    "\n",
    "        possible_ha = ['left', 'center', 'right']\n",
    "        possible_va = ['bottom', 'center', 'top']\n",
    "\n",
    "        # 6) Appel du solveur (qui va mesurer la bbox via ax)\n",
    "        solution = solve_label_placement_matplotlib_2passes(\n",
    "            ax=ax,\n",
    "            positions_and_labels=positions_and_labels,\n",
    "            x_min=x_min, x_max=x_max,\n",
    "            y_min=y_min, y_max=y_max,\n",
    "            offsets_x=offsets_x, offsets_y=offsets_y,\n",
    "            possible_ha=possible_ha, possible_va=possible_va,\n",
    "            time_limit_ms_pass1=1000*60*10,\n",
    "            time_limit_ms_pass2=1000*60*10\n",
    "        )\n",
    "\n",
    "        if solution == []:\n",
    "            solution = solve_label_placement_force_directed(ax,\n",
    "                                                            positions_and_labels,\n",
    "                                                            x_min, x_max, y_min, y_max)\n",
    "\n",
    "        # 7) Affichage de la solution\n",
    "        for (i, X, Y, ha, va, bbox, cost) in solution:\n",
    "            (ox, oy), text_label = positions_and_labels[i]\n",
    "\n",
    "            # Points d'origine (optionnel si on veut les voir en plus du scatter)\n",
    "            ax.plot(ox, oy, color='red', marker='o', alpha=0.5, markersize=10, markeredgewidth=0)\n",
    "\n",
    "            # Le label positionné\n",
    "            ax.text(X, Y, text_label, ha=ha, va=va,\n",
    "                    bbox=main_bbox_style)\n",
    "\n",
    "            # Une flèche qui relie le point d'origine au label\n",
    "            ax.annotate(\n",
    "                \"\",\n",
    "                xy=(ox, oy),\n",
    "                xytext=(X, Y),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color='black', alpha=0.2)\n",
    "            )\n",
    "\n",
    "        explained_var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "        manual_tick_placement_continuous(\n",
    "            ax=ax,\n",
    "            xmin=x_min,\n",
    "            xmax=x_max,\n",
    "            spacing_factor_min=1.02,\n",
    "            spacing_factor_max=1.2,\n",
    "            step=0.001\n",
    "        )\n",
    "        manual_tick_placement_continuous_Y(\n",
    "            ax=ax,\n",
    "            ymin=y_min,\n",
    "            ymax=y_max,\n",
    "            spacing_factor_min=1.02,\n",
    "            spacing_factor_max=1.2,\n",
    "            step=0.001\n",
    "        )\n",
    "\n",
    "        # 15) Labels des axes, etc.\n",
    "        plt.xlabel(\n",
    "            f'Facteur 1 - Variance expliquée={explained_var_ratio[0]*100:.2f}%',\n",
    "            labelpad=35\n",
    "        )\n",
    "        plt.ylabel(\n",
    "            f'Facteur 2 - Variance expliquée={explained_var_ratio[1]*100:.2f}%',\n",
    "            labelpad=34\n",
    "        )\n",
    "\n",
    "        # Supprimer la bordure du haut et de droite\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        # Ligne horizontale y=0\n",
    "        plt.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "        # Ligne verticale x=0\n",
    "        plt.axvline(0, color='black', linewidth=1, alpha=0.3)\n",
    "\n",
    "        # On désactive la grille\n",
    "        plt.grid(False)\n",
    "\n",
    "        class_suffix = \"_\".join(grammatical_classes)\n",
    "\n",
    "        if not os.path.exists(f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\"):\n",
    "            os.makedirs(f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\")\n",
    "\n",
    "        # 9) Afficher la figure\n",
    "        plt.savefig(\n",
    "            f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\"\n",
    "            f\"{base_name}_{matrix_type.lower()}_pca_plot_{topic_count}tc_l2_{class_suffix}_\"\n",
    "            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "            f\"{go_remove_duplicates}dup.png\",\n",
    "            dpi=DPI,\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYht2nb3KBA3"
   },
   "outputs": [],
   "source": [
    "def is_overlapping(text, other_texts, tolerance=0.0, buffer=0.0):\n",
    "    \"\"\"\n",
    "    Vérifie si un texte se chevauche avec d'autres, avec une tolérance très permissive.\n",
    "\n",
    "    Args:\n",
    "        text: L'objet texte à tester.\n",
    "        other_texts: Liste des objets textes existants.\n",
    "        tolerance: Proportion de tolérance (plus grand = plus tolérant).\n",
    "        buffer: Distance minimale entre les boîtes pour ignorer un chevauchement léger.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si chevauchement significatif, sinon False.\n",
    "    \"\"\"\n",
    "    bbox = text.get_window_extent(renderer=plt.gcf().canvas.get_renderer())\n",
    "    bbox_data = bbox.transformed(plt.gca().transData.inverted())  # Conversion en coordonnées data\n",
    "\n",
    "    for other in other_texts:\n",
    "        other_bbox = other.get_window_extent(renderer=plt.gcf().canvas.get_renderer())\n",
    "        other_bbox_data = other_bbox.transformed(plt.gca().transData.inverted())\n",
    "\n",
    "        # Calcul des dimensions avec \"buffer\" pour agrandir légèrement les boîtes existantes\n",
    "        bbox_data_inflated = [\n",
    "            bbox_data.xmin - buffer, bbox_data.xmax + buffer,\n",
    "            bbox_data.ymin - buffer, bbox_data.ymax + buffer\n",
    "        ]\n",
    "        other_bbox_data_inflated = [\n",
    "            other_bbox_data.xmin - buffer, other_bbox_data.xmax + buffer,\n",
    "            other_bbox_data.ymin - buffer, other_bbox_data.ymax + buffer\n",
    "        ]\n",
    "\n",
    "        # Vérifier le chevauchement agrandi\n",
    "        overlap_x = max(0, min(bbox_data_inflated[1], other_bbox_data_inflated[1]) -\n",
    "                           max(bbox_data_inflated[0], other_bbox_data_inflated[0]))\n",
    "        overlap_y = max(0, min(bbox_data_inflated[3], other_bbox_data_inflated[3]) -\n",
    "                           max(bbox_data_inflated[2], other_bbox_data_inflated[2]))\n",
    "\n",
    "        # Surface d'intersection\n",
    "        overlap_area = overlap_x * overlap_y\n",
    "\n",
    "        # Aire minimale de chevauchement tolérée\n",
    "        area_threshold = tolerance * (bbox_data.width * bbox_data.height)\n",
    "\n",
    "        # Ignorer les chevauchements inférieurs à la tolérance\n",
    "        if overlap_area > area_threshold:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPrd5G30IMuS"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(args):\n",
    "    atb, unigrams = args\n",
    "    tokenized_sents = []\n",
    "    for t in atb:\n",
    "        if t[0] in unigrams:\n",
    "            tokenized_sents.append(t[0])\n",
    "\n",
    "    return tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvNMOepHffb9"
   },
   "outputs": [],
   "source": [
    "def go_tfidf_vectorization(gclasses):\n",
    "    # 1) Traiter les classes grammaticales définies globalement\n",
    "    #    On ajoute tqdm pour visualiser l'avancement\n",
    "    global unigrams\n",
    "\n",
    "    for grammatical_class in tqdm(gclasses, desc=\"Mise à jour des unigrams\"):\n",
    "        unigrams = update_candidates_for_unigram(grammatical_class, unigrams)\n",
    "\n",
    "    tokenized_documents = []\n",
    "    for atb in all_tab_pos:\n",
    "        tokenized_document = [t[0] for t in atb if t[0] in unigrams]\n",
    "        tokenized_documents.append(tokenized_document)\n",
    "\n",
    "    # 4) Suppression des stop words via le pipeline spaCy\n",
    "    spacy_stopwords = nlp_pipeline.Defaults.stop_words\n",
    "    # Si on veut voir la progression ici, on peut boucler :\n",
    "    filtered_docs = []\n",
    "    for doc in tqdm(tokenized_documents, desc=\"Filtrage des stopwords\"):\n",
    "        filtered_docs.append(\n",
    "            [token for token in doc if token.lower() not in spacy_stopwords]\n",
    "        )\n",
    "    tokenized_documents = filtered_docs\n",
    "\n",
    "    # 5) Création des vecteurs TF-IDF\n",
    "    #    Pour avoir une barre de progression, on peut découper manuellement en batches.\n",
    "    #    Toutefois, si la liste n'est pas trop grosse, on peut juste faire fit_transform d'un coup.\n",
    "\n",
    "    def identity_analyzer(tokens):\n",
    "        return tokens\n",
    "\n",
    "    count_vectorizer = CountVectorizer(analyzer=identity_analyzer, lowercase=False)\n",
    "\n",
    "    # Si vous voulez découper en batches pour CountVectorizer.fit_transform,\n",
    "    # il faut recourir à un autre mécanisme (car le fit_transform standard ne propose pas de batch).\n",
    "    # Par défaut, on fait donc un fit_transform \"classique\" :\n",
    "    word_count = count_vectorizer.fit_transform(tokenized_documents)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(norm=None, sublinear_tf=False, smooth_idf=True)\n",
    "    X = tfidf_transformer.fit_transform(word_count)\n",
    "\n",
    "    tfidf_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "    return count_vectorizer, X, tfidf_feature_names, tokenized_documents, tfidf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHHdg_Ocffb9"
   },
   "outputs": [],
   "source": [
    "def write_unigrams_results(nb_words, tfidf_feature_names, nmf_H):\n",
    "    tab_words_nmf = []\n",
    "    for topic_idx, topic in enumerate(nmf_H):\n",
    "      subtab_words_nmf = []\n",
    "      for i in topic.argsort()[:-nb_words - 1:-1]:\n",
    "        subtab_words_nmf.append([(tfidf_feature_names[i]), topic[i]])\n",
    "\n",
    "      tab_words_nmf.append(subtab_words_nmf)\n",
    "\n",
    "\n",
    "    new_tab_words_nmf = []\n",
    "    for t in tab_words_nmf:\n",
    "      sorted_t = sorted(t, key = lambda x: (-x[1]))\n",
    "\n",
    "      new_tab_words_nmf.append(sorted_t)\n",
    "\n",
    "\n",
    "\n",
    "    max_rows_nb = 0\n",
    "    for to in new_tab_words_nmf:\n",
    "      if len(to) > max_rows_nb:\n",
    "        max_rows_nb = len(to)\n",
    "\n",
    "\n",
    "    if not os.path.exists(f\"{results_path}{base_name}_EXPLORE_TOPICS/\"):\n",
    "        os.makedirs(f\"{results_path}{base_name}_EXPLORE_TOPICS/\")\n",
    "\n",
    "    class_suffix = \"_\".join(grammatical_classes)\n",
    "    with open(\n",
    "        f\"{results_path}{base_name}_EXPLORE_TOPICS/\"\n",
    "        f\"{base_name}_{len(nmf_H)}tc_topic_modeling_unigrams_{class_suffix}_\"\n",
    "        f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "        f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp.csv\",\n",
    "        \"w\",\n",
    "        encoding=\"utf-8\"\n",
    "    ) as file_object:\n",
    "        writer = csv.writer(file_object)\n",
    "\n",
    "        i = 0\n",
    "        while i < max_rows_nb:\n",
    "            new_row = \"\"\n",
    "            for to in new_tab_words_nmf:\n",
    "                if i < len(to):\n",
    "                    if len(new_row) > 0:\n",
    "                        new_row = new_row + \",\" + (to[i][0]) + \",\" + str(to[i][1])\n",
    "                    else:\n",
    "                        new_row = (to[i][0]) + \",\" + str(to[i][1])\n",
    "\n",
    "            file_object.write(new_row)\n",
    "            file_object.write(\"\\n\")\n",
    "\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ferTRgwzYDN"
   },
   "outputs": [],
   "source": [
    "def determine_nmf(topic_list, alpha_W, alpha_H, l1_ratio, n_top_words=15, window_size=100):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle NMF pour chaque nombre de topics dans topic_list\n",
    "    et calcule la métrique de cohérence (type sliding window) c_npmi.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    topic_list : list\n",
    "        Liste des nombres de topics à tester (ex. [5, 10, 15]).\n",
    "    n_top_words : int\n",
    "        Nombre de mots que l’on va extraire pour chaque topic (top words).\n",
    "    window_size : int\n",
    "        Taille de la fenêtre de co-occurrence pour la cohérence (sliding window).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    None. (Les résultats sont directement enregistrés dans les dictionnaires globaux.)\n",
    "    \"\"\"\n",
    "\n",
    "    # On indique qu’on va modifier à ces variables globales\n",
    "    global all_nmf_H, all_nmf_W\n",
    "    global coherence_scores\n",
    "\n",
    "    # 1. Construire le dictionary Gensim à partir des documents tokenisés\n",
    "  #  dictionary = Dictionary(tokenized_documents)\n",
    "    # Optionnel : filtrer les tokens trop rares ou trop fréquents\n",
    "    # dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "    # 2. Pour chaque valeur de topics dans topic_list, on entraîne un modèle NMF\n",
    "    nmf_models = {}\n",
    "    for num_topic in tqdm(topic_list, desc=\"PROCESSUS DES TOPICS\"):\n",
    "        nmf_model = NMF(\n",
    "            n_components=num_topic,\n",
    "            random_state=1,\n",
    "            max_iter=10000,\n",
    "            alpha_W=alpha_W,      # remplace alpha=0.2\n",
    "            alpha_H=alpha_H,      # idem\n",
    "            l1_ratio=l1_ratio,  # Ratio proche de 0 => plus de L2\n",
    "            init='nndsvd'\n",
    "        ).fit(tfidf)\n",
    "\n",
    "        nmf_W = nmf_model.transform(tfidf)\n",
    "        nmf_H = nmf_model.components_\n",
    "\n",
    "        # Stocker les matrices W et H dans les dictionnaires globaux\n",
    "        all_nmf_W[num_topic] = nmf_W\n",
    "        all_nmf_H[num_topic] = nmf_H\n",
    "\n",
    "        # 3. Extraire les top words de chaque topic\n",
    "        topic_words = []\n",
    "        for t in range(num_topic):\n",
    "            # Trouver les indices des \"top n_top_words\" en ordre décroissant\n",
    "            top_word_indexes = nmf_H[t].argsort()[:-n_top_words-1:-1]\n",
    "            # Récupérer les mots associés à ces indices\n",
    "            words_for_topic_t = [tfidf_feature_names[idx] for idx in top_word_indexes]\n",
    "            topic_words.append(words_for_topic_t)\n",
    "\n",
    "        npmi_calculator = Coherence(\n",
    "            texts=tokenized_documents, # Le corpus tokenisé (liste de documents, chaque document est une liste de tokens)\n",
    "            topics=topics,             # La liste des topics (liste de listes de mots-clés) <-- PARAMÈTRE AJOUTÉ/ESSENTIEL\n",
    "            measure='c_npmi',\n",
    "            window_size = 30\n",
    "        )\n",
    "\n",
    "        # Calculer le score\n",
    "        coherence_score = npmi_calculator.score()\n",
    "\n",
    "        coherence_scores[num_topic] = coherence_score\n",
    "\n",
    "\n",
    "        nmf_models[num_topic] = nmf_model\n",
    "\n",
    "    # 7. Fonctions de post-traitement (optionnelles)\n",
    "    write_topics_unigrams()\n",
    "\n",
    "    return nmf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwmnFsq1yawa"
   },
   "outputs": [],
   "source": [
    "def process_documents(documents):\n",
    "    # Calcul du nombre de cœurs disponibles\n",
    "   # gpu_activated = spacy.prefer_gpu()  # True si GPU détecté, sinon None\n",
    "\n",
    "   # print('gpu_activated', gpu_activated)\n",
    "\n",
    "    # n_process=1 si on a un GPU, sinon on utilise tous les CPU\n",
    "  #  n_process = 1 if gpu_activated else multiprocessing.cpu_count()\n",
    "    n_process = multiprocessing.cpu_count()\n",
    "  #  n_process = 2\n",
    "\n",
    "    print(f\"Utilisation de {n_process} processus parallèles pour spaCy.\")\n",
    "\n",
    "    # On s'assure de (ré)initialiser les tableaux globaux si nécessaire\n",
    "    global all_tab_pos, sentences_norms, all_sentence_pos\n",
    "   # documents_lemmatized = []\n",
    "    all_tab_pos = []\n",
    "    all_sentence_pos = []\n",
    "    sentences_norms = []\n",
    "\n",
    "    # Préparer une barre de progression\n",
    "    pbar = tqdm(total=len(documents), desc='DOCUMENTS PROCESSÉS')\n",
    "\n",
    "    # Traitement en parallèle avec nlp.pipe\n",
    "    try:\n",
    "        # Par défaut, spaCy divise en batch de ~1000 tokens.\n",
    "        # On peut ajuster batch_size si besoin (ex: batch_size=20 ou 50).\n",
    "        for spacy_doc in nlp_pipeline.pipe(documents, n_process=n_process, batch_size=4):\n",
    "            doc_for_ngrams = ''\n",
    "            tab_pos = []\n",
    "\n",
    "            for sent in spacy_doc.sents:\n",
    "                sentence_pos = []\n",
    "                norms = []\n",
    "                lemmes = []\n",
    "\n",
    "                for token in sent:\n",
    "                    pos = token.pos_\n",
    "                    lemma = token.lemma_.lower()\n",
    "\n",
    "                    # Exemple: unidecode si c'est un PROPN\n",
    "                    if pos == 'PROPN':\n",
    "                        lemma = unidecode.unidecode(lemma)\n",
    "\n",
    "                    if lemma not in [\" \", \"\\n\", \"\\t\"]:\n",
    "                        doc_for_ngrams += lemma + ' '\n",
    "                        tab_pos.append([lemma, pos])\n",
    "                        sentence_pos.append([lemma, pos])\n",
    "                        lemmes.append(lemma)\n",
    "                        norms.append(token.norm_)\n",
    "\n",
    "                sentences_norms.append(\" \".join(norms))\n",
    "\n",
    "                all_sentence_pos.append(sentence_pos)\n",
    "\n",
    "            #documents_lemmatized.append(doc_for_ngrams)\n",
    "            all_tab_pos.append(tab_pos)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement des documents : {e}\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Écriture des résultats sur disque (ou autre)\n",
    "    write_raw_documents()\n",
    "  #  write_lemmatized_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsIGROXDJ5e8"
   },
   "outputs": [],
   "source": [
    "def extract_and_convert_date(date_str):\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except (parser.ParserError, ValueError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAw3i_UqzYDO"
   },
   "outputs": [],
   "source": [
    "def write_raw_documents():\n",
    "    if not os.path.exists(f\"{results_path}{base_name}_RAW/\"):\n",
    "            os.makedirs(f\"{results_path}{base_name}_RAW/\")\n",
    "\n",
    "    with open(f\"{results_path}{base_name}_RAW/raw_documents.txt\", \"w\", encoding='utf-8') as file_object:\n",
    "        for dfn in documents:\n",
    "            file_object.write(dfn + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6QrgrEG_2zh"
   },
   "outputs": [],
   "source": [
    "#def write_lemmatized_documents():\n",
    " #   with open(f\"{results_path}{base_name}_RAW/{base_name}_lemmatized_documents.txt\",\n",
    "  #            \"w\",\n",
    "   #           encoding='utf-8') as file_object:\n",
    "    #    for dfn in documents_lemmatized:\n",
    "     #       file_object.write(dfn + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3l4UeX3yzYDO"
   },
   "outputs": [],
   "source": [
    "def extract_information(header, selector):\n",
    "    elements = header.select(selector)\n",
    "    if elements:\n",
    "        return \"////\".join([get_text_from_tag(el).replace(';', ',') for el in elements])\n",
    "    else:\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikKezsdQzYDO"
   },
   "outputs": [],
   "source": [
    "def get_text_from_tag(tag):\n",
    "    return ''.join(tag.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oryU-8PozYDO"
   },
   "outputs": [],
   "source": [
    "def normalize_journal(t):\n",
    "    t = t.strip()\n",
    "\n",
    "    # Supprimer tout ce qui se trouve entre parenthèses (y compris les parenthèses)\n",
    "    t = re.sub(r'\\(.*?\\)', '', t)\n",
    "\n",
    "    # Supprimer tout ce qui se trouve après la première virgule\n",
    "    t = re.sub(r',.*', '', t)\n",
    "\n",
    "    # Supprimer tout ce qui se trouve après le premier tiret précédé d'un espace\n",
    "    t = re.sub(r' -.*', '', t)\n",
    "\n",
    "    # Supprimer tout ce qui suit trois espaces vides ou plus\n",
    "    t = re.sub(r' {3,}.*', '', t)\n",
    "\n",
    "    if not web_paper_differentiation:\n",
    "        # Supprimer les préfixes \"www.\"\n",
    "        t = re.sub(r'^www\\.', '', t)\n",
    "\n",
    "        # Supprimer les extensions de domaine\n",
    "        t = re.sub(r'(\\.\\w{2,3})+$', '', t)\n",
    "\n",
    "    # Trim le texte\n",
    "    t = t.strip()\n",
    "\n",
    "    return t.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEWRP5HBzYDO"
   },
   "outputs": [],
   "source": [
    "def extract_date_info(date_text, language='fr'):\n",
    "    if language == 'fr':\n",
    "        regex = \"([1-3]?[0-9]\\\\s(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)\\\\s20[0-2][0-9])\"\n",
    "    elif language == 'en':\n",
    "        regex = \"([1-3]?[0-9]\\\\s(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s20[0-2][0-9])\"\n",
    "\n",
    "    date_text_clean = re.search(regex, date_text)\n",
    "    return date_text_clean.group() if date_text_clean else date_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2u6MilUzYDO"
   },
   "outputs": [],
   "source": [
    "def normalise_date(date_text):\n",
    "    # Dictionnaire combiné des mois en anglais, français, espagnol et allemand avec leurs variations\n",
    "    month_dict = {\n",
    "        # Mois en anglais\n",
    "        'january': '01', 'jan': '01',\n",
    "        'february': '02', 'feb': '02',\n",
    "        'march': '03', 'mar': '03',\n",
    "        'april': '04', 'apr': '04',\n",
    "        'may': '05',\n",
    "        'june': '06', 'jun': '06',\n",
    "        'july': '07', 'jul': '07',\n",
    "        'august': '08', 'aug': '08',\n",
    "        'september': '09', 'sep': '09', 'sept': '09',\n",
    "        'october': '10', 'oct': '10',\n",
    "        'november': '11', 'nov': '11',\n",
    "        'december': '12', 'dec': '12',\n",
    "        # Mois en français\n",
    "        'janvier': '01', 'janv.': '01', 'janv': '01',\n",
    "        'février': '02', 'févr.': '02', 'févr': '02', 'fevrier': '02', 'fevr': '02',\n",
    "        'mars': '03',\n",
    "        'avril': '04', 'avr.': '04', 'avr': '04',\n",
    "        'mai': '05',\n",
    "        'juin': '06',\n",
    "        'juillet': '07', 'juil.': '07', 'juil': '07',\n",
    "        'août': '08', 'aout': '08', 'aôut': '08',\n",
    "        'septembre': '09', 'sept.': '09', 'sept': '09',\n",
    "        'octobre': '10', 'oct.': '10', 'oct': '10',\n",
    "        'novembre': '11', 'nov.': '11', 'nov': '11',\n",
    "        'décembre': '12', 'déc.': '12', 'déc': '12', 'decembre': '12', 'dec': '12',\n",
    "        # Mois en espagnol\n",
    "        'enero': '01', 'ene.': '01', 'ene': '01',\n",
    "        'febrero': '02', 'feb.': '02', 'feb': '02',\n",
    "        'marzo': '03', 'mar.': '03', 'mar': '03',\n",
    "        'abril': '04', 'abr.': '04', 'abr': '04',\n",
    "        'mayo': '05', 'may.': '05', 'may': '05',\n",
    "        'junio': '06', 'jun.': '06', 'jun': '06',\n",
    "        'julio': '07', 'jul.': '07', 'jul': '07',\n",
    "        'agosto': '08', 'ago.': '08', 'ago': '08',\n",
    "        'septiembre': '09', 'sept.': '09', 'sep': '09', 'setiembre': '09', 'set.': '09', 'set': '09',\n",
    "        'octubre': '10', 'oct.': '10', 'oct': '10',\n",
    "        'noviembre': '11', 'nov.': '11', 'nov': '11',\n",
    "        'diciembre': '12', 'dic.': '12', 'dic': '12',\n",
    "        # Mois en allemand\n",
    "        'januar': '01', 'jan.': '01', 'jan': '01',\n",
    "        'februar': '02', 'feb.': '02', 'feb': '02',\n",
    "        'märz': '03', 'maerz': '03', 'mär.': '03', 'marz': '03', 'mar.': '03', 'mar': '03',\n",
    "        'april': '04', 'apr.': '04', 'apr': '04',\n",
    "        'mai': '05',\n",
    "        'juni': '06', 'jun.': '06', 'jun': '06',\n",
    "        'juli': '07', 'jul.': '07', 'jul': '07',\n",
    "        'august': '08', 'aug.': '08', 'aug': '08',\n",
    "        'september': '09', 'sept.': '09', 'sep': '09', 'sept': '09',\n",
    "        'oktober': '10', 'okt.': '10', 'okt': '10',\n",
    "        'november': '11', 'nov.': '11', 'nov': '11',\n",
    "        'dezember': '12', 'dez.': '12', 'dez': '12'\n",
    "    }\n",
    "\n",
    "    # Nettoyer le texte de la date\n",
    "    date_text = date_text.lower().strip()\n",
    "\n",
    "    # Liste unifiée des formats de dates à essayer\n",
    "    date_formats = [\n",
    "        # Exemples : 19 de noviembre de 2021, 19 novembre 2021, 19 november 2021, 19. November 2021\n",
    "        r\"(?:\\b\\w+\\b,\\s+)?(\\d{1,2})(?:\\.|\\s+de|\\s+)?\\s*([\\w\\.\\-]+)(?:\\s+de)?\\s+(\\d{4})\",\n",
    "        # Exemples : noviembre 19, 2021, november 19, 2021\n",
    "        r\"(?:\\b\\w+\\b,\\s+)?([\\w\\.\\-]+)\\s+(\\d{1,2}),?\\s+(\\d{4})\",\n",
    "        # Formats numériques : 19/11/2021, 11/19/2021\n",
    "        r\"(\\d{1,2})/(\\d{1,2})/(\\d{4})\",\n",
    "        # Formats numériques avec tirets : 19-11-2021, 11-19-2021\n",
    "        r\"(\\d{1,2})-(\\d{1,2})-(\\d{4})\",\n",
    "        # Année en premier : 2021-11-19\n",
    "        r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\",\n",
    "        # Année en premier avec slash : 2021/11/19\n",
    "        r\"(\\d{4})/(\\d{1,2})/(\\d{1,2})\",\n",
    "        # Formats avec points : 19.11.2021\n",
    "        r\"(\\d{1,2})\\.(\\d{1,2})\\.(\\d{4})\",\n",
    "    ]\n",
    "\n",
    "    for pattern in date_formats:\n",
    "        match = re.search(pattern, date_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            # Déterminer l'ordre des éléments en fonction du motif\n",
    "            if pattern.startswith(r\"(?:\\b\\w+\\b,\\s+)?(\\d{1,2})\"):\n",
    "                # Motif : Jour [de] Mois [de] Année (ex : 19 de noviembre de 2021)\n",
    "                day, month, year = groups\n",
    "            elif pattern.startswith(r\"(?:\\b\\w+\\b,\\s+)?([\\w\\.\\-]+)\"):\n",
    "                # Motif : Mois Jour, Année (ex : noviembre 19, 2021)\n",
    "                month, day, year = groups\n",
    "            elif pattern.startswith(r\"(\\d{1,2})/(\\d{1,2})/\"):\n",
    "                # Motif : Numérique avec slash (ambigu)\n",
    "                first, second, year = groups\n",
    "                if int(first) > 12:\n",
    "                    # Probablement Jour/Mois/Année\n",
    "                    day, month = first, second\n",
    "                elif int(second) > 12:\n",
    "                    # Probablement Mois/Jour/Année\n",
    "                    month, day = first, second\n",
    "                else:\n",
    "                    # Ambigu, par défaut Jour/Mois/Année\n",
    "                    day, month = first, second\n",
    "                day = day.zfill(2)\n",
    "                month = month.zfill(2)\n",
    "                return f\"{year}-{month}-{day}\"\n",
    "            elif pattern.startswith(r\"(\\d{1,2})-(\\d{1,2})-\"):\n",
    "                # Motif : Numérique avec tirets (ambigu)\n",
    "                first, second, year = groups\n",
    "                if int(first) > 12:\n",
    "                    day, month = first, second\n",
    "                elif int(second) > 12:\n",
    "                    month, day = first, second\n",
    "                else:\n",
    "                    day, month = first, second\n",
    "                day = day.zfill(2)\n",
    "                month = month.zfill(2)\n",
    "                return f\"{year}-{month}-{day}\"\n",
    "            elif pattern.startswith(r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\"):\n",
    "                # Motif : Année-Mois-Jour\n",
    "                year, month, day = groups\n",
    "            elif pattern.startswith(r\"(\\d{4})/(\\d{1,2})/(\\d{1,2})\"):\n",
    "                # Motif : Année/Mois/Jour\n",
    "                year, month, day = groups\n",
    "            elif pattern.startswith(r\"(\\d{1,2})\\.(\\d{1,2})\\.(\\d{4})\"):\n",
    "                # Motif : Jour.Mois.Année\n",
    "                day, month, year = groups\n",
    "            else:\n",
    "                # Motif non reconnu\n",
    "                continue\n",
    "\n",
    "            month = month.lower().replace('.', '').strip()\n",
    "            day = day.zfill(2)\n",
    "\n",
    "            # Convertir le mois en chiffre\n",
    "            if month.isdigit():\n",
    "                month_num = month.zfill(2)\n",
    "            elif month in month_dict:\n",
    "                month_num = month_dict[month]\n",
    "            else:\n",
    "                print(f\"Attention, mois non reconnu : {month}\")\n",
    "                continue\n",
    "\n",
    "            return f\"{year}-{month_num}-{day}\"\n",
    "\n",
    "    print('Attention, date non gérée :', date_text)\n",
    "    # Retourner None si aucun format n'est reconnu\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kkopFTdzYDP"
   },
   "outputs": [],
   "source": [
    "def standardize_name(name):\n",
    "    words = name.split()\n",
    "    words.sort()\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKrisLVXzYDP"
   },
   "outputs": [],
   "source": [
    "def split_names(s):\n",
    "    words = s.split()\n",
    "    if len(words) == 4:\n",
    "        first_name = ' '.join(words[:2])\n",
    "        second_name = ' '.join(words[2:])\n",
    "        return [first_name, second_name]\n",
    "    elif len(words) == 6:\n",
    "        first_name = ' '.join(words[0:2])\n",
    "        second_name = ' '.join(words[2:4])\n",
    "        third_name = ' '.join(words[4:6])\n",
    "        return [first_name, second_name, third_name]\n",
    "    elif len(words) == 8:\n",
    "        first_name = ' '.join(words[0:2])\n",
    "        second_name = ' '.join(words[2:4])\n",
    "        third_name = ' '.join(words[4:6])\n",
    "        fourth_name = ' '.join(words[4:6])\n",
    "        return [first_name, second_name, third_name, fourth_name]\n",
    "\n",
    "    return [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK_v1A9Geni3"
   },
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "\n",
    "   # text = re.sub(r'[-–—‑‒−]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # écriture inclusive\n",
    "    text = text.replace('(e)', '')\n",
    "    text = text.replace('(E)', '')\n",
    "    text = text.replace('.e.', '')\n",
    "    text = text.replace('.E.', '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjo8OSp5zYDP"
   },
   "outputs": [],
   "source": [
    "def extract_names(line):\n",
    "    if len(line) > 150:\n",
    "        return None\n",
    "\n",
    "    # Supprimer tout ce qui est entre parenthèses\n",
    "    line = re.sub(r'\\(.*?\\)', '', line)\n",
    "\n",
    "    # Ignorer les lignes qui contiennent des domaines ou \"N/A\"\n",
    "    if re.search(r'(\\.fr|\\.com|n/a)', line):\n",
    "        return None\n",
    "\n",
    "    line = re.sub(r'\\s?@\\w+', '', line)\n",
    "    line = line.replace('.', '')\n",
    "    line = line.replace('\"', '')\n",
    "    line = line.replace('«', '')\n",
    "    line = line.replace('»', '')\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "\n",
    "    # Si la ligne contient \"////\", supprimez tout ce qui est à gauche et \"////\" lui-même\n",
    "    if \"////\" in line:\n",
    "        line = line.split(\"////\")[1].strip()\n",
    "\n",
    "    line = line.replace(',', ', ')\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "\n",
    "    # Si la ligne contient des virgules ou \"et\", divisez la ligne et prenez les noms\n",
    "\n",
    "    names = []\n",
    "    if len(line.split()) > 3:\n",
    "        parts = re.split(',| et', line)\n",
    "        for part in parts:\n",
    "            names.extend(split_names(part.strip()))\n",
    "    else:\n",
    "        line = line.replace(',', '')\n",
    "        names.extend(split_names(line.strip()))\n",
    "\n",
    "    return set(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMywMyATQahL"
   },
   "outputs": [],
   "source": [
    "def write_info_europresse(scores, article, actual_doc):\n",
    "    header = article.header\n",
    "\n",
    "    # Extraire les informations (adaptez en fonction de vos fonctions extract_information, etc.)\n",
    "    title_text = extract_information(header, '.titreArticle p')\n",
    "    journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "    date_text = extract_information(header, '.DocHeader')\n",
    "\n",
    "    journal_text = normalize_journal(journal_text)\n",
    "    date_text_clean = extract_date_info(date_text)\n",
    "    normalized_date = normalise_date(date_text_clean)\n",
    "\n",
    "    if normalized_date is not None:\n",
    "        date_normalized = normalized_date.replace(';', '').replace('&', '')\n",
    "    else:\n",
    "        date_normalized = date_text_clean\n",
    "\n",
    "    # Vérifier si le tableau scores n'est pas vide\n",
    "    if scores.size > 0:\n",
    "        max_topic_index = np.argmax(scores)\n",
    "    else:\n",
    "        max_topic_index = -1\n",
    "\n",
    "    # Calculer la clé pour récupérer le bon tableau de labels\n",
    "    config_key = len(scores)\n",
    "\n",
    "    # Récupérer le label correspondant au lieu de l'indice\n",
    "    if config_key in topic_labels_by_config and 0 <= max_topic_index < len(topic_labels_by_config[config_key]):\n",
    "        main_topic_label = topic_labels_by_config[config_key][max_topic_index]\n",
    "    else:\n",
    "        main_topic_label = \"Unknown topic\"\n",
    "\n",
    "    # Convertir chaque score en chaîne\n",
    "    scores_list = [str(score) for score in scores]\n",
    "\n",
    "    # Extraction des noms\n",
    "    names_raw = extract_information(header, '.sm-margin-bottomNews').lower()\n",
    "    names = extract_names(names_raw)\n",
    "    if names:\n",
    "        actual_names = [standardize_name(name) for name in names]\n",
    "        filtered_names = [\n",
    "            name for name in actual_names\n",
    "            if not any(\n",
    "                other_name != name\n",
    "                and set(name.split()) < set(other_name.split())\n",
    "                for other_name in actual_names\n",
    "            )\n",
    "        ]\n",
    "        all_names = filtered_names\n",
    "    else:\n",
    "        all_names = None\n",
    "\n",
    "    chaine_authors = \"None\" if all_names is None else ', '.join(map(str, all_names))\n",
    "\n",
    "    # Retourner une liste plutôt qu'une chaîne\n",
    "    # Remarquez qu'on place main_topic_label à la place de l'ancien max_topic_index\n",
    "    return [\n",
    "        title_text.replace(';', ''),\n",
    "        chaine_authors,\n",
    "        names_raw,\n",
    "        str(len(actual_doc)),\n",
    "        journal_text.replace(';', ''),\n",
    "        date_normalized,\n",
    "        main_topic_label   # Le label au lieu de l'indice\n",
    "    ] + scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMc5J9tR_24d"
   },
   "outputs": [],
   "source": [
    "def write_info_another(scores, columns_dict, i, actual_doc):\n",
    "    # Vérifier si le tableau scores n'est pas vide\n",
    "    if scores.size > 0:\n",
    "        max_topic_index = np.argmax(scores)  # indice de la valeur max\n",
    "    else:\n",
    "        max_topic_index = -1\n",
    "\n",
    "    # Préparer la liste des scores en chaînes de caractères\n",
    "    scores_list = [str(s) for s in scores]\n",
    "\n",
    "    # Construire la ligne sous forme de liste\n",
    "    row = []\n",
    "    for key in columns_dict:\n",
    "        row.append(str(columns_dict[key][i]))\n",
    "\n",
    "    # Ajouter nb_characters (en supposant que actual_doc est une chaîne)\n",
    "    row.append(str(len(actual_doc)))\n",
    "\n",
    "    # --- Récupérer le label à la place de l'indice ---\n",
    "    # La clé dans le dictionnaire : len(scores) + 1\n",
    "    config_key = len(scores)\n",
    "\n",
    "    if config_key in topic_labels_by_config and 0 <= max_topic_index < len(topic_labels_by_config[config_key]):\n",
    "        main_topic_label = topic_labels_by_config[config_key][max_topic_index]\n",
    "    else:\n",
    "        # Au cas où la clé ou l'indice n'existe pas dans le dictionnaire\n",
    "        main_topic_label = \"Unknown topic\"\n",
    "\n",
    "    # Ajouter le label du sujet principal au lieu de l'indice\n",
    "    row.append(main_topic_label)\n",
    "\n",
    "    # Ajouter les scores\n",
    "    row.extend(scores_list)\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP8r-C6EzYDP"
   },
   "outputs": [],
   "source": [
    "def remove_urls_hashtags_emojis_mentions_emails(text):\n",
    "    # Supprimer les URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "\n",
    "    # Supprimer les hashtags\n",
    "   # text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Supprimer les mentions\n",
    "  #  text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Supprimer les e-mails\n",
    " #   text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "\n",
    "    # Supprimer les émojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    " #   text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjTq-_ZEzYDQ"
   },
   "outputs": [],
   "source": [
    "def extract_info(topic_nums, article):\n",
    "    header = article.header\n",
    "\n",
    "    date_text = extract_information(header, '.DocHeader')\n",
    "    date_text_clean = extract_date_info(date_text)\n",
    "    if normalise_date(date_text_clean) != None:\n",
    "        date_normalized = normalise_date(date_text_clean).replace(';', '').replace('&', '')\n",
    "    else:\n",
    "        date_normalized = date_text_clean\n",
    "\n",
    "    topics_dict = dict(topic_nums)\n",
    "\n",
    "    return {date_normalized: topics_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vB-tYyThzYDT"
   },
   "outputs": [],
   "source": [
    "def aggregate_scores(articles_info):\n",
    "    aggregated_scores = {}\n",
    "\n",
    "    for info in articles_info:\n",
    "        for date, topics in info.items():\n",
    "            if date not in aggregated_scores:\n",
    "                aggregated_scores[date] = {}\n",
    "\n",
    "            for topic, score in topics.items():\n",
    "                if topic not in aggregated_scores[date]:\n",
    "                    aggregated_scores[date][topic] = 0\n",
    "                aggregated_scores[date][topic] += score\n",
    "\n",
    "    return aggregated_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvBhn9Z2zYDU"
   },
   "outputs": [],
   "source": [
    "def create_chrono_topics(sigma='auto', \n",
    "                         apply_vertical_normalization=False,\n",
    "                         apply_horizontal_normalization=False):\n",
    "    \"\"\"\n",
    "    Fonction qui agrège les scores de topics par date,\n",
    "    prépare le DataFrame final, puis appelle `plot_custom_heatmap`\n",
    "    pour le tracé (avec clustering, colorbar, ticks, etc.),\n",
    "    en s'assurant d'un alignement robuste entre topic_num et topic_label.\n",
    "    \"\"\"\n",
    "    # 1) Création du répertoire de sortie\n",
    "    if not os.path.exists(f\"{results_path}{base_name}_TOPICS_DYNAMICS_HEATMAPS/\"):\n",
    "        os.makedirs(f\"{results_path}{base_name}_TOPICS_DYNAMICS_HEATMAPS/\")\n",
    "\n",
    "    # 2) Pour chaque configuration (nombre de topics)\n",
    "    for num_topic in tqdm(all_nmf_W, desc=\"CONFIGURATIONS PROCESSÉES\"):\n",
    "        config_path = (\n",
    "            f\"{results_path}{base_name}_TOPICS_DYNAMICS_HEATMAPS\"\n",
    "        )\n",
    "        if not os.path.exists(config_path):\n",
    "            os.makedirs(config_path)\n",
    "\n",
    "        # --- 2.1) Extraction des infos articles selon la source ---\n",
    "        if source_type == 'europresse':\n",
    "            # On construit une liste d'objets \"articles_info\"\n",
    "            articles_info = [\n",
    "                extract_info(\n",
    "                    # On crée un dictionnaire { \"0\": score_doc_i_topic_0, \"1\": score_doc_i_topic_1, ... }\n",
    "                    {str(topic_num): all_nmf_W[num_topic][i, topic_num] for topic_num in range(all_nmf_W[num_topic].shape[1])},\n",
    "                    article\n",
    "                )\n",
    "                for i, article in enumerate(all_soups)\n",
    "            ]\n",
    "        elif source_type in ['csv', 'istex']:\n",
    "            articles_info = []\n",
    "            all_dates = formater_liste_dates(columns_dict['date'])\n",
    "            i = 0\n",
    "            while i < len(all_dates):\n",
    "                score_dict = {\n",
    "                    str(topic_num): all_nmf_W[num_topic][i, topic_num]\n",
    "                    for topic_num in range(all_nmf_W[num_topic].shape[1])\n",
    "                }\n",
    "                articles_info.append({all_dates[i]: score_dict})\n",
    "                i += 1\n",
    "\n",
    "        # --- 2.2) Agrégation par date ---\n",
    "        #     aggregate_scores() doit renvoyer un dict du type :\n",
    "        #     {\n",
    "        #        \"12/05/2020\": {\"0\": 0.43, \"1\": 0.12, ...},\n",
    "        #        \"13/05/2020\": {\"0\": 0.22, \"1\": 0.61, ...},\n",
    "        #         ...\n",
    "        #     }\n",
    "        aggregated_scores = aggregate_scores(articles_info)\n",
    "\n",
    "        renamed_aggregated_scores = {\n",
    "            date: {\n",
    "                topic_labels_by_config[num_topic][int(topic_str)]: score\n",
    "                for topic_str, score in scores_dict.items()\n",
    "            }\n",
    "            for date, scores_dict in aggregated_scores.items()\n",
    "        }\n",
    "\n",
    "        aggregated_scores = renamed_aggregated_scores\n",
    "\n",
    "        # --- 2.3) Calcul du nombre de documents par date ---\n",
    "        doc_counts_by_date = defaultdict(int)\n",
    "        for elt in articles_info:\n",
    "            if isinstance(elt, dict):\n",
    "                for date_str in elt.keys():\n",
    "                    doc_counts_by_date[date_str] += 1\n",
    "\n",
    "        # --- 2.4) Filtrage des dates valides & conversion ---\n",
    "        valid_dates = {}\n",
    "        for date_str, score_dict in aggregated_scores.items():\n",
    "            date_obj = extract_and_convert_date(date_str)\n",
    "            if date_obj:\n",
    "                # Format unifié \"jj/mm/YYYY\"\n",
    "                valid_dates[date_obj.strftime('%d/%m/%Y')] = score_dict\n",
    "\n",
    "        # --- 2.5) Tri chronologique de ces dates ---\n",
    "        aggregated_scores_sorted = {\n",
    "            date: valid_dates[date]\n",
    "            for date in sorted(valid_dates, key=lambda d: datetime.strptime(d, '%d/%m/%Y'))\n",
    "        }\n",
    "\n",
    "        # -----------------------------------------------------------------------\n",
    "        # --- 3) Construction d'un DataFrame \"long\" (date, topic_num, score) ---\n",
    "        # -----------------------------------------------------------------------\n",
    "        #  L'objectif : obtenir une table de la forme :\n",
    "        #\n",
    "        #       date         topic_num   score\n",
    "        #     \"12/05/2020\"       0       0.43\n",
    "        #     \"12/05/2020\"       1       0.12\n",
    "        #     \"13/05/2020\"       0       0.22\n",
    "        #     ...\n",
    "        #\n",
    "        #  afin de pouvoir ensuite fusionner sur un DataFrame de mapping\n",
    "        #  (topic_num -> topic_label) et enfin faire un pivot.\n",
    "        # -----------------------------------------------------------------------\n",
    "\n",
    "        rows = []\n",
    "        for date_str, topics_dict in aggregated_scores_sorted.items():\n",
    "            for topic_num_str, score in topics_dict.items():\n",
    "                rows.append({\n",
    "                    \"Date\": date_str,\n",
    "                    \"Topic\": topic_num_str,\n",
    "                    \"Score\": score\n",
    "                })\n",
    "\n",
    "        df_long = pd.DataFrame(rows)  # \"long format\"\n",
    "\n",
    "        # --- 3.3) Pivot sur le label de topic (index) et la date (columns) ---\n",
    "        #   Les valeurs sont \"Score\"\n",
    "        df_pivoted = df_long.pivot(\n",
    "            index=\"Topic\",\n",
    "            columns=\"Date\",\n",
    "            values=\"Score\"\n",
    "        ).fillna(0)  # on remplit à 0 les absences\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # --- 4) Optionnel : division par le nombre de documents (normalization)\n",
    "        # ---------------------------------------------------------------------\n",
    "        if apply_vertical_normalization:\n",
    "            for col in df_pivoted.columns:\n",
    "                nb_docs = doc_counts_by_date.get(col, 1)  # éviter la division par 0\n",
    "                df_pivoted[col] = df_pivoted[col] / nb_docs\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # --- 5) Conversion des colonnes en datetime & tri chronologique\n",
    "        # ---------------------------------------------------------------------\n",
    "        df_pivoted.columns = pd.to_datetime(df_pivoted.columns, format='%d/%m/%Y', errors='coerce')\n",
    "        # On enlève d'éventuels NaT si conversion ratée (ou on pourrait ignorer)\n",
    "        df_pivoted = df_pivoted.loc[:, df_pivoted.columns.notna()]\n",
    "\n",
    "        # Tri des colonnes par ordre chronologique\n",
    "        df_pivoted = df_pivoted.reindex(sorted(df_pivoted.columns), axis=1)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # --- 6) Réindexation sur la plage complète de dates\n",
    "        #         (de la plus ancienne à la plus récente)\n",
    "        # ---------------------------------------------------------------------\n",
    "        if not df_pivoted.columns.empty:\n",
    "            oldest_date = df_pivoted.columns.min()\n",
    "            newest_date = df_pivoted.columns.max()\n",
    "            date_range = pd.date_range(start=oldest_date, end=newest_date)\n",
    "\n",
    "            # On réindexe, et on remplit à 0 pour les dates manquantes\n",
    "            df_pivoted = df_pivoted.reindex(columns=date_range, fill_value=0)\n",
    "        else:\n",
    "            # S'il n'y a pas de colonne (cas extrême), on ne fait rien\n",
    "            date_range = []\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # --- 7) Préparation pour la heatmap\n",
    "        # ---------------------------------------------------------------------\n",
    "        # df_pivoted est un DataFrame (topics_label en lignes, dates en colonnes)\n",
    "        df_normalized = df_pivoted  # pour garder le même nom que l'ancien code\n",
    "\n",
    "        # Si sigma = 'auto', on le définit maintenant (pour plot_custom_heatmap)\n",
    "        if sigma == 'auto':\n",
    "            sigma = len(df_normalized.columns) / 15 if len(df_normalized.columns) > 0 else 1\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # --- 8) Appel de plot_custom_heatmap ---\n",
    "        # ---------------------------------------------------------------------\n",
    "        plot_custom_heatmap(\n",
    "            df=df_normalized,\n",
    "            sigma=sigma,\n",
    "            cmap=\"YlGnBu\",            # palette\n",
    "            apply_horizontal_normalization=apply_horizontal_normalization,  # normalisation interne\n",
    "            with_colormap=False\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # --- 9) Sauvegarde de la figure ---\n",
    "        # ---------------------------------------------------------------------\n",
    "        class_suffix = \"_\".join(grammatical_classes)\n",
    "        plt.savefig(\n",
    "            f\"{results_path}{base_name}_TOPICS_DYNAMICS_HEATMAPS/\"\n",
    "            f\"{base_name}_topics_dynamics_heatmap_{num_topic}tc_{apply_vertical_normalization}vn_{apply_horizontal_normalization}hn_{('auto' if sigma=='auto' else int(sigma))}s\"\n",
    "            f\"_{class_suffix}_\"\n",
    "            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "            f\"{go_remove_duplicates}dup.png\",\n",
    "            dpi=DPI,\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-emWCg7JUfTc"
   },
   "outputs": [],
   "source": [
    "def correct_dates(dictionary):\n",
    "    corrected_dict = {}\n",
    "    for date, count in dictionary.items():\n",
    "        if len(date) < 10:\n",
    "            # Ajouter '0' au début de la date\n",
    "            corrected_date = '0' + date\n",
    "        else:\n",
    "            corrected_date = date\n",
    "        corrected_dict[corrected_date] = count\n",
    "    return corrected_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiMYxaJsiVsF"
   },
   "outputs": [],
   "source": [
    "def create_chrono_group_column(group_column, sigma, apply_vertical_normalization=False, apply_horizontal_normalization=False):\n",
    "    \"\"\"\n",
    "    Prépare les données par groupe/colonne (journal ou autre) au fil du temps,\n",
    "    puis crée la heatmap temporelle en utilisant `plot_custom_heatmap`,\n",
    "    en s'appuyant sur la stratégie \"robuste\" (format long + pivot).\n",
    "    \"\"\"\n",
    "    # 1) Création du répertoire de sortie\n",
    "    output_dir = f\"{results_path}{base_name}_GROUPS_DYNAMICS_HEATMAPS/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2) Agrégation des comptes (scores) en fonction de la source\n",
    "    #    => On va créer un dict: date -> {groupe: count}\n",
    "    # ---------------------------------------------------------\n",
    "    aggregated_scores = {}\n",
    "\n",
    "    if source_type == 'europresse':\n",
    "        for article in all_soups:\n",
    "            header = article.header\n",
    "            journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "            date_text = extract_information(header, '.DocHeader')\n",
    "\n",
    "            journal_text = normalize_journal(journal_text)\n",
    "            date_text_clean = extract_date_info(date_text)\n",
    "\n",
    "            date_normalized = normalise_date(date_text_clean)\n",
    "            if date_normalized is None:\n",
    "                # Si on n'a pas pu normaliser, on prend la valeur brute\n",
    "                date_normalized = date_text_clean\n",
    "            else:\n",
    "                # Nettoyage de base\n",
    "                date_normalized = date_normalized.replace(';', '').replace('&', '')\n",
    "\n",
    "            if date_normalized not in aggregated_scores:\n",
    "                aggregated_scores[date_normalized] = {}\n",
    "            aggregated_scores[date_normalized].setdefault(journal_text, 0)\n",
    "            aggregated_scores[date_normalized][journal_text] += 1\n",
    "\n",
    "    elif source_type == 'istex':\n",
    "        for i in range(len(columns_dict['date'])):\n",
    "            date_raw = columns_dict['date'][i]\n",
    "            journal = columns_dict['journal'][i]\n",
    "\n",
    "            # Nettoyage basique\n",
    "            date_normalized = date_raw.replace(';', '').replace('&', '')\n",
    "\n",
    "            if date_normalized not in aggregated_scores:\n",
    "                aggregated_scores[date_normalized] = {}\n",
    "            aggregated_scores[date_normalized].setdefault(journal, 0)\n",
    "            aggregated_scores[date_normalized][journal] += 1\n",
    "\n",
    "    elif source_type == 'csv':\n",
    "        # Vérification de la disponibilité des colonnes requises\n",
    "        if 'date' not in columns_dict:\n",
    "            print(\"La colonne 'date' n'existe pas dans columns_dict. Elle est requise.\")\n",
    "            return\n",
    "\n",
    "        if group_column not in columns_dict:\n",
    "            print(f\"La colonne '{group_column}' n'existe pas dans columns_dict. Vérifiez les colonnes disponibles.\")\n",
    "            return\n",
    "\n",
    "        for i in range(len(columns_dict['date'])):\n",
    "            date_raw = columns_dict['date'][i]\n",
    "            group_value = columns_dict[group_column][i]\n",
    "\n",
    "            # Nettoyage\n",
    "            date_normalized = date_raw.replace(';', '').replace('&', '')\n",
    "\n",
    "            if date_normalized not in aggregated_scores:\n",
    "                aggregated_scores[date_normalized] = {}\n",
    "            aggregated_scores[date_normalized].setdefault(group_value, 0)\n",
    "            aggregated_scores[date_normalized][group_value] += 1\n",
    "\n",
    "    else:\n",
    "        # Source non gérée\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3) Conversion des dates & tri chronologique\n",
    "    #    => format \"jj/mm/YYYY\" pour éviter les ambiguïtés\n",
    "    # ---------------------------------------------------------\n",
    "    valid_dates = {}\n",
    "    for date_str, score_dict in aggregated_scores.items():\n",
    "        date_obj = extract_and_convert_date(date_str)\n",
    "        if date_obj:\n",
    "            valid_dates[date_obj.strftime('%d/%m/%Y')] = score_dict\n",
    "\n",
    "    # Tri chronologique des dates\n",
    "    sorted_dates = sorted(valid_dates, key=lambda d: datetime.strptime(d, '%d/%m/%Y'))\n",
    "    aggregated_scores_sorted = {\n",
    "        date: valid_dates[date]\n",
    "        for date in sorted_dates\n",
    "    }\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4) Calcul du nombre total d'articles par date (pour normaliser si besoin)\n",
    "    # ---------------------------------------------------------\n",
    "    aggregated_article_counts = {}\n",
    "    for date, group_counts in aggregated_scores_sorted.items():\n",
    "        aggregated_article_counts[date] = sum(group_counts.values())\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5) Passage en \"long format\"\n",
    "    #    => on obtient des lignes : [Date, Group, Count]\n",
    "    # ---------------------------------------------------------\n",
    "    rows = []\n",
    "    for date, group_dict in aggregated_scores_sorted.items():\n",
    "        for group_value, count in group_dict.items():\n",
    "            rows.append({\n",
    "                \"Date\": date,\n",
    "                \"Group\": group_value,\n",
    "                \"Count\": count\n",
    "            })\n",
    "\n",
    "    df_long = pd.DataFrame(rows, columns=[\"Date\", \"Group\", \"Count\"])\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 6) Pivot => index = Group, columns = Date, values = Count\n",
    "    # ---------------------------------------------------------\n",
    "    df_pivoted = df_long.pivot(\n",
    "        index='Group',\n",
    "        columns='Date',\n",
    "        values='Count'\n",
    "    ).fillna(0)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 7) Application d’un threshold sur les lignes (si besoin)\n",
    "    #    => on ne garde que les groupes dont la somme >= threshold\n",
    "    # ---------------------------------------------------------\n",
    "    df_pivoted = df_pivoted[df_pivoted.sum(axis=1) >= threshold]\n",
    "\n",
    "    # Copie pour d’éventuelles normalisations\n",
    "    df_normalized = df_pivoted.copy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 8) Division par nb_docs si apply_normalizations == True\n",
    "    # ---------------------------------------------------------\n",
    "    if len(df_normalized) > 1:\n",
    "        if apply_vertical_normalization:\n",
    "            for col in df_normalized.columns:\n",
    "                nb_docs = aggregated_article_counts.get(col, 1)  # éviter division par 0\n",
    "                df_normalized[col] = df_normalized[col] / nb_docs\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 9) Conversion des colonnes en datetime et tri chrono\n",
    "    # ---------------------------------------------------------\n",
    "    df_normalized.columns = pd.to_datetime(df_normalized.columns, format='%d/%m/%Y', errors='coerce')\n",
    "    # On enlève d’éventuelles colonnes non converties\n",
    "    df_normalized = df_normalized.loc[:, df_normalized.columns.notnull()]\n",
    "\n",
    "    # Tri des dates chronologiquement\n",
    "    df_normalized = df_normalized.reindex(sorted(df_normalized.columns), axis=1)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 10) Réindexer pour inclure TOUTES les dates manquantes\n",
    "    #     => on remplit par 0\n",
    "    # ---------------------------------------------------------\n",
    "    if not df_normalized.columns.empty:\n",
    "        oldest_date = df_normalized.columns.min()\n",
    "        newest_date = df_normalized.columns.max()\n",
    "        date_range = pd.date_range(start=oldest_date, end=newest_date)\n",
    "        df_normalized = df_normalized.reindex(columns=date_range, fill_value=0)\n",
    "    else:\n",
    "        # Pas de dates valides, on ne fait rien\n",
    "        date_range = []\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 11) (Optionnel) Filtrage gaussien ici\n",
    "    #     => si vous préférez laisser plot_custom_heatmap s'en charger, commentez.\n",
    "    # ---------------------------------------------------------\n",
    "    # ...\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 12) (Optionnel) MinMaxScaling par ligne\n",
    "    #     => idem, si vous le faites ici, ne le refaites pas dans plot_custom_heatmap.\n",
    "    # ---------------------------------------------------------\n",
    "    # ...\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 13) Ajustement sigma si \"auto\"\n",
    "    # ---------------------------------------------------------\n",
    "    if sigma == 'auto':\n",
    "        nb_cols = len(df_normalized.columns)\n",
    "        sigma = nb_cols / 15.0 if nb_cols > 0 else 1\n",
    "\n",
    "    if len(df_normalized) == 1 and apply_horizontal_normalization:\n",
    "        return\n",
    "    else:\n",
    "        plot_custom_heatmap(\n",
    "            df=df_normalized,\n",
    "            sigma=sigma,\n",
    "            cmap=\"YlGnBu\",\n",
    "            apply_horizontal_normalization=apply_horizontal_normalization,\n",
    "            with_colormap=False\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 15) Sauvegarde de la figure\n",
    "        # ---------------------------------------------------------\n",
    "        plt.savefig(\n",
    "            f\"{output_dir}{base_name}_groups_dynamics_heatmap_{apply_vertical_normalization}vn_{apply_horizontal_normalization}hn_{('auto' if sigma=='auto' else int(sigma))}s_\"\n",
    "            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_\"\n",
    "            f\"{threshold}thr.png\",\n",
    "            dpi=DPI,\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBSjjwG90nW2"
   },
   "outputs": [],
   "source": [
    "def create_results_folder(base_name):\n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "\n",
    "    name_document = f'{base_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pd5bYgQKqsZU"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Fonction principale pour charger les documents\n",
    "# =============================================================================\n",
    "def load_documents(name, source_type, minimum_caracters_nb_by_document, pbar):\n",
    "    \"\"\"\n",
    "    Récupère et nettoie les documents à partir d'un fichier ou dossier donné,\n",
    "    en fonction du 'source_type' (europresse, csv, istex).\n",
    "    Retourne :\n",
    "      - documents : liste de textes\n",
    "      - all_soups : liste de BeautifulSoup (pour europresse, sinon vide)\n",
    "      - columns_dict : dictionnaire contenant d'autres colonnes (pour CSV, ISTEX...)\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    all_soups = []\n",
    "    columns_dict = {}\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # CAS 1 : Fichiers issus de Europresse\n",
    "    # -------------------------------------------------------------------------\n",
    "    if source_type == 'europresse':\n",
    "        document_europresse = ''\n",
    "\n",
    "        # Lecture du fichier HTML brut\n",
    "        with open(name, 'r', encoding='utf-8', errors='xmlcharrefreplace') as file:\n",
    "            for line in file:\n",
    "                document_europresse += line\n",
    "\n",
    "        # Décode les entités HTML et répare la séparation entre articles\n",
    "        document_europresse = html.unescape(document_europresse)\n",
    "        document_europresse = document_europresse.replace('</article> <article>', '</article><article>')\n",
    "        documents_europresse = document_europresse.split('</article><article>')\n",
    "\n",
    "        nb_not_occur = 0\n",
    "        for d in documents_europresse:\n",
    "            soup = BeautifulSoup(d, features=\"html.parser\")\n",
    "\n",
    "            # On met à jour la barre de progression pour chaque article\n",
    "            pbar.update(1)\n",
    "\n",
    "            # On retire les paragraphes \"Lire aussi ...\" qui ne contiennent pas assez de texte\n",
    "            for p in soup.find_all('p'):\n",
    "                p_text = p.get_text()\n",
    "                if (\"Lire aussi\" in p_text and (\"http\" in p_text or \"https\" in p_text) and len(p_text) <= 1000):\n",
    "                    p.decompose()\n",
    "\n",
    "            # Si on trouve la div \"docOcurrContainer\", c'est là qu'est le texte\n",
    "            if len(soup('div', {'class': 'docOcurrContainer'})) > 0:\n",
    "                # Corrige des fins de paragraphes manquantes (ponctuation)\n",
    "                for p in soup.find_all('p'):\n",
    "                    # Trouver le prochain caractère alphabétique après ce paragraphe\n",
    "                    next_char_match = re.search(\n",
    "                        r'(?<=' + re.escape(p.text) + r')\\s*(?:<[^>]*>)*\\s*([a-zA-Z])',\n",
    "                        str(soup)\n",
    "                    )\n",
    "                    # Ajoute un point si le paragraphe ne se termine pas par '.'\n",
    "                    # et que le prochain char est une majuscule\n",
    "                    if not p.text.endswith('.') and next_char_match and next_char_match.group(1).isupper():\n",
    "                        p.string = p.text + '. '\n",
    "\n",
    "                # On recrée la soupe après modifications\n",
    "                soup = BeautifulSoup(str(soup), features='html.parser')\n",
    "\n",
    "                candidate_text = soup('div', {'class': 'docOcurrContainer'})[0].get_text()\n",
    "                if (minimum_caracters_nb_by_document <= len(candidate_text) < maximum_caracters_nb_by_document):\n",
    "                    candidate_text = remove_urls_hashtags_emojis_mentions_emails(candidate_text)\n",
    "                    candidate_text = transform_text(candidate_text)\n",
    "                    documents.append(candidate_text)\n",
    "                    all_soups.append(soup)\n",
    "            else:\n",
    "                nb_not_occur += 1\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # CAS 2 : Fichier CSV\n",
    "    # -------------------------------------------------------------------------\n",
    "    elif source_type == 'csv':\n",
    "        # 1. Vérifier la taille du fichier\n",
    "        file_size_bytes = os.path.getsize(name)\n",
    "        file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "        print(f\"[*] Taille du fichier CSV = {file_size_mb:.2f} Mo\")\n",
    "\n",
    "        if file_size_mb > 100:\n",
    "            # 2. Si le fichier dépasse 200 Mo, on lit directement en UTF-8, sep=';'\n",
    "            print(\"[*] Le fichier est > 200 Mo : lecture directe en UTF-8 (séparateur ';')\")\n",
    "            try:\n",
    "                df = pd.read_csv(name, encoding='utf-8', sep=';', on_bad_lines='skip', low_memory=False)\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Erreur lors de la lecture du fichier >200Mo en UTF-8/';' : {e}\")\n",
    "                return ([], [], {})  # Retourne des listes/dict vides\n",
    "\n",
    "        else:\n",
    "            # 3. Fichier <= 200 Mo : détection d'encodage via charset-normalizer\n",
    "            print(\"[*] Le fichier est <= 200 Mo : on effectue la détection d'encodage\")\n",
    "            results = from_path(name)\n",
    "            best_guess = results.best()\n",
    "\n",
    "            if best_guess is not None:\n",
    "                detected_encoding = best_guess.encoding\n",
    "                raw_data = best_guess.raw\n",
    "            else:\n",
    "                detected_encoding = None\n",
    "                with open(name, 'rb') as f:\n",
    "                    raw_data = f.read()\n",
    "\n",
    "            # On se limite à utf-8 + encodage système éventuel\n",
    "            common_encodings = ['utf-8']\n",
    "            system_encoding = locale.getpreferredencoding(False)\n",
    "            if system_encoding and system_encoding.lower() not in [enc.lower() for enc in common_encodings]:\n",
    "                common_encodings.append(system_encoding)\n",
    "\n",
    "            # Si charset-normalizer propose un encodage non dans la liste, on l'insère en priorité\n",
    "            if detected_encoding and detected_encoding.lower() not in [enc.lower() for enc in common_encodings]:\n",
    "                common_encodings.insert(0, detected_encoding)\n",
    "\n",
    "            best_score = None\n",
    "            best_df = None\n",
    "            best_sep = None\n",
    "\n",
    "            # On tente les encodages présents dans 'common_encodings'\n",
    "            for enc in common_encodings:\n",
    "                try:\n",
    "                    # Décodage en mémoire (bytes -> str)\n",
    "                    content_str = raw_data.decode(enc, errors='replace')\n",
    "                    file_like = io.StringIO(content_str)\n",
    "\n",
    "                    # Détection du séparateur via la 1ère ligne\n",
    "                    first_line = file_like.readline().strip()\n",
    "                    file_like.seek(0)\n",
    "                    if ',' in first_line:\n",
    "                        sep = ','\n",
    "                    elif ';' in first_line:\n",
    "                        sep = ';'\n",
    "                    else:\n",
    "                        sep = None\n",
    "\n",
    "                    file_like.seek(0)\n",
    "                    try:\n",
    "                        # Lecture du CSV multi-colonnes\n",
    "                        df_test = pd.read_csv(file_like, header=0, sep=sep, on_bad_lines='skip', low_memory=False)\n",
    "                    except Exception:\n",
    "                        # Si échec, on retente en \"mono-colonne\"\n",
    "                        file_like.seek(0)\n",
    "                        header = file_like.readline().strip()\n",
    "                        content = [line.strip() for line in file_like]\n",
    "                        df_test = pd.DataFrame(content, columns=[header])\n",
    "\n",
    "                    # Compter les caractères invalides\n",
    "                    invalid_chars = df_test.to_string().count('�')\n",
    "                    if best_score is None or invalid_chars < best_score:\n",
    "                        best_score = invalid_chars\n",
    "                        best_df = df_test\n",
    "                        best_sep = sep\n",
    "\n",
    "                except Exception:\n",
    "                    # Si l'encodage échoue, on ignore\n",
    "                    continue\n",
    "\n",
    "            # À la fin, best_df est le DataFrame \"le moins corrompu\"\n",
    "            df = best_df\n",
    "            if df is None:\n",
    "                print(\"[!] Impossible de lire le CSV avec les encodages testés.\")\n",
    "                return ([], [], {})\n",
    "\n",
    "            # Conversion en minuscules et renommage éventuel\n",
    "            df.columns = df.columns.str.lower()\n",
    "            df = df.rename(columns={'post created date': 'date'})\n",
    "            df.fillna('', inplace=True)\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # À ce stade, on possède un DataFrame 'df', soit gros CSV (lecture directe),\n",
    "        # soit petit CSV (<=200 Mo) après détection d'encodage.\n",
    "        # Le code qui suit (choix de la colonne, filtrage, etc.) reste inchangé :\n",
    "        # -------------------------------------------------------------------------\n",
    "\n",
    "        # Choix de la colonne texte\n",
    "        if 'text' in df.columns:\n",
    "            column_to_use = 'text'\n",
    "        elif 'description' in df.columns:\n",
    "            column_to_use = 'description'\n",
    "        else:\n",
    "            print(\"Les colonnes 'text' ou 'description' ne sont pas présentes dans le DataFrame.\")\n",
    "            return ([], [], {})  # Retourne des listes/dict vides\n",
    "\n",
    "        # Filtrage par longueur\n",
    "        df = df.loc[\n",
    "            (df[column_to_use].str.len() >= minimum_caracters_nb_by_document) &\n",
    "            (df[column_to_use].str.len() <= maximum_caracters_nb_by_document)\n",
    "        ]\n",
    "\n",
    "        # Nettoyage de chaque document\n",
    "        documents = df[column_to_use].tolist()\n",
    "        for i in range(len(documents)):\n",
    "            documents[i] = remove_urls_hashtags_emojis_mentions_emails(documents[i])\n",
    "            documents[i] = transform_text(documents[i])\n",
    "\n",
    "        # Stockage des autres colonnes\n",
    "        for column in df.columns:\n",
    "            if column not in ['text', 'description']:\n",
    "                columns_dict[column] = df[column].tolist()\n",
    "\n",
    "        # À ce stade, on a un DataFrame 'df' lisible\n",
    "        # -------------------------------------------------------------------------\n",
    "        # Choix de la colonne texte\n",
    "        if 'text' in df.columns:\n",
    "            column_to_use = 'text'\n",
    "        elif 'description' in df.columns:\n",
    "            column_to_use = 'description'\n",
    "        else:\n",
    "            print(\"Les colonnes 'text' ou 'description' ne sont pas présentes dans le DataFrame.\")\n",
    "            return ([], [], {})  # Retourne des listes/dict vides\n",
    "\n",
    "        # Filtrage par longueur\n",
    "        df = df.loc[\n",
    "            (df[column_to_use].str.len() >= minimum_caracters_nb_by_document) &\n",
    "            (df[column_to_use].str.len() <= maximum_caracters_nb_by_document)\n",
    "        ]\n",
    "\n",
    "        # Nettoyage de chaque document\n",
    "        documents = df[column_to_use].tolist()\n",
    "        for i in range(len(documents)):\n",
    "            documents[i] = remove_urls_hashtags_emojis_mentions_emails(documents[i])\n",
    "            documents[i] = transform_text(documents[i])\n",
    "\n",
    "        # Stockage des autres colonnes\n",
    "        for column in df.columns:\n",
    "            if column not in ['text', 'description']:\n",
    "                columns_dict[column] = df[column].tolist()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # CAS 3 : ISTEX\n",
    "    # -------------------------------------------------------------------------\n",
    "    elif source_type == 'istex':\n",
    "        def get_nested(data, keys):\n",
    "            \"\"\"Fonction utilitaire pour extraire des données imbriquées dans un dictionnaire.\"\"\"\n",
    "            for key in keys:\n",
    "                if isinstance(data, dict):\n",
    "                    data = data.get(key)\n",
    "                else:\n",
    "                    return None\n",
    "            return data\n",
    "\n",
    "        documents = []\n",
    "        columns_dict = {}\n",
    "\n",
    "        # Champs ISTEX à extraire\n",
    "        fields_to_extract = [\n",
    "            'date', 'title', 'doi', 'journal', 'language', 'originalGenre',\n",
    "            'accessCondition', 'pdfVersion', 'abstractCharCount', 'pdfPageCount',\n",
    "            'pdfWordCount', 'score', 'pdfText', 'imageCount', 'refCount',\n",
    "            'sectionCount', 'paragraphCount', 'tableCount', 'categories_scopus',\n",
    "            'categories_scienceMetrix', 'host_volume', 'host_issue',\n",
    "            'host_publisher', 'host_pages_first', 'host_pages_last', 'host_title',\n",
    "            'refBibs_count',\n",
    "        ]\n",
    "\n",
    "        field_mappings = {\n",
    "            'date': ['publicationDate'],\n",
    "            'title': ['title'],\n",
    "            'doi': ['doi'],\n",
    "            'journal': ['host', 'title'],\n",
    "            'language': ['language'],\n",
    "            'originalGenre': ['originalGenre'],\n",
    "            'accessCondition': ['accessCondition', 'value'],\n",
    "            'pdfVersion': ['qualityIndicators', 'pdfVersion'],\n",
    "            'abstractCharCount': ['qualityIndicators', 'abstractCharCount'],\n",
    "            'pdfPageCount': ['qualityIndicators', 'pdfPageCount'],\n",
    "            'pdfWordCount': ['qualityIndicators', 'pdfWordCount'],\n",
    "            'score': ['qualityIndicators', 'score'],\n",
    "            'pdfText': ['qualityIndicators', 'pdfText'],\n",
    "            'imageCount': ['qualityIndicators', 'xmlStats', 'imageCount'],\n",
    "            'refCount': ['qualityIndicators', 'xmlStats', 'refCount'],\n",
    "            'sectionCount': ['qualityIndicators', 'xmlStats', 'sectionCount'],\n",
    "            'paragraphCount': ['qualityIndicators', 'xmlStats', 'paragraphCount'],\n",
    "            'tableCount': ['qualityIndicators', 'xmlStats', 'tableCount'],\n",
    "            'categories_scopus': ['categories', 'scopus'],\n",
    "            'categories_scienceMetrix': ['categories', 'scienceMetrix'],\n",
    "            'host_volume': ['host', 'volume'],\n",
    "            'host_issue': ['host', 'issue'],\n",
    "            'host_publisher': ['host', 'publisher'],\n",
    "            'host_pages_first': ['host', 'pages', 'first'],\n",
    "            'host_pages_last': ['host', 'pages', 'last'],\n",
    "            'host_title': ['host', 'title'],\n",
    "            'refBibs_count': ['refBibs'],\n",
    "        }\n",
    "\n",
    "        # Initialiser columns_dict avec listes vides\n",
    "        for field in fields_to_extract:\n",
    "            columns_dict[field] = []\n",
    "\n",
    "        \n",
    "        if os.path.isdir(name):\n",
    "            with contextlib.redirect_stdout(None):\n",
    "                files_in_dir = os.listdir(name)\n",
    "                txt_files = [f for f in files_in_dir if f.endswith('.txt')]\n",
    "                json_files = [f for f in files_in_dir if f.endswith('.json')]\n",
    "\n",
    "                # On cherche les basenames communs\n",
    "                txt_basenames = set(os.path.splitext(f)[0] for f in txt_files)\n",
    "                json_basenames = set(os.path.splitext(f)[0] for f in json_files)\n",
    "                common_basenames = txt_basenames.intersection(json_basenames)\n",
    "\n",
    "                if not common_basenames:\n",
    "                    print(f\"Le répertoire '{name}' est ignoré (pas de fichiers .txt et .json correspondants).\")\n",
    "                else:\n",
    "                    for basename in common_basenames:\n",
    "                        txt_file_path = os.path.join(name, basename + '.txt')\n",
    "                        try:\n",
    "                            with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "                                txt_content = f.read()\n",
    "                        except Exception as e:\n",
    "                            print(f\"Erreur lors de la lecture du fichier texte '{txt_file_path}': {e}\")\n",
    "                            continue\n",
    "\n",
    "                        # Filtrage de longueur\n",
    "                        if (minimum_caracters_nb_by_document <= len(txt_content) <= maximum_caracters_nb_by_document):\n",
    "                            txt_content = remove_urls_hashtags_emojis_mentions_emails(txt_content)\n",
    "                            txt_content = transform_text(txt_content)\n",
    "                            documents.append(txt_content)\n",
    "\n",
    "                            # Lecture du JSON\n",
    "                            json_file_path = os.path.join(name, basename + '.json')\n",
    "                            try:\n",
    "                                with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                                    json_data = json.load(f)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Erreur lors de la lecture du fichier JSON '{json_file_path}': {e}\")\n",
    "                                # On met des None pour chaque champ\n",
    "                                for field in fields_to_extract:\n",
    "                                    columns_dict[field].append(None)\n",
    "                                continue\n",
    "\n",
    "                            # Extraction champs\n",
    "                            for field in fields_to_extract:\n",
    "                                json_keys = field_mappings.get(field)\n",
    "                                value = None\n",
    "                                if json_keys is not None:\n",
    "                                    if field == 'refBibs_count':\n",
    "                                        # Nombre de références bibliographiques\n",
    "                                        refbibs = get_nested(json_data, json_keys)\n",
    "                                        value = len(refbibs) if refbibs is not None else 0\n",
    "                                    else:\n",
    "                                        value = get_nested(json_data, json_keys)\n",
    "                                        # Si c'est une liste, on joint par virgule\n",
    "                                        if isinstance(value, list):\n",
    "                                            value = ', '.join(map(str, value))\n",
    "                                columns_dict[field].append(value)\n",
    "                        else:\n",
    "                            # Document trop court ou trop long\n",
    "                            continue\n",
    "\n",
    "                    # Vérification des longueurs\n",
    "                    length_documents = len(documents)\n",
    "                    for field in columns_dict:\n",
    "                        assert len(columns_dict[field]) == length_documents, \\\n",
    "                            f\"Incohérence pour le champ '{field}'\"\n",
    "            # On modifie la date si besoin\n",
    "            columns_dict[\"date\"] = [\"01/01/\" + date for date in columns_dict[\"date\"]]\n",
    "        else:\n",
    "            print(f\"Le chemin '{name}' n'est pas un répertoire valide.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    return documents, all_soups, columns_dict\n",
    "\n",
    "# =============================================================================\n",
    "# Fonction pour rassembler les documents de manière simplifiée\n",
    "# =============================================================================\n",
    "def meta_load_documents():\n",
    "    \"\"\"\n",
    "    Identifie tous les fichiers/dossiers sources en fonction de 'source_type',\n",
    "    puis appelle directement load_documents sur chacun d'entre eux.\n",
    "    Fusionne le tout dans les variables globales :\n",
    "      - documents\n",
    "      - all_soups\n",
    "      - columns_dict\n",
    "    Gère la suppression de doublons si go_remove_duplicates == True.\n",
    "    \"\"\"\n",
    "    global documents\n",
    "    global all_soups\n",
    "    global columns_dict\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Construction de la liste des fichiers à traiter\n",
    "    # -------------------------------------------------------------------------\n",
    "    if source_type == 'europresse':\n",
    "        # Tous les .html qui contiennent base_name\n",
    "        fichiers_html = [\n",
    "            f for f in os.listdir(f\"{folder_path}DATA/\")\n",
    "            if f.lower().endswith('.html')\n",
    "            and os.path.isfile(os.path.join(f\"{folder_path}DATA/\", f))\n",
    "            and base_name in f\n",
    "        ]\n",
    "\n",
    "    elif source_type == 'csv':\n",
    "        # Tous les .csv qui contiennent base_name\n",
    "        fichiers_html = [\n",
    "            f for f in os.listdir(f\"{folder_path}DATA/\")\n",
    "            if f.lower().endswith('.csv')\n",
    "            and os.path.isfile(os.path.join(f\"{folder_path}DATA/\", f))\n",
    "            and base_name in f\n",
    "        ]\n",
    "\n",
    "    elif source_type == 'istex':\n",
    "        # Vérifier si 'DATA' est déjà dans folder_path\n",
    "        if folder_path.endswith(\"DATA\") or folder_path.endswith(\"DATA/\"):\n",
    "            data_folder_path = folder_path\n",
    "        else:\n",
    "            data_folder_path = os.path.join(folder_path, \"DATA\")\n",
    "\n",
    "        # On cherche un sous-dossier dont le nom contient base_name\n",
    "        sous_dossier_principal = None\n",
    "        for f in os.listdir(data_folder_path):\n",
    "            if os.path.isdir(os.path.join(data_folder_path, f)) and base_name in f:\n",
    "                sous_dossier_principal = f\n",
    "                break\n",
    "\n",
    "        if sous_dossier_principal:\n",
    "            # Pour ISTEX, on récupère la liste des sous-sous-dossiers\n",
    "            fichiers_html = [\n",
    "                os.path.join(sous_dossier_principal, sub_f)\n",
    "                for sub_f in os.listdir(os.path.join(data_folder_path, sous_dossier_principal))\n",
    "                if os.path.isdir(os.path.join(data_folder_path, sous_dossier_principal, sub_f))\n",
    "            ]\n",
    "        else:\n",
    "            fichiers_html = []\n",
    "    else:\n",
    "        fichiers_html = []\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Barre de progression\n",
    "    # -------------------------------------------------------------------------\n",
    "    pbar = tqdm(\n",
    "        total=len(fichiers_html),\n",
    "        desc='DOCUMENTS PROCESSÉS'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Récupération des documents et fusion des informations\n",
    "    # -------------------------------------------------------------------------\n",
    "    all_columns_dicts = []\n",
    "    for f in fichiers_html:\n",
    "        # Construction du chemin complet\n",
    "        full_path = os.path.join(folder_path, 'DATA', f) \\\n",
    "            if source_type in ['europresse', 'csv'] else os.path.join(folder_path, 'DATA', f)\n",
    "\n",
    "        # On appelle directement load_documents\n",
    "        d, s, cd = load_documents(full_path, source_type, minimum_caracters_nb_by_document, pbar)\n",
    "        documents.extend(d)\n",
    "        all_soups.extend(s)\n",
    "        all_columns_dicts.append(cd)\n",
    "\n",
    "        # Mise à jour de la barre de progression\n",
    "        pbar.update(1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Fusionner les dictionnaires de colonnes\n",
    "    # -------------------------------------------------------------------------\n",
    "    for dico in all_columns_dicts:\n",
    "        for cle, valeur in dico.items():\n",
    "            if cle not in columns_dict:\n",
    "                columns_dict[cle] = []\n",
    "            columns_dict[cle].extend(valeur)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Suppression des doublons si demandé\n",
    "    # -------------------------------------------------------------------------\n",
    "    if go_remove_duplicates:\n",
    "        remove_duplicates_lsh()\n",
    "\n",
    "\n",
    "    # Supposons que les variables soient déjà définies :\n",
    "    # documents = [...]\n",
    "    # all_soups = [...]\n",
    "    # columns_dict = {...}\n",
    "\n",
    "    # On détermine les indices des documents \"valides\"\n",
    "    valid_indices = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        # Tokenisation basique\n",
    "        tokens = doc.split()\n",
    "        # Comptage des mots uniques\n",
    "        if len(set(tokens)) >= 10:\n",
    "            valid_indices.append(i)\n",
    "\n",
    "    # On reconstruit la liste documents et all_soups\n",
    "    documents = [documents[i] for i in valid_indices]\n",
    "    if len(all_soups) > 0:\n",
    "        all_soups = [all_soups[i] for i in valid_indices]\n",
    "\n",
    "    # On reconstruit les listes dans columns_dict en fonction des indices conservés\n",
    "    for key in columns_dict:\n",
    "        columns_dict[key] = [columns_dict[key][i] for i in valid_indices]\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Affichage final\n",
    "    # -------------------------------------------------------------------------\n",
    "    print('\\n')\n",
    "    print(len(documents), 'documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6WxTZ_WzYDU"
   },
   "outputs": [],
   "source": [
    "def update_candidates_for_unigram(kind, unigrams):\n",
    "    \"\"\"\n",
    "    Met à jour le dictionnaire `unigrams` en affectant la valeur 1\n",
    "    aux tokens pour lesquels le POS majoritaire (mode) est `kind`\n",
    "    et qui ont au moins 3 caractères.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Prépare la liste de travail, avec un éventuel unidecode pour les PROPN\n",
    "    if kind == 'PROPN':\n",
    "        all_tab_pos_for_work = copy.deepcopy(all_tab_pos)\n",
    "        for sentence in all_tab_pos_for_work:\n",
    "            for token_info in sentence:\n",
    "                token_info[0] = unidecode.unidecode(token_info[0])\n",
    "    else:\n",
    "        all_tab_pos_for_work = all_tab_pos\n",
    "\n",
    "    # 2) Un dictionnaire token -> liste des POS rencontrés\n",
    "    token_pos_map = defaultdict(list)\n",
    "\n",
    "    # 3) Un ensemble pour repérer vite lesquels ont déjà eu 'kind' et >= 3 caractères\n",
    "    candidates = set()\n",
    "\n",
    "    # 4) Parcours unique de all_tab_pos_for_work\n",
    "    for sentence in all_tab_pos_for_work:\n",
    "        for token, pos in sentence:\n",
    "            token_pos_map[token].append(pos)\n",
    "            # Si ce token a le POS recherché et au moins 3 lettres, on le \"mark\" comme candidat\n",
    "            if pos == kind and len(token) >= 3:\n",
    "                candidates.add(token)\n",
    "\n",
    "    # 5) Calcule le POS majoritaire pour chaque candidat et met à jour unigrams\n",
    "    for token in candidates:\n",
    "        pos_list = token_pos_map[token]\n",
    "        mode_pos, _ = Counter(pos_list).most_common(1)[0]\n",
    "        if mode_pos == kind:\n",
    "            unigrams[token] = 1\n",
    "\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8I0UxIrzYDU"
   },
   "outputs": [],
   "source": [
    "def remove_duplicates_lsh(threshold=0.8, num_perm=256):\n",
    "    \"\"\"\n",
    "    Détecte et supprime les quasi-doublons dans la variable globale `documents`\n",
    "    en utilisant un MinHash LSH (Locality-Sensitive Hashing).\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - threshold : float\n",
    "        Seuil de similarité Jaccard en-deçà duquel on ne considère pas les documents comme doublons.\n",
    "        (ex: 0.8 = 80% de similarité)\n",
    "    - num_perm : int\n",
    "        Nombre de permutations utilisées pour le MinHash. Plus ce nombre est grand,\n",
    "        plus la précision est élevée, mais le coût de calcul augmente.\n",
    "\n",
    "    Effets:\n",
    "    -------\n",
    "    - Modifie la liste globale `documents` en supprimant les quasi-doublons.\n",
    "    - Met à jour `columns_dict` et `all_soups` (si `source_type == 'europresse'`)\n",
    "      pour rester cohérents avec les documents restants.\n",
    "    \"\"\"\n",
    "    global documents, columns_dict, all_soups\n",
    "\n",
    "    # 1) Construire l'index LSH\n",
    "    # -------------------------------------------------------------------------\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "\n",
    "    # Pour stocker les MinHash de chaque document\n",
    "    doc_minhashes = []\n",
    "\n",
    "    # On ne garde que les 100 premiers tokens (comme dans votre code initial)\n",
    "    # puis on crée la signature MinHash\n",
    "    for i, doc in enumerate(documents):\n",
    "        tokens_100 = doc.split()[:100]  # tronque à 100 tokens\n",
    "\n",
    "        # Construire un MinHash pour ce document\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for token in tokens_100:\n",
    "            # Pour éviter les collisions d'encodage, on encode en UTF-8\n",
    "            m.update(token.encode('utf-8'))\n",
    "        doc_minhashes.append(m)\n",
    "\n",
    "        # On insère dans la structure LSH en associant l'ID du doc\n",
    "        lsh.insert(str(i), m)\n",
    "\n",
    "    # 2) Détecter les doublons via l'interrogation LSH\n",
    "    # -------------------------------------------------------------------------\n",
    "    # On va construire un ensemble d'indices à supprimer\n",
    "    to_remove = set()\n",
    "\n",
    "    # On parcourt chaque document dans l'ordre :\n",
    "    # si un document n'est pas déjà marqué pour suppression,\n",
    "    # on récupère tous ses quasi-doublons et on les marque pour suppression.\n",
    "    for i in range(len(documents)):\n",
    "        if i in to_remove:\n",
    "            continue  # déjà marqué, on passe\n",
    "\n",
    "        # Récupérer les documents similaires dans l'index\n",
    "        candidates = lsh.query(doc_minhashes[i])  # renvoie la liste des \"keys\" insérées\n",
    "\n",
    "        for c in candidates:\n",
    "            c_idx = int(c)\n",
    "            if c_idx != i:\n",
    "                # c_idx est jugé quasi-doublon de i\n",
    "                to_remove.add(c_idx)\n",
    "\n",
    "    # 3) Supprimer les doublons en ordre décroissant d'indice\n",
    "    # -------------------------------------------------------------------------\n",
    "    # (pour ne pas invalider les indices suivants lors du 'del')\n",
    "    indices_to_remove = sorted(to_remove, reverse=True)\n",
    "\n",
    "    for idx in indices_to_remove:\n",
    "        del documents[idx]\n",
    "        for key in columns_dict:\n",
    "            del columns_dict[key][idx]\n",
    "        if source_type == 'europresse':\n",
    "            del all_soups[idx]\n",
    "\n",
    "    # 4) Éventuel affichage / log\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"[LSH] {len(indices_to_remove)} quasi-doublons supprimés parmi {len(doc_minhashes)} documents initiaux.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ipay3YovzYDU"
   },
   "outputs": [],
   "source": [
    "def write_topics_unigrams():\n",
    "    for num_topic in all_nmf_H:\n",
    "        write_unigrams_results(100,\n",
    "                        tfidf_feature_names,\n",
    "                        all_nmf_H[num_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZUKeMCIzYDV"
   },
   "outputs": [],
   "source": [
    "def write_documents_infos():\n",
    "    # On va stocker nos données non plus sous forme de lignes strings,\n",
    "    # mais en listes de valeurs. Le csv.writer se chargera d'assembler\n",
    "    # correctement le tout.\n",
    "\n",
    "    for num_topic in tqdm(all_nmf_W,\n",
    "                          desc=\"ÉCRITURE DES FICHIERS SUR LE DISQUE\"):\n",
    "\n",
    "        # Préparation des données\n",
    "        rows = []\n",
    "\n",
    "        # Création de l'entête\n",
    "        config_key = num_topic  # ou bien len(scores) + 1, selon votre logique\n",
    "\n",
    "        # Exemple : pour Europresse\n",
    "        if source_type == 'europresse':\n",
    "            # On définit explicitement l'ordre des colonnes\n",
    "            header = [\n",
    "                'title',\n",
    "                'authors',\n",
    "                'raw_authors',\n",
    "                'nb_characters',\n",
    "                'journal',\n",
    "                'date',\n",
    "                'main_topic'\n",
    "            ]\n",
    "\n",
    "            # On ajoute les colonnes score_? où ? est le label associé\n",
    "            for i in range(num_topic):\n",
    "                if config_key in topic_labels_by_config and i < len(topic_labels_by_config[config_key]):\n",
    "                    label = topic_labels_by_config[config_key][i]\n",
    "                    # (Optionnel) Nettoyer/transformer le label pour éviter caractères spéciaux\n",
    "                    # Par exemple :\n",
    "                    # label_sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', label)\n",
    "                    # header.append(f\"score_{label_sanitized}\")\n",
    "                    # Ou simplement :\n",
    "                    header.append(f\"score_{label}\")\n",
    "                else:\n",
    "                    # Si jamais la clé ou l’indice n’existe pas, on met un fallback\n",
    "                    header.append(f\"score_Unknown_{i}\")\n",
    "\n",
    "            rows.append(header)\n",
    "\n",
    "        elif source_type in ['csv', 'istex']:\n",
    "            # On utilise columns_dict pour construire l'entête\n",
    "            header = list(columns_dict.keys())\n",
    "            header.extend(['nb_characters', 'main_topic'])\n",
    "\n",
    "            for i in range(num_topic):\n",
    "                if config_key in topic_labels_by_config and i < len(topic_labels_by_config[config_key]):\n",
    "                    label = topic_labels_by_config[config_key][i]\n",
    "                    header.append(f\"score_{label}\")\n",
    "                else:\n",
    "                    header.append(f\"score_Unknown_{i}\")\n",
    "\n",
    "            rows.append(header)\n",
    "\n",
    "        # Remplissage des données\n",
    "        if source_type == 'europresse':\n",
    "            for i in range(len(all_soups)):\n",
    "                if i < len(all_nmf_W[num_topic]):\n",
    "                    row_data = write_info_europresse(\n",
    "                        all_nmf_W[num_topic][i],\n",
    "                        all_soups[i],\n",
    "                        documents[i]\n",
    "                    )\n",
    "                    # Assurez-vous que write_info_europresse renvoie une liste et non une string\n",
    "                    rows.append(row_data)\n",
    "\n",
    "        elif source_type in ['csv', 'istex']:\n",
    "            for i in range(len(columns_dict['date'])):\n",
    "                row_data = write_info_another(\n",
    "                    all_nmf_W[num_topic][i],\n",
    "                    columns_dict,\n",
    "                    i,\n",
    "                    documents[i]\n",
    "                )\n",
    "                # Même remarque : write_info_another doit renvoyer une liste\n",
    "                rows.append(row_data)\n",
    "\n",
    "        class_suffix = \"_\".join(grammatical_classes)\n",
    "\n",
    "        # Écriture du CSV en utilisant le csv.writer\n",
    "        csv_path = (\n",
    "            f\"{results_path}{base_name}_EXPLORE_TOPICS/\"\n",
    "            f\"{base_name}_database_{num_topic}tc_{class_suffix}_\"\n",
    "            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp.csv\"\n",
    "        )\n",
    "\n",
    "        with open(csv_path, \"w\", encoding='utf-8', newline='') as file_object:\n",
    "            writer = csv.writer(file_object, delimiter=';', quoting=csv.QUOTE_MINIMAL)\n",
    "            for row in rows:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aV2Qz11nKeZ"
   },
   "outputs": [],
   "source": [
    "def process_sentiments():\n",
    "    sentiments = []\n",
    "    dates = []\n",
    "    transformed_sentiments = []\n",
    "\n",
    "    model_name = 'nlptown/bert-base-multilingual-uncased-sentiment' # Modèle spécifique pour l'analyse de sentiments\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 0  # ou torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = -1\n",
    "\n",
    "    print('device', device)\n",
    "\n",
    "    sentiment_pipeline = pipeline('sentiment-analysis',\n",
    "                                  model=model,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  truncation=True,\n",
    "                                  max_length=512,\n",
    "                                  device=device)\n",
    "\n",
    "    # Imaginons que 'documents' est votre tableau de textes\n",
    "    sentiments = [analyze_sentiment(doc, sentiment_pipeline) for doc in tqdm(documents, desc=\"Processing Documents\")]\n",
    "\n",
    "    if source_type == 'europresse':\n",
    "        for soup in all_soups:\n",
    "            header = soup\n",
    "            date_text = extract_information(header, '.DocHeader')\n",
    "            date_text_clean = extract_date_info(date_text)\n",
    "            date_normalized = normalise_date(date_text_clean).replace(';', '').replace('&', '')\n",
    "            dates.append(date_normalized)\n",
    "    else:\n",
    "        dates = formater_liste_dates(columns_dict['date'])\n",
    "\n",
    "    # Filtrer et convertir les dates\n",
    "    new_dates = []\n",
    "    for date_str in dates:\n",
    "        date = extract_and_convert_date(date_str)\n",
    "        new_dates.append(date)\n",
    "\n",
    "    dates = new_dates\n",
    "\n",
    "    # Transformer les sentiments en scores basés sur les étoiles\n",
    "    for doc_sentiments in sentiments:\n",
    "        if doc_sentiments:  # Assurez-vous qu'il n'est pas None ou vide\n",
    "            doc_scores = []\n",
    "            # doc_sentiments est maintenant une liste de dictionnaires\n",
    "            for sentiment_dict in doc_sentiments:\n",
    "                label = sentiment_dict['label']         # ex: '4 stars'\n",
    "                star_rating = int(label.split()[0])     # ex: 4\n",
    "                doc_scores.append(star_rating)\n",
    "\n",
    "            average_score = sum(doc_scores) / len(doc_scores)\n",
    "            transformed_sentiments.append(average_score)\n",
    "        else:\n",
    "            transformed_sentiments.append(None)\n",
    "\n",
    "\n",
    "\n",
    "    # Ici, on modifie la fonction sentiments_heatmaps (ou son appel)\n",
    "    # pour qu'elle considère les arrays comme base. Par exemple :\n",
    "    for apply_horizontal_normalization in [False, True]:\n",
    "        sentiments_heatmaps(\n",
    "            apply_horizontal_normalization=apply_horizontal_normalization,\n",
    "            sigma='auto',\n",
    "            transformed_sentiments=transformed_sentiments,\n",
    "            dates=dates\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV5LKXIdllXM"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 2) nouvelle : calc_positions_for_continuous_spacing\n",
    "# ===================================================================\n",
    "def calc_positions_for_continuous_spacing(xmin, xmax, text_width, spacing_factor):\n",
    "    \"\"\"\n",
    "    Calcule (sans dessiner) les positions X (en data coords)\n",
    "    où l’on placerait chaque label, en avançant de 'text_width * spacing_factor'\n",
    "    tant que le bord droit du label (current_x + text_width)\n",
    "    ne dépasse pas xmax (avec une petite tolérance).\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    current_x = xmin + text_width / 2\n",
    "\n",
    "    tolerance_max = xmax + (text_width / 2)\n",
    "    while True:\n",
    "        right_edge = current_x + text_width\n",
    "        if right_edge > tolerance_max:\n",
    "            break\n",
    "        positions.append(current_x)\n",
    "        current_x += text_width * spacing_factor\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 3) nouvelle : find_optimal_continuous_spacing\n",
    "# ===================================================================\n",
    "def find_optimal_continuous_spacing(xmin, xmax, text_width,\n",
    "                                    spacing_factor_min=1.02,\n",
    "                                    spacing_factor_max=1.2,\n",
    "                                    step=0.001):\n",
    "\n",
    "    best_sf = spacing_factor_min\n",
    "    best_diff = float('inf')\n",
    "    best_positions = []\n",
    "\n",
    "    tolerance_max = xmax + (text_width / 2)\n",
    "    spacing_values = np.arange(spacing_factor_min, spacing_factor_max + step, step)\n",
    "\n",
    "    for sf in spacing_values:\n",
    "        positions = calc_positions_for_continuous_spacing(\n",
    "            xmin=xmin,\n",
    "            xmax=xmax,\n",
    "            text_width=text_width,\n",
    "            spacing_factor=sf\n",
    "        )\n",
    "\n",
    "        if not positions:\n",
    "            diff = 9999\n",
    "        else:\n",
    "            last_x = positions[-1]\n",
    "            right_edge = last_x + text_width\n",
    "            diff = abs(tolerance_max - right_edge)\n",
    "\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_sf = sf\n",
    "            best_positions = positions\n",
    "            if diff < 1e-9:\n",
    "                break\n",
    "\n",
    "    return best_sf, best_positions\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 4) nouvelle : manual_tick_placement_continuous\n",
    "# ===================================================================\n",
    "def manual_tick_placement_continuous(\n",
    "    ax,\n",
    "    xmin,\n",
    "    xmax,\n",
    "    spacing_factor_min=1.02,\n",
    "    spacing_factor_max=1.2,\n",
    "    step=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Place manuellement des pseudo-ticks pour l'axe X dans [xmin..xmax].\n",
    "    On coupe les ticks officiels et on dessine les labels via ax.text.\n",
    "    \"\"\"\n",
    "    text_width = compute_text_width_in_data_coords(ax)\n",
    "\n",
    "    # 1) Désactiver les ticks \"officiels\"\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "\n",
    "    # 2) Recherche du spacing_factor optimal\n",
    "    best_sf, _ = find_optimal_continuous_spacing(\n",
    "        xmin=xmin,\n",
    "        xmax=xmax,\n",
    "        text_width=text_width,\n",
    "        spacing_factor_min=spacing_factor_min,\n",
    "        spacing_factor_max=spacing_factor_max,\n",
    "        step=step\n",
    "    )\n",
    "    # 3) Placement effectif\n",
    "    positions = calc_positions_for_continuous_spacing(\n",
    "        xmin=xmin,\n",
    "        xmax=xmax,\n",
    "        text_width=text_width,\n",
    "        spacing_factor=best_sf\n",
    "    )\n",
    "\n",
    "    # 4) Transform \"Axes\" + offset en points\n",
    "    offset_axes_transform = mtransforms.offset_copy(\n",
    "        ax.transAxes,\n",
    "        fig=ax.figure,\n",
    "        x=0,\n",
    "        y=-1.0,\n",
    "        units='points'\n",
    "    )\n",
    "\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    # 5) Dessin de chaque label\n",
    "    for x_val in positions:\n",
    "        x_val_axes = (x_val - x_min) / (x_max - x_min)\n",
    "        label_str = f\"{x_val:.3f}\"  # arrondi; adapter si besoin\n",
    "\n",
    "        ax.text(\n",
    "            x_val_axes,\n",
    "            0.0,   # tout en bas du subplot\n",
    "            label_str,\n",
    "            rotation=90,\n",
    "            rotation_mode='anchor',\n",
    "            ha='right',\n",
    "            va='center',\n",
    "            transform=offset_axes_transform,\n",
    "            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.0')\n",
    "        )\n",
    "\n",
    "    return best_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lJlX8yPqlpd"
   },
   "outputs": [],
   "source": [
    "# Vérification de la Multicollinéarité\n",
    "# Calcul du VIF (Facteur d'Inflation de la Variance)\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
    "\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihE7Cy5Fq40a"
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(text, sentiment_pipeline):\n",
    "    try:\n",
    "        # Analyse de sentiments directement sur le texte complet,\n",
    "        # en demandant explicitement la troncation à 512 tokens\n",
    "        result = sentiment_pipeline(text, truncation=True, max_length=512)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sentiment analysis: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AqPelFWq4zL"
   },
   "outputs": [],
   "source": [
    "def compute_text_width_in_data_coords(ax):\n",
    "    \"\"\"\n",
    "    Mesure la largeur (en 'data coords') d'un texte donné,\n",
    "    via un placement temporaire invisible pour récupérer la bounding box en pixels.\n",
    "    \"\"\"\n",
    "    temp_text = ax.text(\n",
    "        0,\n",
    "        0,\n",
    "        '0123456789',\n",
    "        rotation=90,\n",
    "        rotation_mode='anchor',\n",
    "        ha='right',\n",
    "        va='center',\n",
    "        alpha=0,\n",
    "        transform=ax.transAxes  # On place ceci en Axes coords (peu importe où)\n",
    "    )\n",
    "    ax.figure.canvas.draw()\n",
    "    renderer = ax.figure.canvas.get_renderer()\n",
    "    bbox = temp_text.get_window_extent(renderer=renderer)\n",
    "    pixel_width = bbox.width\n",
    "\n",
    "    # Convertir la largeur (en pixels) -> (en data coords sur l'axe X) :\n",
    "    x0_data, _ = ax.transData.transform((0, 0))\n",
    "    x1_data, _ = ax.transData.transform((1, 0))\n",
    "    one_unit_in_pixels = x1_data - x0_data\n",
    "    data_width = pixel_width / one_unit_in_pixels\n",
    "\n",
    "    temp_text.remove()\n",
    "    return data_width\n",
    "\n",
    "\n",
    "def compute_label_positions_for_spacing(ncols, text_width, spacing_factor, with_colormap):\n",
    "    \"\"\"\n",
    "    Calcule (sans dessiner) les positions X (en data coords)\n",
    "    où l’on placerait chaque label, sachant que TOUS les labels\n",
    "    ont la même 'text_width'.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    ncols         : int\n",
    "        Nombre total de labels (i.e. nombre de colonnes).\n",
    "    text_width    : float\n",
    "        Largeur fixe (en data coords) pour chaque label.\n",
    "    spacing_factor: float\n",
    "        Facteur d'espacement entre deux labels successifs.\n",
    "\n",
    "    Retourne\n",
    "    --------\n",
    "    list of (x, text_width)\n",
    "        La liste des positions (x, text_width) en data coords.\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    current_x = 0.0\n",
    "\n",
    "    while True:\n",
    "        col_index = int(np.floor(current_x))\n",
    "        if col_index >= ncols:\n",
    "            # On a dépassé le nombre de colonnes, on s’arrête\n",
    "            break\n",
    "\n",
    "        # Bord droit (en data coords) si on place le label à \"current_x\"\n",
    "        right_edge = current_x + text_width\n",
    "\n",
    "        if with_colormap:\n",
    "            tolerance_max = (ncols - 1) + 0.17*text_width\n",
    "        else:\n",
    "            tolerance_max = (ncols - 1) + 0.001*text_width\n",
    "\n",
    "        if right_edge <= tolerance_max:\n",
    "            # On enregistre la position (pour info)\n",
    "            positions.append((current_x, text_width))\n",
    "            # On décale \"current_x\" pour le prochain label\n",
    "            current_x += text_width * spacing_factor\n",
    "        else:\n",
    "            # Si on dépasse trop, on arrête\n",
    "            break\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def find_best_spacing_factor(\n",
    "    ncols,\n",
    "    text_width,\n",
    "    spacing_factor_min=1.02,\n",
    "    spacing_factor_max=1.2,\n",
    "    step=0.001,\n",
    "    with_colormap=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Cherche le spacing_factor qui permet d'occuper au mieux la place disponible\n",
    "    sans trop déborder la tolérance max = (ncols - 1) pour le dernier label.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    ncols : int\n",
    "        Nombre total de labels.\n",
    "    text_width : float\n",
    "        Largeur fixe (en data coords) à utiliser pour tous les labels.\n",
    "    spacing_factor_min : float\n",
    "        Borne inférieure pour la recherche du spacing factor.\n",
    "    spacing_factor_max : float\n",
    "        Borne supérieure pour la recherche du spacing factor.\n",
    "    step : float\n",
    "        Pas d'incrémentation pour la recherche brute force.\n",
    "\n",
    "    Renvoie\n",
    "    -------\n",
    "    (best_sf, positions):\n",
    "        best_sf : float\n",
    "            Le spacing factor optimal.\n",
    "        positions : list of (x, text_width)\n",
    "            Liste des positions correspondant à best_sf.\n",
    "    \"\"\"\n",
    "    best_sf = spacing_factor_min\n",
    "    best_diff = float('inf')\n",
    "    best_positions = []\n",
    "\n",
    "    spacing_values = np.arange(spacing_factor_min, spacing_factor_max + step, step)\n",
    "\n",
    "    for sf in spacing_values:\n",
    "        positions = compute_label_positions_for_spacing(\n",
    "            ncols=ncols,\n",
    "            text_width=text_width,\n",
    "            spacing_factor=sf,\n",
    "            with_colormap=with_colormap\n",
    "        )\n",
    "\n",
    "        if not positions:\n",
    "            # Aucune position => on fixe un diff arbitraire\n",
    "            diff = 9999\n",
    "        else:\n",
    "            # On regarde la position (et la largeur) du dernier label\n",
    "            last_x, last_w = positions[-1]\n",
    "            right_edge = last_x + last_w\n",
    "            # Tolerance max\n",
    "            if with_colormap:\n",
    "                tolerance_max = (ncols - 1) + 0.17*text_width\n",
    "            else:\n",
    "                tolerance_max = (ncols - 1) + 0.001*text_width\n",
    "\n",
    "            # On calcule la différence\n",
    "            diff = abs(tolerance_max - right_edge)\n",
    "\n",
    "        # Mise à jour du meilleur spacing factor\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_sf = sf\n",
    "            best_positions = positions\n",
    "            # Si diff est vraiment très faible, on peut s'arrêter (optionnel)\n",
    "            if diff < 1e-9:\n",
    "                break\n",
    "\n",
    "    return best_sf, best_positions\n",
    "\n",
    "\n",
    "def manual_tick_placement(\n",
    "    ax,\n",
    "    df,\n",
    "    spacing_factor_min=1.02,\n",
    "    spacing_factor_max=1.2,\n",
    "    step=0.001,\n",
    "    with_colormap=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Place manuellement des \"pseudo-ticks\" et leurs labels,\n",
    "    SANS utiliser ax.set_xticks / ax.set_xticklabels / ax.tick_params.\n",
    "\n",
    "    Hypothèse simplifiée :\n",
    "    - on considère que TOUS les labels ont la même largeur\n",
    "      (calculée sur une date aléatoire par exemple).\n",
    "\n",
    "    Étapes :\n",
    "    1) On désactive l'axe officiel.\n",
    "    2) On crée un transform mixte (X en data, Y en Axes).\n",
    "    3) On prend une date aléatoire dans df.columns -> on calcule la largeur du texte.\n",
    "    4) On cherche le spacing_factor optimal (find_best_spacing_factor).\n",
    "    5) On calcule les positions finales et on dessine chaque label manuellement.\n",
    "    \"\"\"\n",
    "    # 1) Désactiver l'axe \"officiel\"\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(0, len(df.columns) - 1)\n",
    "\n",
    "\n",
    "\n",
    "    ncols = len(df.columns)\n",
    "    if ncols == 0:\n",
    "        return  # Rien à faire si df n'a pas de colonnes\n",
    "\n",
    "    # 3) Prendre une date \"au hasard\" (ou la première), calculer sa largeur\n",
    "    random_col = df.columns[0]   # ou n’importe quel index\n",
    "    text_random = random_col.strftime(\"%Y-%m-%d\")\n",
    "    text_width = compute_text_width_in_data_coords(ax)\n",
    "\n",
    "    # 4) Trouver le spacing_factor optimal\n",
    "    best_sf, positions_preview = find_best_spacing_factor(\n",
    "        ncols=ncols,\n",
    "        text_width=text_width,\n",
    "        spacing_factor_min=spacing_factor_min,\n",
    "        spacing_factor_max=spacing_factor_max,\n",
    "        step=step,\n",
    "        with_colormap=with_colormap\n",
    "    )\n",
    "\n",
    "    # 5) Placement effectif\n",
    "    positions = compute_label_positions_for_spacing(\n",
    "        ncols=ncols,\n",
    "        text_width=text_width,\n",
    "        spacing_factor=best_sf,\n",
    "        with_colormap=with_colormap\n",
    "    )\n",
    "\n",
    "    # On récupère les limites X de l'axe\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "\n",
    "    # 1) Construire une transform \"Axes\" + offset en points\n",
    "    offset_axes_transform = mtransforms.offset_copy(\n",
    "        ax.transAxes,            # on part du repère Axes (0..1)\n",
    "        fig=ax.figure,\n",
    "        x=0,\n",
    "        y=-0.7,\n",
    "        units='points'\n",
    "    )\n",
    "\n",
    "    # 2) Boucle d'affichage\n",
    "    for (x_val, _) in positions:\n",
    "        col_index = int(np.floor(x_val))\n",
    "        if col_index < ncols:\n",
    "            label_str = df.columns[col_index].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # Convertir x_val (data) -> x_val_axes (0..1)\n",
    "            x_val_axes = (x_val - x_min) / (x_max - x_min)\n",
    "\n",
    "            # On place le texte en Axes coords\n",
    "            ax.text(\n",
    "                x_val_axes,   # X en [0..1]\n",
    "                0.0,          # Y=0 en Axes coords (bas de l'axe)\n",
    "                label_str,\n",
    "                rotation=90,\n",
    "                rotation_mode='anchor',\n",
    "                ha='right',\n",
    "                va='top',     # ancré \"en haut\" pour que le -2 pts décale vers le bas\n",
    "                transform=offset_axes_transform\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    return best_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGMT8MD62vY8"
   },
   "outputs": [],
   "source": [
    "def measure_text_height_axes(ax, label=\"999.99\"):\n",
    "    \"\"\"\n",
    "    Renvoie :\n",
    "      - text_height_axes : la hauteur TOTALE (bottom -> top) en coords AXES\n",
    "      - offset_axes      : la distance (baseline - bottom) de la bbox en coords AXES\n",
    "    \"\"\"\n",
    "    # Placement (invisible) d'un texte aligné baseline, à la position (0,0) en AXES\n",
    "    text_baseline = ax.text(\n",
    "        0,\n",
    "        0,\n",
    "        label,\n",
    "        va='baseline',\n",
    "        ha='left',\n",
    "        alpha=0,  # invisible\n",
    "        transform=ax.transAxes  # <-- On le place en Axes !\n",
    "    )\n",
    "\n",
    "    # On force un rendu pour obtenir la bbox en coords pixels\n",
    "    ax.figure.canvas.draw()\n",
    "    renderer = ax.figure.canvas.get_renderer()\n",
    "    bbox = text_baseline.get_window_extent(renderer=renderer)\n",
    "    text_baseline.remove()\n",
    "\n",
    "    # Coordonnées (x, y) en PIXELS du point (0, 0) Axes\n",
    "    anchor_pixel = ax.transAxes.transform((0, 0))\n",
    "\n",
    "    # Hauteur de la bbox en pixels\n",
    "    text_height_pixel = bbox.height\n",
    "\n",
    "    # Décalage (baseline -> bottom) en pixels\n",
    "    offset_pixel = anchor_pixel[1] - bbox.y0\n",
    "\n",
    "    # 1 “unité Axes” = combien de pixels ?\n",
    "    y0_pix = ax.transAxes.transform((0, 0))[1]\n",
    "    y1_pix = ax.transAxes.transform((0, 1))[1]\n",
    "    one_axes_unit_in_pixels = abs(y1_pix - y0_pix)\n",
    "\n",
    "    # Conversion des pixels -> coords Axes\n",
    "    text_height_axes = text_height_pixel / one_axes_unit_in_pixels\n",
    "    offset_axes      = offset_pixel      / one_axes_unit_in_pixels\n",
    "\n",
    "    return text_height_axes, offset_axes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_to_axes(y_data, data_min, data_max):\n",
    "    \"\"\"Convertit y_data (dans [data_min, data_max]) en [0,1].\"\"\"\n",
    "    return (y_data - data_min) / (data_max - data_min)\n",
    "\n",
    "\n",
    "\n",
    "def axes_to_data(y_axes, data_min, data_max):\n",
    "    \"\"\"Convertit y_axes (dans [0,1]) en [data_min, data_max].\"\"\"\n",
    "    return y_axes*(data_max - data_min) + data_min\n",
    "\n",
    "\n",
    "def compute_label_positions_axes(\n",
    "    text_height_axes,\n",
    "    offset_axes,\n",
    "    spacing_factor,\n",
    "    vmin_axes=0.0,\n",
    "    vmax_axes=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcule les positions en Axes-coords pour placer les labels\n",
    "    de vmin_axes à vmax_axes (en général [0,1]).\n",
    "\n",
    "    On part baseline = vmin_axes,\n",
    "    et on incrémente de (text_height_axes * spacing_factor).\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    current_baseline = vmin_axes + 0.0016\n",
    "\n",
    "    while True:\n",
    "        top_of_bbox = current_baseline + (text_height_axes - offset_axes)\n",
    "        if top_of_bbox > vmax_axes:\n",
    "            break\n",
    "\n",
    "        # On enregistre la position en Axes-coords\n",
    "        positions.append(current_baseline)\n",
    "\n",
    "        current_baseline += (text_height_axes * spacing_factor)\n",
    "        if current_baseline > vmax_axes:\n",
    "            break\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_final_top_baseline_axes(baseline, text_height_axes, offset_axes):\n",
    "    \"\"\"\n",
    "    baseline = la baseline du texte (axes-coords)\n",
    "    Retourne la coord \"top\" de la bbox du dernier label.\n",
    "    \"\"\"\n",
    "    return baseline + (text_height_axes - offset_axes)\n",
    "\n",
    "def find_best_spacing_factor_axes(\n",
    "    ax,\n",
    "    text_height_axes,\n",
    "    offset_axes,\n",
    "    spacing_factor_min=1.02,\n",
    "    spacing_factor_max=1.2,\n",
    "    step=0.001,\n",
    "    vmin_axes=0.0,\n",
    "    vmax_axes=1.0\n",
    "):\n",
    "    best_sf = spacing_factor_min\n",
    "    best_diff = float('inf')\n",
    "    spacing_factors = np.arange(spacing_factor_min, spacing_factor_max + step, step)\n",
    "\n",
    "    for sf in tqdm(spacing_factors):\n",
    "        positions = compute_label_positions_axes(\n",
    "            text_height_axes, offset_axes, sf,\n",
    "            vmin_axes, vmax_axes\n",
    "        )\n",
    "        if positions:\n",
    "            last_baseline = positions[-1]\n",
    "            last_top = get_final_top_baseline_axes(last_baseline, text_height_axes, offset_axes)\n",
    "            diff = abs(last_top - vmax_axes)\n",
    "        else:\n",
    "            # Si on ne trouve aucun label => diff = 1.0\n",
    "            diff = 1.0\n",
    "\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_sf = sf\n",
    "            if diff == 0:\n",
    "                break\n",
    "\n",
    "    return best_sf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def manual_colorbar_ticks(\n",
    "    fig,\n",
    "    ax,\n",
    "    data_min,      # 2.60 par exemple\n",
    "    data_max,      # 3.46 par exemple\n",
    "    spacing_factor_min=1.02,\n",
    "    spacing_factor_max=1.2,\n",
    "    step=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Mesure la hauteur du texte en Axes-coords\n",
    "    2) Trouve un spacing_factor optimal pour remplir [0,1] verticalement\n",
    "    3) Calcule toutes les baselines en [0,1]\n",
    "    4) Affiche des labels correspondant à la \"vraie\" valeur data\n",
    "       sur la baseline Axes correspondante\n",
    "    \"\"\"\n",
    "    # 1) Hauteur du texte\n",
    "    text_height_axes, offset_axes = measure_text_height_axes(ax, label=\"999.99\")\n",
    "\n",
    "    # 2) Spacing factor optimal\n",
    "    sf_opt = find_best_spacing_factor_axes(\n",
    "        ax,\n",
    "        text_height_axes,\n",
    "        offset_axes,\n",
    "        spacing_factor_min,\n",
    "        spacing_factor_max,\n",
    "        step,\n",
    "        vmin_axes=0.0,\n",
    "        vmax_axes=1.0\n",
    "    )\n",
    "\n",
    "    # 3) Calcul positions\n",
    "    positions_axes = compute_label_positions_axes(\n",
    "        text_height_axes,\n",
    "        offset_axes,\n",
    "        sf_opt,\n",
    "        vmin_axes=0.0,\n",
    "        vmax_axes=1.0\n",
    "    )\n",
    "\n",
    "    # Créez un offset de 10 points vers la droite et 0 points vers le haut\n",
    " #   offset = transforms.ScaledTranslation(0.7/72, 0, fig.dpi_scale_trans)\n",
    "\n",
    "    offset_axes_transform = mtransforms.offset_copy(\n",
    "        ax.transAxes,\n",
    "        fig=ax.figure,\n",
    "        x=1.5,\n",
    "        y=0,\n",
    "        units='points'\n",
    "    )\n",
    "    # 10/72 car 1 point = 1/72 inch\n",
    "\n",
    "    # Combinez ax.transAxes avec cet offset\n",
    "  #  trans = ax.transAxes + offset_axes_transform\n",
    "\n",
    "    # 4) Dessin\n",
    "    for baseline_axes in positions_axes:\n",
    "        # Convertir la baseline axes -> data\n",
    "        val_data = axes_to_data(baseline_axes, data_min, data_max)\n",
    "        label_str = f\"{val_data:.2f}\"\n",
    "\n",
    "        ax.text(\n",
    "            1,\n",
    "            baseline_axes,\n",
    "            label_str,\n",
    "            va='baseline',\n",
    "            ha='left',\n",
    "            transform=offset_axes_transform\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diRhYIUQ2vXH"
   },
   "outputs": [],
   "source": [
    "def create_custom_colorbar(fig,\n",
    "                           df_normalized,\n",
    "                           cmap=plt.cm.coolwarm,\n",
    "                           colorbar_position=[1.0, 0.0, 0.02, 0.85],\n",
    "                           step=0.0005,\n",
    "                           spacing_factor_min=1.02,\n",
    "                           spacing_factor_max=1.2):\n",
    "\n",
    "    # Déterminer les valeurs min et max pour la normalisation\n",
    "    vmin = df_normalized.min().min()\n",
    "    vmax = df_normalized.max().max()\n",
    "\n",
    "    # Créer la normalisation et l'objet ScalarMappable\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])  # Nécessaire pour l'affectation de la colorbar\n",
    "\n",
    "    # Ajouter un axe pour la colorbar (à droite, ici)\n",
    "    cbar_ax = fig.add_axes(colorbar_position)\n",
    "\n",
    "    # Créer la colorbar\n",
    "    cbar = fig.colorbar(sm,\n",
    "                        cax=cbar_ax,\n",
    "                        orientation='vertical',\n",
    "                        spacing='proportional',\n",
    "                        extend='neither',\n",
    "                        fraction=1.0,\n",
    "                        pad=0.0)\n",
    "\n",
    "    # Personnaliser l'apparence de la colorbar\n",
    "    cbar.outline.set_visible(False)  # Supprime la bordure\n",
    "    cbar.set_ticks([])               # Supprime les graduations par défaut\n",
    "\n",
    "    # Appeler la fonction manuelle pour définir les graduations\n",
    "    manual_colorbar_ticks(\n",
    "        fig,\n",
    "        cbar_ax,\n",
    "        vmin,\n",
    "        vmax,\n",
    "        step=step,\n",
    "        spacing_factor_min=spacing_factor_min,\n",
    "        spacing_factor_max=spacing_factor_max\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMRbA_5SwbxO"
   },
   "outputs": [],
   "source": [
    "def remove_outliers_by_mean(df, threshold=100):\n",
    "    \"\"\"\n",
    "    Pour chaque (Group, Date), remplace par 0\n",
    "    toute valeur > threshold * moyenne_des_autres (dans la même ligne).\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Le DataFrame dont les lignes sont des Group et les colonnes des dates.\n",
    "    threshold : float\n",
    "        Facteur multiplicatif pour détecter les outliers (défaut = 100).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    df_out : pd.DataFrame\n",
    "        Copie de df avec les outliers remplacés par 0.\n",
    "    outliers_df : pd.DataFrame\n",
    "        Tableau listant les outliers détectés (Group, Date, Value).\n",
    "    \"\"\"\n",
    "    # Copie pour ne pas modifier l'original\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Liste pour stocker les outliers détectés\n",
    "    outliers = []\n",
    "\n",
    "    # Parcours de chaque ligne (group)\n",
    "    for group, row in df_out.iterrows():\n",
    "        # Parcours des colonnes (dates)\n",
    "        for date in df_out.columns:\n",
    "            val = row[date]\n",
    "\n",
    "            # Moyenne des autres colonnes de la ligne\n",
    "            avg_ignore = row.drop(labels=date).mean()\n",
    "\n",
    "            # Test d'outlier : val > threshold * moyenne_des_autres\n",
    "            if (avg_ignore > 0) and (val > threshold * avg_ignore):\n",
    "                # On remplace par 0 dans le DataFrame\n",
    "                df_out.at[group, date] = 0\n",
    "                # On garde trace de l'outlier\n",
    "                outliers.append((group, date, val))\n",
    "\n",
    "    # Création d'un DataFrame pour les outliers détectés\n",
    "    outliers_df = pd.DataFrame(outliers, columns=['Group', 'Date', 'Value'])\n",
    "\n",
    "    return df_out, outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuRdVcp92vUy"
   },
   "outputs": [],
   "source": [
    "def plot_custom_heatmap(\n",
    "    df,\n",
    "    sigma='auto',\n",
    "    cmap=\"coolwarm\",\n",
    "    apply_horizontal_normalization=True,\n",
    "    with_colormap=True\n",
    "):\n",
    "    df, outliers_table = remove_outliers_by_mean(df, threshold=100000)\n",
    "\n",
    "    if len(outliers_table) > 1:\n",
    "        print('Valeurs aberrantes trouvées')\n",
    "        print(outliers_table)\n",
    "\n",
    "    df = df.astype(float)\n",
    "\n",
    "    # 1) Application du filtre gaussien, ligne par ligne\n",
    "    list_of_series = []\n",
    "    for index, row in df.iterrows():\n",
    "        filtered_values = gaussian_filter(row, sigma=sigma)\n",
    "        s = pd.Series(filtered_values, index=df.columns, name=index)\n",
    "        list_of_series.append(s)\n",
    "\n",
    "    df_normalized = pd.concat(list_of_series, axis=1).T\n",
    "\n",
    "    if apply_horizontal_normalization:\n",
    "        list_of_series = []\n",
    "        for index, row in df_normalized.iterrows():\n",
    "            normalized_values = (row - row.min()) / (row.max() - row.min()) if (row.max() != row.min()) else row\n",
    "            s = pd.Series(normalized_values, index=df_normalized.columns, name=index)\n",
    "            list_of_series.append(s)\n",
    "\n",
    "        df_normalized = pd.concat(list_of_series, axis=1).T\n",
    "\n",
    "    if len(df_normalized) > 1: # On ne trie que s'il y a plus d'une ligne\n",
    "        if apply_horizontal_normalization:\n",
    "            # 1) On calcule, pour chaque série, la colonne où se situe son max\n",
    "            # idxmax() renvoie le premier index (nom de colonne ici) où le max est trouvé par ligne\n",
    "            max_positions = df_normalized.idxmax(axis=1)\n",
    "\n",
    "            # Note: Si les noms de colonnes ne sont pas nativement ordonnables (ex: 'Day1', 'Day10', 'Day2'),\n",
    "            # idxmax peut ne pas trier comme attendu \"chronologiquement\".\n",
    "            # Si vos colonnes représentent des étapes ordonnées et ont des noms simples (0, 1, 2...) ou\n",
    "            # des dates/timestamps, sort_values() fonctionnera correctement.\n",
    "            # Si ce sont des strings comme 'Day1', 'Day10', le tri sera lexicographique ('Day1', 'Day10', 'Day2').\n",
    "            # Si un tri chronologique strict basé sur les colonnes est nécessaire avec des noms complexes,\n",
    "            # une étape supplémentaire de mappage ou de conversion des noms de colonnes pourrait être requise.\n",
    "\n",
    "            # 2) On trie les index des lignes (les séries) selon ces positions de max (ordre des colonnes)\n",
    "            sorted_index = max_positions.sort_values().index\n",
    "\n",
    "            # 3) On réordonne le DataFrame dans ce nouvel ordre\n",
    "            df_normalized = df_normalized.loc[sorted_index]\n",
    "\n",
    "        else: # apply_horizontal_normalization is False\n",
    "            # 1) On calcule l'intensité générale (somme cumulée) pour chaque série (ligne)\n",
    "            series_intensity = df_normalized.sum(axis=1)\n",
    "\n",
    "            # 2) On trie les index des lignes (les séries) selon cette intensité, de la plus forte à la plus basse\n",
    "            sorted_index = series_intensity.sort_values(ascending=False).index\n",
    "\n",
    "            # 3) On réordonne le DataFrame dans ce nouvel ordre\n",
    "            df_normalized = df_normalized.loc[sorted_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 8) Ajustement de la hauteur de figure\n",
    "    figure_height_inch = (df_normalized.shape[0] * PX_PER_TOPIC) / DPI\n",
    "\n",
    "    # 9) Création de la figure et tracé de la heatmap\n",
    "    fig = plt.figure(figsize=(FIGURE_WIDTH_INCH, figure_height_inch), dpi=DPI)\n",
    "    main_ax = fig.add_axes([0.3, 0.0, 0.697, 0.85])\n",
    "\n",
    "    mask = pd.DataFrame(False, index=df_normalized.index, columns=df_normalized.columns)\n",
    "\n",
    "    # On passe à True directement via .loc,\n",
    "    # en prenant outliers_table[\"Group\"] comme index de lignes\n",
    "    # et outliers_table[\"Date\"] comme index de colonnes\n",
    "    mask.loc[outliers_table[\"Group\"], outliers_table[\"Date\"]] = True\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        df_normalized,\n",
    "        cmap=cmap,\n",
    "        ax=main_ax,\n",
    "        cbar=False,\n",
    "        rasterized=False,\n",
    "        linewidths=0.0,\n",
    "        linecolor=\"white\",\n",
    "        mask=mask\n",
    "    )\n",
    "\n",
    "\n",
    "    # 10) Tracé des lignes de séparation horizontales\n",
    "    for i in range(1, df_normalized.shape[0]):\n",
    "        ax.axhline(i, color=\"white\", linewidth=1)\n",
    "\n",
    "    # 11) Affichage manuel des labels (index) à gauche\n",
    "    offset_axes_transform_2 = mtransforms.offset_copy(\n",
    "        ax.transAxes,\n",
    "        fig=ax.figure,\n",
    "        x=-1.5,  # Décalage en points\n",
    "        y=0,\n",
    "        units='points'\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(df_normalized.index):\n",
    "        # On calcule la position du haut vers le bas\n",
    "        y_pos = (df_normalized.shape[0] - i - 0.5) / df_normalized.shape[0]\n",
    "        ax.text(\n",
    "            0,\n",
    "            y_pos,\n",
    "            label,\n",
    "            ha='right',\n",
    "            va='center',\n",
    "            transform=offset_axes_transform_2\n",
    "        )\n",
    "\n",
    "    # 12) Supprimer les bordures & masquer les graduations Y\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Placement manuel des ticks X (dates ou colonnes) si besoin\n",
    "    manual_tick_placement(\n",
    "        ax,\n",
    "        df_normalized,\n",
    "        spacing_factor_min=1.02,\n",
    "        spacing_factor_max=1.2,\n",
    "        step=0.0001,\n",
    "        with_colormap=with_colormap\n",
    "    )\n",
    "\n",
    "    # 13) Colorbar facultative\n",
    "    if with_colormap:\n",
    "        create_custom_colorbar(\n",
    "            fig,\n",
    "            df_normalized=df_normalized,\n",
    "            cmap=cmap,\n",
    "            step=0.0001,\n",
    "            spacing_factor_min=1.02,\n",
    "            spacing_factor_max=1.2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2f3fbqW2vSb"
   },
   "outputs": [],
   "source": [
    "def sentiments_heatmaps(apply_horizontal_normalization, sigma, transformed_sentiments, dates):\n",
    "    # Calcul du poids total de chaque topic par jour pour chaque topic_count\n",
    "    total_weight_by_topic_count_topic_and_date = {}\n",
    "    for topic_count, W_matrix in all_nmf_W.items():\n",
    "        for article_num, topic_scores in enumerate(W_matrix):\n",
    "            # Vérification pour éviter les erreurs d'index\n",
    "            if article_num >= len(dates):\n",
    "                continue\n",
    "            article_date = dates[article_num]\n",
    "\n",
    "            # topic_scores est un array / liste de poids. Exemple: [0.2, 0.5, 0.1, ...]\n",
    "            for topic_num, topic_weight in enumerate(topic_scores):\n",
    "                key = (topic_count, topic_num, article_date)\n",
    "                if key not in total_weight_by_topic_count_topic_and_date:\n",
    "                    total_weight_by_topic_count_topic_and_date[key] = topic_weight\n",
    "                else:\n",
    "                    total_weight_by_topic_count_topic_and_date[key] += topic_weight\n",
    "\n",
    "\n",
    "    # Initialisation d'un dictionnaire pour stocker les sentiments normalisés\n",
    "    # par date, topic_count et topic_num\n",
    "    sentiment_by_date_and_topic = {}\n",
    "\n",
    "    for topic_count, W_matrix in all_nmf_W.items():\n",
    "        for article_num, topic_scores in enumerate(W_matrix):\n",
    "            if article_num >= len(dates) or article_num >= len(transformed_sentiments):\n",
    "                continue\n",
    "            article_date = dates[article_num]\n",
    "            sentiment_score = transformed_sentiments[article_num]\n",
    "\n",
    "            for topic_num, topic_weight in enumerate(topic_scores):\n",
    "                adjusted_sentiment_score = sentiment_score * topic_weight\n",
    "\n",
    "                weight_key = (topic_count, topic_num, article_date)\n",
    "                total_weight = total_weight_by_topic_count_topic_and_date.get(weight_key, 0)\n",
    "\n",
    "                if total_weight > 0:\n",
    "                    normalized_sentiment_score = adjusted_sentiment_score / total_weight\n",
    "                else:\n",
    "                    normalized_sentiment_score = 0\n",
    "\n",
    "                combined_key = (topic_count, topic_num, article_date)\n",
    "                if combined_key not in sentiment_by_date_and_topic:\n",
    "                    sentiment_by_date_and_topic[combined_key] = [normalized_sentiment_score]\n",
    "                else:\n",
    "                    sentiment_by_date_and_topic[combined_key].append(normalized_sentiment_score)\n",
    "\n",
    "\n",
    "    # Calcul de la moyenne pour chaque combinaison de (topic_count, topic_num, date)\n",
    "    for key, normalized_sentiments in sentiment_by_date_and_topic.items():\n",
    "        average_sentiment = sum(normalized_sentiments)  # / len(normalized_sentiments) si besoin\n",
    "        sentiment_by_date_and_topic[key] = average_sentiment\n",
    "\n",
    "    # Filtrer les combinaisons avec un score de 0\n",
    "    sentiment_by_date_and_topic = {\n",
    "        k: v for k, v in sentiment_by_date_and_topic.items() if v != 0\n",
    "    }\n",
    "\n",
    "    # Création du dossier de résultats si nécessaire\n",
    "    if not os.path.exists(f\"{results_path}{base_name}_TOPICS_SENTIMENTS_DYNAMICS_HEATMAPS/\"):\n",
    "        os.makedirs(f\"{results_path}{base_name}_TOPICS_SENTIMENTS_DYNAMICS_HEATMAPS/\")\n",
    "\n",
    "    # Boucle principale : on génère la heatmap pour chaque topic_count\n",
    "    for topic_count in all_nmf_W:\n",
    "        # 1) On construit d'abord un dictionnaire (topic_num, date) -> sentiment\n",
    "        filtered_data = {\n",
    "            (topic_num, date): sentiment\n",
    "            for (count, topic_num, date), sentiment in sentiment_by_date_and_topic.items()\n",
    "            if count == topic_count\n",
    "        }\n",
    "\n",
    "        # 2) Conversion en DataFrame\n",
    "        #    On sépare (topic_num, date) en deux colonnes distinctes : Topic et Date\n",
    "        df = pd.DataFrame(list(filtered_data.items()), columns=['Topic_Date', 'Sentiment'])\n",
    "        df[['Topic', 'Date']] = pd.DataFrame(df['Topic_Date'].tolist(), index=df.index)\n",
    "\n",
    "        # 3) On crée un DataFrame de mapping \"Topic -> Label\"\n",
    "        #    (en s'appuyant sur la liste topic_labels_by_config[topic_count])\n",
    "        df_labels = pd.DataFrame({\n",
    "            'Topic': range(len(topic_labels_by_config[topic_count])),\n",
    "            'Topic_label': topic_labels_by_config[topic_count]\n",
    "        })\n",
    "\n",
    "        # 4) On fusionne df et le mapping pour obtenir le label de chaque topic_num\n",
    "        df = df.merge(df_labels, on='Topic', how='left')\n",
    "\n",
    "        # 5) On fait le pivot en utilisant le **label** comme index\n",
    "        df = df.pivot(index=\"Topic_label\", columns=\"Date\", values=\"Sentiment\")\n",
    "\n",
    "        # 6) Interpolation et imputation\n",
    "        df_imputed = df.ffill(axis=1).bfill(axis=1)\n",
    "        df = df_imputed.interpolate(method='linear', axis=1)\n",
    "\n",
    "        # 7) Comme dans le code original, on transpose pour gérer la chronologie\n",
    "        df_transposed = df.T\n",
    "\n",
    "        start_date = df_transposed.index.min()\n",
    "        end_date = df_transposed.index.max()\n",
    "        all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "        # 8) On réindexe pour ne rien perdre, puis on interpole\n",
    "        df_reindexed = df_transposed.reindex(all_dates)\n",
    "        df_interpolated = df_reindexed.interpolate(method='linear')\n",
    "\n",
    "        # 9) Re-transposer : chaque ligne correspond désormais à un label (Topic_label)\n",
    "        df = df_interpolated.T\n",
    "\n",
    "        # NOTE IMPORTANTE :\n",
    "        # À ce stade, l'index de df est constitué des \"Topic_label\", et non plus des topic_num.\n",
    "        # Tu n'as donc plus besoin de faire un `labels_for_my_df = [...]`.\n",
    "        # Les labels SONT déjà dans df.index, donc si on veut un array/list pour le plotting :\n",
    "    #    labels_for_my_df = df.index.to_list()\n",
    "\n",
    "        # Ajustement éventuel de sigma si c'est 'auto'\n",
    "        if sigma == 'auto':\n",
    "            sigma = len(df.columns) / 15\n",
    "\n",
    "        # Plot de la heatmap\n",
    "        plot_custom_heatmap(\n",
    "            df,\n",
    "            cmap='coolwarm',\n",
    "            sigma=sigma,\n",
    "            apply_horizontal_normalization=apply_horizontal_normalization,\n",
    "            with_colormap=True\n",
    "        )\n",
    "\n",
    "        # Sauvegarde de la figure\n",
    "        plt.savefig(\n",
    "            f\"{results_path}{base_name}_TOPICS_SENTIMENTS_DYNAMICS_HEATMAPS/\"\n",
    "            f\"{base_name}_topics_sentiments_dynamics_heatmap_{topic_count}tc_{apply_horizontal_normalization}hn_{int(sigma)}s_\"\n",
    "            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp.png\",\n",
    "            dpi=DPI,\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMlv2EWW2vQE"
   },
   "outputs": [],
   "source": [
    "def compute_text_height_in_data_coords(ax):\n",
    "    \"\"\"\n",
    "    Mesure la hauteur (en 'data coords') d'un texte donné,\n",
    "    via un placement temporaire invisible pour récupérer la bounding box en pixels.\n",
    "    \"\"\"\n",
    "    # 1) On place du texte \"invisible\" (alpha=0) dans l'axe,\n",
    "    #    peu importe où (ici en Axes coords = transAxes).\n",
    "    #    Note : rotation=0 pour mesurer une hauteur \"verticale\" classique.\n",
    "    temp_text = ax.text(\n",
    "        0,\n",
    "        0,\n",
    "        '0123456789',       # Exemple de chaîne un peu longue\n",
    "        rotation=0,\n",
    "        rotation_mode='anchor',\n",
    "        ha='left',\n",
    "        va='bottom',\n",
    "        alpha=0,\n",
    "        transform=ax.transAxes  # On le place en Axes coords\n",
    "    )\n",
    "\n",
    "    # 2) On force un dessin pour que la bounding box soit calculée\n",
    "    ax.figure.canvas.draw()\n",
    "    renderer = ax.figure.canvas.get_renderer()\n",
    "\n",
    "    # 3) On récupère la bounding box en pixels\n",
    "    bbox = temp_text.get_window_extent(renderer=renderer)\n",
    "    pixel_height = bbox.height  # Hauteur en pixels\n",
    "\n",
    "    # 4) Convertir la hauteur (en pixels) -> (en 'data coords' sur l'axe Y) :\n",
    "    #    On regarde le décalage vertical en pixels pour \"1\" unité sur l'axe Y\n",
    "    x0_data, y0_data = ax.transData.transform((0, 0))\n",
    "    x1_data, y1_data = ax.transData.transform((0, 1))\n",
    "    one_unit_in_pixels = y1_data - y0_data\n",
    "\n",
    "    data_height = pixel_height / one_unit_in_pixels\n",
    "\n",
    "    # 5) Nettoyage : on supprime le texte temporaire\n",
    "    temp_text.remove()\n",
    "\n",
    "    return data_height\n",
    "\n",
    "\n",
    "def calc_positions_for_continuous_spacing_Y(ymin, ymax, text_height, spacing_factor):\n",
    "    \"\"\"\n",
    "    Calcule (sans dessiner) les positions Y (en coordonnées 'data')\n",
    "    où l’on placerait chaque label, en avançant de 'text_height * spacing_factor'\n",
    "    tant que le bord \"supérieur\" du label (current_y + text_height)\n",
    "    ne dépasse pas ymax (avec une petite tolérance).\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    # On démarre de ymin + text_height/2 pour centrer le label sur cette position\n",
    "    current_y = ymin + text_height / 2\n",
    "\n",
    "    # Tolérance permettant de s'assurer que le label complet reste dans [ymin..ymax]\n",
    "    tolerance_max = ymax + text_height / 2\n",
    "\n",
    "    while True:\n",
    "        top_edge = current_y + text_height\n",
    "        if top_edge > tolerance_max:\n",
    "            break\n",
    "        positions.append(current_y)\n",
    "        current_y += text_height * spacing_factor\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def find_optimal_continuous_spacing_Y(ymin, ymax, text_height,\n",
    "                                      spacing_factor_min=1.02,\n",
    "                                      spacing_factor_max=1.2,\n",
    "                                      step=0.001):\n",
    "    \"\"\"\n",
    "    Cherche le \"spacing_factor\" optimal (entre spacing_factor_min et spacing_factor_max)\n",
    "    pour maximiser le remplissage de [ymin..ymax] par des labels\n",
    "    espacés de manière continue.\n",
    "    \"\"\"\n",
    "    best_sf = spacing_factor_min\n",
    "    best_diff = float('inf')\n",
    "    best_positions = []\n",
    "\n",
    "    # Comme dans la fonction X, on définit une tolérance similaire\n",
    "    tolerance_max = ymax + text_height / 2\n",
    "    spacing_values = np.arange(spacing_factor_min, spacing_factor_max + step, step)\n",
    "\n",
    "    for sf in spacing_values:\n",
    "        positions = calc_positions_for_continuous_spacing_Y(\n",
    "            ymin=ymin,\n",
    "            ymax=ymax,\n",
    "            text_height=text_height,\n",
    "            spacing_factor=sf\n",
    "        )\n",
    "\n",
    "        # Si aucune position n'est retournée, c'est que sf est trop grand\n",
    "        if not positions:\n",
    "            diff = 9999\n",
    "        else:\n",
    "            last_y = positions[-1]\n",
    "            top_edge = last_y + text_height\n",
    "            diff = abs(tolerance_max - top_edge)\n",
    "\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_sf = sf\n",
    "            best_positions = positions\n",
    "            # Si on est extrêmement proche de la limite, on arrête\n",
    "            if diff < 1e-9:\n",
    "                break\n",
    "\n",
    "    return best_sf, best_positions\n",
    "\n",
    "\n",
    "def manual_tick_placement_continuous_Y(\n",
    "    ax,\n",
    "    ymin,\n",
    "    ymax,\n",
    "    spacing_factor_min=1.02,\n",
    "    spacing_factor_max=1.2,\n",
    "    step=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Place manuellement des pseudo-ticks pour l'axe Y dans [ymin..ymax].\n",
    "    On coupe les ticks officiels et on dessine les labels via ax.text.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) On suppose que vous avez une fonction qui calcule la hauteur\n",
    "    #    d'un label en 'data coords' :\n",
    "    text_height = compute_text_height_in_data_coords(ax)  # À adapter\n",
    "\n",
    "    # 2) Désactiver les ticks \"officiels\" de l'axe Y\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    # 3) Recherche du spacing_factor optimal\n",
    "    best_sf, _ = find_optimal_continuous_spacing_Y(\n",
    "        ymin=ymin,\n",
    "        ymax=ymax,\n",
    "        text_height=text_height,\n",
    "        spacing_factor_min=spacing_factor_min,\n",
    "        spacing_factor_max=spacing_factor_max,\n",
    "        step=step\n",
    "    )\n",
    "\n",
    "    # 4) Placement effectif des positions optimisées\n",
    "    positions = calc_positions_for_continuous_spacing_Y(\n",
    "        ymin=ymin,\n",
    "        ymax=ymax,\n",
    "        text_height=text_height,\n",
    "        spacing_factor=best_sf\n",
    "    )\n",
    "\n",
    "    # 5) Création d'un offset transform pour décaler légèrement le texte\n",
    "    #    vers la gauche (x<0) ou la droite (x>0) en points\n",
    "    offset_axes_transform = mtransforms.offset_copy(\n",
    "        ax.transAxes,\n",
    "        fig=ax.figure,\n",
    "        x=-2.0,   # Décalage à gauche en points (ajustez selon vos besoins)\n",
    "        y=0,\n",
    "        units='points'\n",
    "    )\n",
    "\n",
    "    # Récupération pour la conversion data -> coords Axe\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "\n",
    "    # 6) Dessin de chaque label\n",
    "    for y_val in positions:\n",
    "        # Convertir la coordonnée data -> coordonnée \"Axes\" (entre 0 et 1)\n",
    "        y_val_axes = (y_val - y_min) / (y_max - y_min)\n",
    "\n",
    "        label_str = f\"{y_val:.3f}\"  # Format de l'étiquette (à adapter si besoin)\n",
    "\n",
    "        ax.text(\n",
    "            0.0,               # On place le texte \"à gauche\" du subplot\n",
    "            y_val_axes,\n",
    "            label_str,\n",
    "            rotation=0,        # On peut mettre 0 ou toute autre rotation\n",
    "            rotation_mode='anchor',\n",
    "            ha='right',        # Alignement horizontal à droite\n",
    "            va='center',       # Alignement vertical centré\n",
    "            transform=offset_axes_transform,\n",
    "            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.0')\n",
    "        )\n",
    "\n",
    "    return best_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xi1SPlBnLB3A"
   },
   "outputs": [],
   "source": [
    "def create_box_plots(group_column=None):\n",
    "    # Ce dictionnaire contiendra pour chaque \"n_components\" (nombre de topics),\n",
    "    # la distribution des scores de chaque topic par journal\n",
    "    distri_topics_by_journal_by_num_topic = {}\n",
    "\n",
    "    # Parcours de chaque nombre de composantes (chaque clé de all_nmf_W)\n",
    "    for n_components, W_matrix in all_nmf_W.items():\n",
    "        # Initialiser un dictionnaire pour stocker la distribution des sujets par journal pour ce n_components\n",
    "        distri_topics_by_journal = {}\n",
    "\n",
    "        # W_matrix est une matrice de taille (nb_articles, n_components).\n",
    "        # num_article correspond ici à l'index de la ligne (document) dans la matrice.\n",
    "        for num_article, row_values in enumerate(W_matrix):\n",
    "            # Récupération du \"journal\" selon la source\n",
    "            if source_type == 'europresse':\n",
    "                header = all_soups[num_article].header\n",
    "                journal_text = extract_information(header, '.rdp__DocPublicationName')\n",
    "                journal_text = normalize_journal(journal_text)\n",
    "\n",
    "            elif source_type == 'istex':\n",
    "                journal_text = columns_dict['journal'][num_article]\n",
    "\n",
    "            elif source_type == 'csv':\n",
    "                # Vérification de l'existence de la colonne\n",
    "                if group_column not in columns_dict:\n",
    "                    print(f\"La colonne '{group_column}' n'a pas été trouvée dans le fichier CSV.\")\n",
    "                    return\n",
    "\n",
    "                journal_text = columns_dict[group_column][num_article]\n",
    "\n",
    "            # row_values est un vecteur de scores de longueur n_components,\n",
    "            # chaque \"topic\" est l'index dans ce vecteur.\n",
    "            for topic, score in enumerate(row_values):\n",
    "                # On initialise le sous-dictionnaire si nécessaire\n",
    "                if topic not in distri_topics_by_journal:\n",
    "                    distri_topics_by_journal[topic] = {}\n",
    "\n",
    "                if journal_text not in distri_topics_by_journal[topic]:\n",
    "                    distri_topics_by_journal[topic][journal_text] = []\n",
    "\n",
    "                # Ajout du score dans la liste correspondant à ce journal et ce topic\n",
    "                distri_topics_by_journal[topic][journal_text].append(score)\n",
    "\n",
    "        # On stocke ensuite cette distribution pour le n_components courant\n",
    "        distri_topics_by_journal_by_num_topic[n_components] = distri_topics_by_journal\n",
    "\n",
    "    \"\"\"\n",
    "    Remplace le test de Kruskal-Wallis par un test bootstrap sur les moyennes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Création du dossier principal\n",
    "    if not os.path.exists(f\"{results_path}{base_name}_BOX_PLOTS/\"):\n",
    "        os.makedirs(f\"{results_path}{base_name}_BOX_PLOTS/\")\n",
    "\n",
    "    for num_topic in distri_topics_by_journal_by_num_topic:\n",
    "\n",
    "        # Sous-dossier spécifique au nombre de topics\n",
    "        if not os.path.exists(f\"{results_path}{base_name}_BOX_PLOTS/{base_name}_{num_topic}TC_BOX_PLOTS/\"):\n",
    "            os.makedirs(f\"{results_path}{base_name}_BOX_PLOTS/{base_name}_{num_topic}TC_BOX_PLOTS/\")\n",
    "\n",
    "        for topic in tqdm(distri_topics_by_journal_by_num_topic[num_topic], desc=\"Processing topics\"):\n",
    "            topic_data = distri_topics_by_journal_by_num_topic[num_topic][topic]\n",
    "\n",
    "            # Collecte des données de score pour chaque journal\n",
    "            data = []\n",
    "            journals = []  # Pour les étiquettes\n",
    "            for journal, scores in topic_data.items():\n",
    "                data.extend(scores)\n",
    "                journals.extend([journal] * len(scores))\n",
    "\n",
    "            # Création d'un DataFrame pour Seaborn\n",
    "            df = pd.DataFrame({'Journal': journals, 'Score': data})\n",
    "\n",
    "            # Filtrer les journaux (ceux qui ont au moins \"threshold\" valeurs)\n",
    "            journal_counts = df['Journal'].value_counts()\n",
    "            journals_to_keep = journal_counts[journal_counts >= threshold].index\n",
    "            df = df[df['Journal'].isin(journals_to_keep)]\n",
    "\n",
    "            # Test bootstrap (si au moins 2 groupes)\n",
    "            if len(set(df['Journal'])) < 2:\n",
    "                print(\"Pas assez de groupes pour effectuer le test bootstrap pour ce sujet et topic\")\n",
    "                continue\n",
    "\n",
    "            # Journaux uniques\n",
    "            unique_journals = df['Journal'].unique()\n",
    "\n",
    "            # Calcul de la hauteur de la figure\n",
    "            figure_height_inch = (len(unique_journals) * PX_PER_TOPIC) / DPI\n",
    "\n",
    "            # Figure avec un sous-axe par journal\n",
    "            fig, axes = plt.subplots(len(unique_journals), 1,\n",
    "                                     figsize=(FIGURE_WIDTH_INCH, figure_height_inch),\n",
    "                                     dpi=DPI, sharex=True)\n",
    "\n",
    "            # On trie les journaux par moyenne\n",
    "            mean_scores = df.groupby('Journal')['Score'].mean().sort_values(ascending=False)\n",
    "            sorted_journals = mean_scores.index.tolist()\n",
    "\n",
    "            # Plot de chaque boxplot\n",
    "            # Si un seul journal, axes n'est pas un array => on le transforme en liste\n",
    "            if len(unique_journals) == 1:\n",
    "                axes = [axes]\n",
    "\n",
    "            for i, journal in enumerate(sorted_journals):\n",
    "                # Boxplot\n",
    "                sns.boxplot(\n",
    "                    x='Score',\n",
    "                    data=df[df['Journal'] == journal],\n",
    "                    ax=axes[i],\n",
    "                    whis=[0, 100],\n",
    "                    showmeans=True,\n",
    "                    width=0.98,\n",
    "                    meanprops={\n",
    "                        'marker': '|',\n",
    "                        'markeredgecolor': 'red',\n",
    "                        'markeredgewidth': 5,\n",
    "                        'markersize': 16\n",
    "                    },\n",
    "                    boxprops={\n",
    "                        'facecolor': (0.0, 0.2, 0.8),\n",
    "                        'edgecolor': (0.0, 0.2, 0.8)\n",
    "                    },\n",
    "                    medianprops={\n",
    "                        'color': 'none',\n",
    "                        'linewidth': 10\n",
    "                    },\n",
    "                    whiskerprops={\n",
    "                        'color': 'black',\n",
    "                        'linewidth': 2\n",
    "                    },\n",
    "                    capprops={\n",
    "                        'color': 'none',\n",
    "                        'linewidth': 0\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # Ajustement des x-lims\n",
    "                axes[i].set_xlim(left=0, right=df['Score'].max())\n",
    "\n",
    "                # Masque l'axe X pour tous sauf le dernier\n",
    "                if i < len(unique_journals) - 1:\n",
    "                    axes[i].xaxis.set_visible(False)\n",
    "                else:\n",
    "                    # Placement manuel des ticks (exemple de fonction custom que vous aviez)\n",
    "                    manual_tick_placement_continuous(\n",
    "                        ax=axes[i],\n",
    "                        xmin=0,\n",
    "                        xmax=df['Score'].max(),\n",
    "                        spacing_factor_min=1.02,\n",
    "                        spacing_factor_max=1.2,\n",
    "                        step=0.001\n",
    "                    )\n",
    "\n",
    "                # Retirer y-label et y-ticks\n",
    "                axes[i].set_ylabel('')\n",
    "                axes[i].set_yticks([])\n",
    "                axes[i].set_xticks([])\n",
    "\n",
    "                offset_axes_transform = mtransforms.offset_copy(\n",
    "                    axes[i].transAxes,\n",
    "                    fig=axes[i].figure,\n",
    "                    x=-3.0,\n",
    "                    y=0.0,\n",
    "                    units='points'\n",
    "                )\n",
    "\n",
    "                # Petit label à gauche (nom du journal)\n",
    "                axes[i].text(\n",
    "                    0,\n",
    "                    0.5,\n",
    "                    journal,\n",
    "                    ha='right',\n",
    "                    va='center',\n",
    "                    transform=offset_axes_transform\n",
    "                )\n",
    "\n",
    "            sns.despine(left=True, bottom=True)\n",
    "\n",
    "            # Sauvegarde de la figure\n",
    "            plt.savefig(\n",
    "                f\"{results_path}{base_name}_BOX_PLOTS/{base_name}_{num_topic}TC_BOX_PLOTS/\"\n",
    "                f\"{base_name}_{num_topic}tc_{topic_labels_by_config[num_topic][topic]}_\"\n",
    "                f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n",
    "                f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_\"\n",
    "                f\"{threshold}thr_journals_boxplots.png\",\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0\n",
    "            )\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # Ajout import spacy\n",
    "import multiprocessing\n",
    "import tempfile\n",
    "import os\n",
    "import unidecode # Assurez-vous que cette bibliothèque est installée\n",
    "import numpy as np # Assurez-vous que cette bibliothèque est installée\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# Note: cosine_similarity n'est pas utilisé dans le code fourni pour refactoring,\n",
    "# mais est présent dans extract_relevant_sentences_and_titles que je conserve tel quel.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- Étape 1 (Révisée): Traitement SpaCy unique et collecte de données ---\n",
    "# Renommée pour clarifier son rôle interne et éviter confusion avec l'originale\n",
    "def _process_documents_single_pass(\n",
    "    documents,\n",
    "    nlp_pipeline,\n",
    "    sentence_file_path,\n",
    "    gclasses, # Filtre POS (set)\n",
    "    spacy_stopwords, # Filtre Stopwords (set)\n",
    "    min_len=3 # Filtre longueur minimale\n",
    "    ):\n",
    "    \"\"\"\n",
    "    (Interne) Processes documents via spaCy ONCE, applies filters, writes sentences\n",
    "    to file, captures offsets, and returns structured sentence data.\n",
    "\n",
    "    Args:\n",
    "        documents (iterable): Raw documents.\n",
    "        nlp_pipeline: Loaded spaCy pipeline.\n",
    "        sentence_file_path (str): Path to the file to store normalized sentences.\n",
    "        gclasses (set): SET of allowed POS tags to KEEP.\n",
    "        spacy_stopwords (set): SET of spaCy stopwords.\n",
    "        min_len (int): Minimum lemma length to keep.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (processed_sentences_data, sentence_offsets)\n",
    "            processed_sentences_data (list): List of dicts per sentence.\n",
    "            sentence_offsets (list): List of byte offsets.\n",
    "            Returns ([], []) on error.\n",
    "    \"\"\"\n",
    "    batch_size = 4 # Ajuster selon mémoire GPU/CPU\n",
    "    n_process = 1   # Défaut pour GPU, ajusté si CPU\n",
    "\n",
    "    # Vérifier le device du pipeline pour ajuster le parallélisme\n",
    "    if hasattr(nlp_pipeline, 'device') and nlp_pipeline.device.type == 'cuda':\n",
    "        print(f\"SpaCy pipeline operating on GPU: {nlp_pipeline.device}.\")\n",
    "        n_process = 1 # Le parallélisme n_process est moins pertinent/peut nuire sur GPU\n",
    "    else:\n",
    "        cpu_count = multiprocessing.cpu_count()\n",
    "        print(f\"SpaCy pipeline operating on CPU. Using {cpu_count} processes for nlp.pipe.\")\n",
    "        n_process = cpu_count\n",
    "\n",
    "    global_sentence_index = 0\n",
    "    processed_sentences_data = []\n",
    "    sentence_offsets = []\n",
    "\n",
    "    try:\n",
    "        with open(sentence_file_path, 'w', encoding='utf-8') as f_sent:\n",
    "            total_docs = None\n",
    "            try:\n",
    "                total_docs = len(documents)\n",
    "            except TypeError:\n",
    "                print(\"Input 'documents' is a generator/iterator; tqdm total cannot be determined.\")\n",
    "\n",
    "            print(f\"Starting single spaCy processing pass...\")\n",
    "            with tqdm(total=total_docs, desc='Processing Docs (spaCy)') as pbar:\n",
    "                # --- SINGLE SPACY PASS ---\n",
    "                for doc_index, spacy_doc in enumerate(nlp_pipeline.pipe(documents, n_process=n_process, batch_size=batch_size)):\n",
    "                    for sent in spacy_doc.sents:\n",
    "                        sentence_norm_tokens = []\n",
    "                        sentence_filtered_lemmas = []\n",
    "\n",
    "                        for token in sent:\n",
    "                            if token.is_space or not token.text.strip():\n",
    "                                continue\n",
    "\n",
    "                            lemma = token.lemma_.lower()\n",
    "                            pos = token.pos_\n",
    "\n",
    "                            if pos == 'PROPN':\n",
    "                                try:\n",
    "                                     # Utiliser unidecode si disponible\n",
    "                                     lemma = unidecode.unidecode(lemma)\n",
    "                                except NameError:\n",
    "                                     # Si unidecode n'est pas importé ou disponible, on garde le lemme original\n",
    "                                     pass # ou print un warning une seule fois\n",
    "\n",
    "                            passes_filter = (\n",
    "                                pos in gclasses and\n",
    "                                lemma not in spacy_stopwords and\n",
    "                                len(lemma) >= min_len\n",
    "                            )\n",
    "\n",
    "                            sentence_norm_tokens.append(token.norm_)\n",
    "                            if passes_filter:\n",
    "                                sentence_filtered_lemmas.append(lemma)\n",
    "\n",
    "                        if sentence_norm_tokens:\n",
    "                            sentence_norm_text = \" \".join(sentence_norm_tokens)\n",
    "                            try:\n",
    "                                current_offset = f_sent.tell()\n",
    "                                f_sent.write(sentence_norm_text + \"\\n\")\n",
    "\n",
    "                                # Store data for this sentence\n",
    "                                sentence_data = {\n",
    "                                    'doc_id': doc_index,\n",
    "                                    'sent_id_global': global_sentence_index,\n",
    "                                    'offset': current_offset,\n",
    "                                    'filtered_lemmas': sentence_filtered_lemmas,\n",
    "                                    # 'norm_text': sentence_norm_text, # On peut omettre si non strictement nécessaire après\n",
    "                                }\n",
    "                                processed_sentences_data.append(sentence_data)\n",
    "                                sentence_offsets.append(current_offset)\n",
    "                                global_sentence_index += 1\n",
    "\n",
    "                            except IOError as e:\n",
    "                                print(f\"Error writing to sentence file: {e}. Stopping processing.\")\n",
    "                                return [], []\n",
    "\n",
    "                    if pbar.total is not None:\n",
    "                        pbar.update(1)\n",
    "                    else:\n",
    "                         # Mettre à jour même sans total, pour montrer une activité\n",
    "                         pbar.set_description(f\"Processing Doc Index {doc_index}\")\n",
    "                         pbar.update(1)\n",
    "\n",
    "\n",
    "    except IOError as e:\n",
    "         print(f\"Error opening or writing to sentence file '{sentence_file_path}': {e}\")\n",
    "         return [], []\n",
    "\n",
    "    print(f\"SpaCy processing complete. Collected data for {len(processed_sentences_data)} sentences.\")\n",
    "    return processed_sentences_data, sentence_offsets\n",
    "\n",
    "\n",
    "# --- Étape 2 (Révisée): Générateur de flux de tokens depuis les données collectées ---\n",
    "# Renommée pour correspondre à la nouvelle entrée\n",
    "def generate_token_stream_from_data(processed_data, level='sentence'):\n",
    "    \"\"\"\n",
    "    Generator yielding lists of PRE-FILTERED tokens from the collected sentence data.\n",
    "    Aggregates if level='document'.\n",
    "\n",
    "    Args:\n",
    "        processed_data (list): List of dicts returned by _process_documents_single_pass.\n",
    "        level (str): 'sentence' or 'document'.\n",
    "\n",
    "    Yields:\n",
    "        list: List of pre-filtered tokens for a sentence or document.\n",
    "    \"\"\"\n",
    "    if level == 'sentence':\n",
    "        for sentence_data in processed_data:\n",
    "            yield sentence_data['filtered_lemmas']\n",
    "    elif level == 'document':\n",
    "        current_doc_id = -1\n",
    "        current_doc_tokens = []\n",
    "        for sentence_data in processed_data:\n",
    "            doc_id = sentence_data['doc_id']\n",
    "            if doc_id != current_doc_id and current_doc_id != -1:\n",
    "                yield current_doc_tokens\n",
    "                current_doc_tokens = []\n",
    "            current_doc_id = doc_id\n",
    "            current_doc_tokens.extend(sentence_data['filtered_lemmas'])\n",
    "        if current_doc_tokens: # Yield the last document's tokens\n",
    "            yield current_doc_tokens\n",
    "    else:\n",
    "        raise ValueError(\"level must be 'sentence' or 'document'\")\n",
    "\n",
    "\n",
    "# --- Étape 3 (Révisée): Vectorisation TF-IDF depuis les données collectées ---\n",
    "# Renommée pour correspondre à la nouvelle entrée\n",
    "def vectorize_corpus_from_data(processed_data, level='sentence', num_items=None):\n",
    "    \"\"\"\n",
    "    Vectorizes the corpus (sentences or documents) into TF-IDF\n",
    "    using pre-filtered tokens from the processed_data list.\n",
    "\n",
    "    Args:\n",
    "        processed_data (list): List of dicts from _process_documents_single_pass.\n",
    "        level (str): 'sentence' or 'document'.\n",
    "        num_items (int, optional): Expected number of items (sentences/docs) for tqdm.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_tfidf, tfidf_feature_names)\n",
    "               Returns (None, []) on error or empty input.\n",
    "    \"\"\"\n",
    "    print(f\"Starting TF-IDF vectorization (level: {level})...\")\n",
    "    if not processed_data:\n",
    "        print(f\"Warning: No processed data provided for vectorization (level: {level}).\")\n",
    "        return None, []\n",
    "\n",
    "    def identity_analyzer(tokens):\n",
    "        return tokens\n",
    "\n",
    "    count_vectorizer = CountVectorizer(analyzer=identity_analyzer, lowercase=False, min_df=2)\n",
    "    tfidf_transformer = TfidfTransformer(norm='l2', sublinear_tf=False, smooth_idf=True)\n",
    "\n",
    "    token_stream = generate_token_stream_from_data(processed_data, level)\n",
    "\n",
    "    print(f\"Applying CountVectorizer (fit_transform) for level '{level}'...\")\n",
    "    X_counts = None\n",
    "    try:\n",
    "        if num_items is not None:\n",
    "            if num_items == 0:\n",
    "                 print(f\"Warning: num_items is 0 for level '{level}', vectorization will yield empty results.\")\n",
    "                 X_counts = count_vectorizer.fit_transform([])\n",
    "            else:\n",
    "                 token_stream_with_tqdm = tqdm(token_stream, total=num_items, desc=f'Vectorizing {level}s')\n",
    "                 X_counts = count_vectorizer.fit_transform(token_stream_with_tqdm)\n",
    "        else:\n",
    "            X_counts = count_vectorizer.fit_transform(token_stream)\n",
    "    except ValueError as e:\n",
    "         print(f\"Error during CountVectorizer for level '{level}': {e}\")\n",
    "         return None, []\n",
    "\n",
    "    if X_counts is None: # Should not happen if try/except is fine, but safe check\n",
    "         print(f\"Error: CountVectorizer failed to produce output for level '{level}'.\")\n",
    "         return None, []\n",
    "\n",
    "    print(f\"Counts matrix shape for level '{level}': {X_counts.shape}\")\n",
    "    if X_counts.shape[0] == 0 and num_items != 0:\n",
    "         print(f\"Warning: Counts matrix is empty for level '{level}' despite expecting {num_items} items.\")\n",
    "\n",
    "    X_tfidf = None\n",
    "    if X_counts.shape[0] > 0 or X_counts.shape[1] > 0 :\n",
    "        print(f\"Applying TfidfTransformer (fit_transform) for level '{level}'...\")\n",
    "        X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "        print(f\"TF-IDF matrix shape for level '{level}': {X_tfidf.shape}\")\n",
    "    else:\n",
    "        print(f\"Skipping TfidfTransformer for level '{level}' as Counts matrix is empty.\")\n",
    "        X_tfidf = X_counts.copy().astype(np.float64) # Return empty float matrix\n",
    "\n",
    "    tfidf_feature_names = count_vectorizer.get_feature_names_out()\n",
    "    print(f\"Vocabulary size for level '{level}': {len(tfidf_feature_names)}\")\n",
    "    if len(tfidf_feature_names) == 0:\n",
    "         print(f\"Warning: Vocabulary is empty for level '{level}'.\")\n",
    "\n",
    "    if X_tfidf is not None and X_tfidf.shape[1] != len(tfidf_feature_names):\n",
    "         print(f\"CRITICAL WARNING: TF-IDF matrix columns ({X_tfidf.shape[1]}) mismatch vocabulary size ({len(tfidf_feature_names)}) for level '{level}'.\")\n",
    "\n",
    "    return X_tfidf, list(tfidf_feature_names)\n",
    "\n",
    "\n",
    "# --- Étape 4: Récupération de phrase depuis le fichier (INCHANGÉE PAR RAPPORT À VOTRE ORIGINAL) ---\n",
    "# Fonction conservée telle que fournie initialement\n",
    "def get_sentence_from_file(global_sentence_index, sentence_file_path, sentence_offsets):\n",
    "    \"\"\"Reads a specific sentence from the text file using its global index and offsets.\"\"\"\n",
    "    if global_sentence_index < 0 or global_sentence_index >= len(sentence_offsets):\n",
    "        print(f\"Warning: Invalid sentence index requested: {global_sentence_index}\")\n",
    "        return None\n",
    "    if not os.path.exists(sentence_file_path):\n",
    "        print(f\"Error: Sentence file not found at '{sentence_file_path}'\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(sentence_file_path, 'r', encoding='utf-8') as f:\n",
    "            f.seek(sentence_offsets[global_sentence_index])\n",
    "            line = f.readline()\n",
    "            return line.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading sentence {global_sentence_index} from {sentence_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Étape 5: Extraction des phrases et titres (INCHANGÉE PAR RAPPORT À VOTRE ORIGINAL) ---\n",
    "# Fonction conservée telle que fournie initialement.\n",
    "# Note: Elle dépend de variables (nmf_models, language, preprompt, fonctions GPT)\n",
    "# qui ne sont pas définies ou retournées par le workflow principal ci-dessous,\n",
    "# mais je la garde pour la cohérence avec votre code source.\n",
    "def extract_relevant_sentences_and_titles(\n",
    "    nmf_models, X_sentences, sentence_file_path, sentence_offsets,\n",
    "    language, preprompt, call_gpt4o_mini, count_tokens, check_and_wait_if_needed\n",
    "    ):\n",
    "    \"\"\"Extracts relevant sentences and generates titles (adapted for disk lookup).\"\"\"\n",
    "    # (Code identique à celui fourni dans votre première question)\n",
    "    print(\"Extracting relevant sentences and generating titles...\")\n",
    "    titles_per_num_topic = {}\n",
    "    all_previous_titles = []\n",
    "\n",
    "    # Vérifier si X_sentences est valide\n",
    "    if X_sentences is None or X_sentences.shape[0] == 0:\n",
    "        print(\"Warning: X_sentences matrix is empty or None in extract_relevant_sentences. Skipping.\")\n",
    "        return {}\n",
    "\n",
    "    num_sentences = X_sentences.shape[0]\n",
    "    if num_sentences != len(sentence_offsets):\n",
    "         print(f\"CRITICAL WARNING: Mismatch between X_sentences rows ({num_sentences}) and sentence offsets ({len(sentence_offsets)}) in extract_relevant_sentences. Results might be incorrect.\")\n",
    "\n",
    "    # Assurer que nmf_models est un dictionnaire\n",
    "    if not isinstance(nmf_models, dict):\n",
    "        print(f\"Error: nmf_models should be a dictionary, but got {type(nmf_models)}. Skipping.\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "    for num_topic_components, nmf_model in nmf_models.items():\n",
    "        print(f\"\\nProcessing NMF model with {num_topic_components} topics...\")\n",
    "        if nmf_model is None:\n",
    "             print(f\"  Skipping NMF model for {num_topic_components} topics as it is None.\")\n",
    "             continue\n",
    "        try:\n",
    "            # Vérifier la compatibilité avant transform\n",
    "            if hasattr(nmf_model, 'n_features_in_') and nmf_model.n_features_in_ != X_sentences.shape[1]:\n",
    "                 print(f\"  Error: NMF model features ({nmf_model.n_features_in_}) mismatch X_sentences features ({X_sentences.shape[1]}). Skipping.\")\n",
    "                 continue\n",
    "            score_phrases = nmf_model.transform(X_sentences)\n",
    "            n_topics = nmf_model.n_components_ # Attribut post-fit\n",
    "        except AttributeError:\n",
    "             print(f\"  Error: NMF model for {num_topic_components} seems invalid or not fitted (missing attributes). Skipping.\")\n",
    "             continue\n",
    "        except Exception as e:\n",
    "            print(f\"  Error applying transform with NMF model {num_topic_components}: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        final_top_phrases_per_topic = []\n",
    "        top_n = 20\n",
    "        candidate_size = 100\n",
    "        similarity_threshold = 0.8\n",
    "\n",
    "        for topic_idx in range(n_topics):\n",
    "            topic_scores = score_phrases[:, topic_idx]\n",
    "            actual_candidate_size = min(candidate_size, num_sentences)\n",
    "            if actual_candidate_size <= 0: continue\n",
    "\n",
    "            try:\n",
    "                 # Indices triés pour les candidats (du plus haut score au plus bas)\n",
    "                 top_indices_candidate = np.argsort(topic_scores)[-actual_candidate_size:][::-1]\n",
    "                 # Assurer que les indices sont valides\n",
    "                 if top_indices_candidate.max() >= num_sentences:\n",
    "                      top_indices_candidate = top_indices_candidate[top_indices_candidate < num_sentences]\n",
    "                      if len(top_indices_candidate) == 0: continue\n",
    "            except IndexError:\n",
    "                 print(f\"  IndexError getting candidate indices for topic {topic_idx+1}.\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            try:\n",
    "                 candidate_vectors = X_sentences[top_indices_candidate]\n",
    "            except IndexError:\n",
    "                 print(f\"  IndexError accessing X_sentences rows for topic {topic_idx+1}.\")\n",
    "                 continue\n",
    "            except Exception as e:\n",
    "                 print(f\"  Error accessing X_sentences rows for topic {topic_idx+1}: {e}\")\n",
    "                 continue\n",
    "\n",
    "            # --- Similarity filtering ---\n",
    "            selected_indices_in_candidate_list = []\n",
    "            if candidate_vectors.shape[0] > 0:\n",
    "                for i in range(len(top_indices_candidate)):\n",
    "                    vec_i = candidate_vectors[i:i+1]\n",
    "                    if not selected_indices_in_candidate_list:\n",
    "                        selected_indices_in_candidate_list.append(i); continue\n",
    "                    selected_vectors = candidate_vectors[selected_indices_in_candidate_list]\n",
    "                    is_similar = False\n",
    "                    try:\n",
    "                         similarities = cosine_similarity(vec_i, selected_vectors)\n",
    "                         if np.any(similarities >= similarity_threshold): is_similar = True\n",
    "                    except Exception as e:\n",
    "                         print(f\"    Warning: Cosine similarity calculation error: {e}\")\n",
    "                         # is_similar = True # Option conservatrice\n",
    "                    if not is_similar: selected_indices_in_candidate_list.append(i)\n",
    "                    if len(selected_indices_in_candidate_list) >= top_n: break\n",
    "\n",
    "            # --- Retrieve sentences and scores ---\n",
    "            sub_array = []\n",
    "            for idx_in_candidates in selected_indices_in_candidate_list:\n",
    "                global_sentence_idx = top_indices_candidate[idx_in_candidates]\n",
    "                phrase_brute = get_sentence_from_file(global_sentence_idx, sentence_file_path, sentence_offsets)\n",
    "                if phrase_brute is None: continue\n",
    "                score_value = topic_scores[global_sentence_idx]\n",
    "                sub_array.append((phrase_brute, round(float(score_value), 4)))\n",
    "            final_top_phrases_per_topic.append(sub_array)\n",
    "\n",
    "        # --- Generate Titles (GPT Logic - Placeholder comme dans l'original) ---\n",
    "        # Vérifier si les fonctions GPT sont disponibles et valides\n",
    "        gpt_available = all(callable(f) for f in [call_gpt4o_mini, count_tokens, check_and_wait_if_needed])\n",
    "        if not gpt_available:\n",
    "             print(f\"  Warning: GPT utility functions not available/callable. Skipping title generation for k={num_topic_components}.\")\n",
    "             titles_per_num_topic[num_topic_components] = [f\"Error: GPT unavailable\" for _ in range(n_topics)]\n",
    "             continue # Ne pas essayer de générer les titres\n",
    "\n",
    "        print(f\"  Generating titles for {n_topics} topics...\") # Message gardé de l'original\n",
    "        topic_titles = []\n",
    "        for topic_idx, sub_array in enumerate(tqdm(final_top_phrases_per_topic, desc=f\"Generating Titles (k={num_topic_components})\", leave=False)):\n",
    "            if not sub_array:\n",
    "                topic_titles.append(f\"Error: No sentences for Topic {topic_idx + 1}\")\n",
    "                continue\n",
    "\n",
    "            hierarchical_sentences = \"\\n\".join(f\"- {item[0]} (score: {item[1]})\" for item in sub_array)\n",
    "            previous_titles_text = \"\\n\".join(f\"- {t}\" for t in all_previous_titles) or \"(none so far)\"\n",
    "\n",
    "            # --- Construct prompt based on language ---\n",
    "            if language == 'fr':\n",
    "                prompt = f\"{preprompt}\\nTitres déjà utilisés:\\n{previous_titles_text}\\n\\nPhrases:\\n{hierarchical_sentences}\\n\\nTa tâche : ... (Reste du prompt FR)\"\n",
    "            elif language == 'en': # Ajout support 'en' comme vu dans l'original\n",
    "                prompt = f\"{preprompt}\\nAlready used titles:\\n{previous_titles_text}\\n\\nSentences:\\n{hierarchical_sentences}\\n\\nYour task: ... (Rest of EN prompt)\"\n",
    "            else:\n",
    "                 prompt = f\"Language {language} not fully configured for prompt. Using basic format.\\nSentences:\\n{hierarchical_sentences}\"\n",
    "\n",
    "            # --- Call GPT ---\n",
    "            try:\n",
    "                token_count = count_tokens(prompt)\n",
    "                check_and_wait_if_needed(token_count)\n",
    "                new_title = call_gpt4o_mini(prompt).strip()\n",
    "                if ':' not in new_title or len(new_title.split(':', 1)[0].strip()) == 0 or len(new_title.split(':', 1)[1].strip()) == 0:\n",
    "                     print(f\"    Warning: Title '{new_title}' doesn't match 'X: Y' format.\")\n",
    "                topic_titles.append(new_title)\n",
    "                all_previous_titles.append(new_title)\n",
    "            except Exception as e:\n",
    "                 print(f\"    Error calling GPT for topic {topic_idx+1}: {e}\")\n",
    "                 topic_titles.append(f\"Error: Title generation failed\")\n",
    "\n",
    "        titles_per_num_topic[num_topic_components] = topic_titles\n",
    "\n",
    "    return titles_per_num_topic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import spacy # Assurez-vous que spacy est importé\n",
    "import multiprocessing\n",
    "import tempfile\n",
    "import os\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity # Gardé pour extract_relevant_sentences...\n",
    "from gensim.corpora import Dictionary # <-- AJOUT IMPORTANT\n",
    "\n",
    "# --- Supposons que les autres fonctions (_process_documents_single_pass, etc.) existent ---\n",
    "# ... (Collez ici les définitions des autres fonctions si nécessaire) ...\n",
    "\n",
    "import spacy\n",
    "import tempfile\n",
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "# Assurez-vous que _process_documents_single_pass et vectorize_corpus_from_data\n",
    "# sont définis ailleurs et importés correctement.\n",
    "# from your_module import _process_documents_single_pass, vectorize_corpus_from_data\n",
    "\n",
    "# --- Orchestration Principale (Révisée avec Dictionnaire Gensim et Tokenized Corpus) ---\n",
    "def main_workflow_single_pass(\n",
    "    documents,           # Iterable de textes bruts\n",
    "    gclasses             # Liste ou set des tags POS à garder\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Main workflow using a single spaCy pass and simplified on-the-fly filtering.\n",
    "    Generates document/sentence TF-IDF matrices, a Gensim Dictionary, and a tokenized corpus.\n",
    "\n",
    "    Args:\n",
    "        documents (iterable): Raw documents.\n",
    "        nlp_pipeline: Pre-loaded spaCy pipeline.\n",
    "        gclasses (list or set): Allowed POS tags.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_docs, feature_names_doc, X_sentences, feature_names_sent,\n",
    "                gensim_dictionary, tokenized_corpus, # <-- Ajouté tokenized_corpus\n",
    "                sentence_file_path, sentence_offsets)\n",
    "               Returns (None, [], None, [], None, [], None, []) on critical errors. # <-- Mis à jour (8 éléments)\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Main Workflow (Single SpaCy Pass) ---\")\n",
    "    # Vérification essentielle de nlp_pipeline\n",
    "    if not nlp_pipeline:\n",
    "        print(\"Error: spaCy nlp_pipeline is not provided.\")\n",
    "        # Retourner le bon nombre d'éléments None/[] (8 maintenant)\n",
    "        return None, [], None, [], None, [], None, []\n",
    "\n",
    "    # --- Préférence GPU ---\n",
    "    # (Votre code pour la préférence GPU reste identique)\n",
    "    try:\n",
    "        print(\"Attempting to set spaCy GPU preference...\")\n",
    "        spacy.prefer_gpu()\n",
    "        if spacy.require_gpu():\n",
    "            print(\"-> SpaCy GPU preference successful.\")\n",
    "        else:\n",
    "            print(\"-> SpaCy GPU preference set, but no compatible GPU detected or required.\")\n",
    "    except Exception as e:\n",
    "         print(f\"-> Could not set/check spaCy GPU preference: {e}. Relying on pipeline's loaded device.\")\n",
    "\n",
    "    device_info = \"CPU (default)\"\n",
    "    if hasattr(nlp_pipeline, 'device'):\n",
    "        # Note: 'device' peut ne pas être un attribut direct standard.\n",
    "        # Vous pourriez avoir besoin d'inspecter nlp_pipeline.config ou similaire\n",
    "        # selon comment il a été chargé. Adaptez si nécessaire.\n",
    "        # Pour les transformers, c'est souvent via le modèle interne.\n",
    "        # Exemple simplifié :\n",
    "        try:\n",
    "             # Essayez d'accéder au device via une méthode commune si possible\n",
    "             if hasattr(nlp_pipeline, 'pipe_names') and 'transformer' in nlp_pipeline.pipe_names:\n",
    "                 device_info = nlp_pipeline.get_pipe('transformer').model.device\n",
    "             elif hasattr(nlp_pipeline, '_device'): # Certains pipelines peuvent l'avoir\n",
    "                  device_info = nlp_pipeline._device\n",
    "             else:\n",
    "                 # Tente une vérification générale (peut échouer)\n",
    "                 if any(p.has_attr(\"model\") and p.model.has_attr(\"device\") for p in nlp_pipeline.pipeline):\n",
    "                     device_info = next(p.model.device for p in nlp_pipeline.pipeline if p.has_attr(\"model\") and p.model.has_attr(\"device\"))\n",
    "                 else:\n",
    "                     device_info = \"Unknown (check pipeline config)\"\n",
    "        except Exception:\n",
    "             device_info = \"Unknown (inspection failed)\"\n",
    "\n",
    "    print(f\"Using provided spaCy pipeline potentially configured for device: {device_info}\")\n",
    "\n",
    "\n",
    "    # --- Préparation ---\n",
    "    spacy_stopwords = nlp_pipeline.Defaults.stop_words if hasattr(nlp_pipeline, 'Defaults') else set() # Default vide si non trouvé\n",
    "    gclasses_set = set(gclasses)\n",
    "    sentence_file_path = None # Initialiser\n",
    "\n",
    "    try:\n",
    "        # Créer un fichier temporaire\n",
    "        temp_sentence_fd, sentence_file_path = tempfile.mkstemp(suffix=\".txt\", prefix=\"sentences_\", text=True)\n",
    "        os.close(temp_sentence_fd)\n",
    "        print(f\"Temporary sentence file created at: {sentence_file_path}\")\n",
    "\n",
    "        # --- Étape 1: Traitement SpaCy Unique ---\n",
    "        print(\"\\n--- Running Single SpaCy Processing and Data Collection ---\")\n",
    "        # Assurez-vous que _process_documents_single_pass retourne bien une structure\n",
    "        # comme: [{'doc_id': int, 'sent_id': int, 'filtered_lemmas': list[str], ...}, ...]\n",
    "        processed_data, sentence_offsets = _process_documents_single_pass(\n",
    "            documents,\n",
    "            nlp_pipeline, # Passe le pipeline à la fonction de traitement\n",
    "            sentence_file_path,\n",
    "            gclasses_set,\n",
    "            spacy_stopwords\n",
    "        )\n",
    "\n",
    "        if not processed_data or not sentence_offsets:\n",
    "            # Si le fichier temporaire a été créé mais le traitement échoue, le supprimer\n",
    "            if sentence_file_path and os.path.exists(sentence_file_path):\n",
    "                 os.remove(sentence_file_path)\n",
    "                 print(f\"Cleaned up temporary file due to processing failure: {sentence_file_path}\")\n",
    "            raise ValueError(\"SpaCy processing yielded no data or offsets. Cannot proceed.\")\n",
    "        print(f\"Successfully processed {len(sentence_offsets)} sentences.\")\n",
    "\n",
    "        # Calculer num_docs et num_sentences pour tqdm et la création de tokenized_corpus\n",
    "        num_sentences = len(processed_data)\n",
    "        num_docs = 0\n",
    "        if processed_data:\n",
    "             # +1 car les IDs commencent souvent à 0\n",
    "             num_docs = max(item['doc_id'] for item in processed_data) + 1\n",
    "        print(f\"Found {num_docs} documents in the processed data.\")\n",
    "\n",
    "        # --- NOUVELLE ÉTAPE: Création du Dictionnaire Gensim ET du Tokenized Corpus ---\n",
    "        print(\"\\n--- Creating Gensim Dictionary and Tokenized Corpus (Document Level) ---\")\n",
    "\n",
    "        # Initialiser une structure pour regrouper les tokens par document\n",
    "        # Une liste de listes, indexée par doc_id\n",
    "        doc_tokens_grouped = [[] for _ in range(num_docs)]\n",
    "\n",
    "        # Itérer sur les données traitées (phrases) pour construire le dictionnaire\n",
    "        # et regrouper les tokens par document en même temps.\n",
    "        lemma_lists_for_gensim = [] # Pour le dictionnaire Gensim\n",
    "        for sentence_data in processed_data:\n",
    "            doc_id = sentence_data['doc_id']\n",
    "            filtered_lemmas = sentence_data['filtered_lemmas']\n",
    "\n",
    "            # Ajouter les lemmes de cette phrase au bon document dans doc_tokens_grouped\n",
    "            if 0 <= doc_id < num_docs: # Vérification de sécurité\n",
    "                doc_tokens_grouped[doc_id].extend(filtered_lemmas)\n",
    "            else:\n",
    "                print(f\"Warning: Encountered unexpected doc_id {doc_id}. Max expected was {num_docs-1}. Skipping for tokenized_corpus.\")\n",
    "\n",
    "            # Ajouter la liste des lemmes de la phrase pour le dictionnaire Gensim\n",
    "            lemma_lists_for_gensim.append(filtered_lemmas)\n",
    "\n",
    "        # 1. Construire le tokenized_corpus (maintenant que tout est groupé)\n",
    "        tokenized_corpus = doc_tokens_grouped # C'est déjà la structure voulue: list[list[str]]\n",
    "        print(f\"Tokenized corpus created with {len(tokenized_corpus)} documents.\")\n",
    "        # Vérification optionnelle : nombre de tokens total\n",
    "        total_tokens_in_corpus = sum(len(doc_tokens) for doc_tokens in tokenized_corpus)\n",
    "        print(f\"Total tokens in tokenized corpus: {total_tokens_in_corpus}\")\n",
    "\n",
    "        # 2. Construire le dictionnaire Gensim à partir des lemmes de phrases\n",
    "        gensim_dictionary = Dictionary(lemma_lists_for_gensim)\n",
    "        # Optionnel: Filtrer le dictionnaire si nécessaire (appliquer après création)\n",
    "        # gensim_dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "        # gensim_dictionary.compactify() # Réassigner les IDs si filtré\n",
    "        print(f\"Gensim Dictionary created with {len(gensim_dictionary)} unique tokens.\")\n",
    "        # --- FIN NOUVELLE ÉTAPE ---\n",
    "\n",
    "\n",
    "        # --- Étape 2a: Vectorisation des DOCUMENTS (TF-IDF Scikit-learn) ---\n",
    "        print(\"\\n--- Vectorizing Documents (TF-IDF) ---\")\n",
    "        # Note: Cette vectorisation utilise les données _par phrase_ regroupées par document,\n",
    "        # ce qui est correct pour TF-IDF au niveau document.\n",
    "        X_docs, feature_names_doc = vectorize_corpus_from_data(\n",
    "            processed_data,\n",
    "            level='document',\n",
    "            num_items=num_docs\n",
    "        )\n",
    "        if X_docs is None:\n",
    "             raise ValueError(\"Document vectorization failed.\")\n",
    "\n",
    "\n",
    "        # --- Étape 2b: Vectorisation des PHRASES (TF-IDF Scikit-learn) ---\n",
    "        print(\"\\n--- Vectorizing Sentences (TF-IDF) ---\")\n",
    "        # Cette vectorisation utilise les données _par phrase_.\n",
    "        X_sentences, feature_names_sent = vectorize_corpus_from_data(\n",
    "            processed_data,\n",
    "            level='sentence',\n",
    "            num_items=num_sentences\n",
    "        )\n",
    "        if X_sentences is None:\n",
    "            raise ValueError(\"Sentence vectorization failed.\")\n",
    "\n",
    "\n",
    "        # --- Vérifications de Cohérence ---\n",
    "        # (Vos vérifications restent identiques, mais notez les comparaisons avec le nouveau vocabulaire Gensim)\n",
    "        if X_sentences.shape[0] != num_sentences:\n",
    "            print(f\"CRITICAL WARNING: Mismatch! X_sentences rows ({X_sentences.shape[0]}) != number of processed sentences ({num_sentences}).\")\n",
    "        if len(feature_names_doc) != len(feature_names_sent) or set(feature_names_doc) != set(feature_names_sent):\n",
    "             print(f\"Warning: Vocabularies differ between document ({len(feature_names_doc)}) and sentence ({len(feature_names_sent)}) vectorization (expected if min_df > 1 in vectorizer).\")\n",
    "        if X_docs.shape[1] != len(feature_names_doc):\n",
    "             print(f\"CRITICAL WARNING: X_docs column count ({X_docs.shape[1]}) != document feature names count ({len(feature_names_doc)}).\")\n",
    "        if X_sentences.shape[1] != len(feature_names_sent):\n",
    "             print(f\"CRITICAL WARNING: X_sentences column count ({X_sentences.shape[1]}) != sentence feature names count ({len(feature_names_sent)}).\")\n",
    "        # Comparaison vocabulaire Scikit-learn (niveau phrase) et Gensim\n",
    "        if set(feature_names_sent) != set(gensim_dictionary.token2id.keys()):\n",
    "             print(f\"Info: Scikit-learn sentence vocabulary ({len(feature_names_sent)} terms) and Gensim dictionary ({len(gensim_dictionary)} tokens) may differ due to vectorizer settings (e.g., min_df) or Gensim filtering (if applied).\")\n",
    "        # Vérification de la cohérence du tokenized_corpus\n",
    "        if len(tokenized_corpus) != num_docs:\n",
    "             print(f\"CRITICAL WARNING: Tokenized corpus length ({len(tokenized_corpus)}) != calculated num_docs ({num_docs}).\")\n",
    "\n",
    "\n",
    "        print(\"\\n--- Main Workflow Completed Successfully ---\")\n",
    "        # Retourner les artefacts principaux, y compris le dictionnaire ET le corpus tokenisé\n",
    "        return (X_docs, feature_names_doc, X_sentences, # Ajout feature_names_sent\n",
    "                gensim_dictionary, tokenized_corpus, # Ajout tokenized_corpus\n",
    "                sentence_file_path, sentence_offsets) # Ordre mis à jour\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- CRITICAL ERROR during workflow ---\")\n",
    "        import traceback\n",
    "        print(f\"Error Type: {type(e).__name__}\")\n",
    "        print(f\"Error Message: {e}\")\n",
    "        print(\"Traceback:\")\n",
    "        print(traceback.format_exc())\n",
    "        # Nettoyer le fichier temporaire en cas d'erreur\n",
    "        if sentence_file_path and os.path.exists(sentence_file_path):\n",
    "            try:\n",
    "                print(f\"Attempting to clean up temporary file: {sentence_file_path}\")\n",
    "                os.remove(sentence_file_path)\n",
    "            except OSError as rm_err:\n",
    "                print(f\"  Error removing temporary file: {rm_err}\")\n",
    "        # Retourner des valeurs indiquant l'échec (8 éléments)\n",
    "        return None, [], None, [], None, [], None, [] # <-- Mis à jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
